<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[页面级去重-文本相似度计算]]></title>
    <url>%2F2018%2F04%2F16%2F%E9%A1%B5%E9%9D%A2%E7%BA%A7%E5%8E%BB%E9%87%8D-%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97%2F</url>
    <content type="text"><![CDATA[一. 前言很久没写博客了，这段时间事情实在是多，很多看到的新知识只能草草的记下来没时间整理。今天下午面完阿里爸爸二面，晚上应该不会再突击我了，趁热把下午遇到的一个没解决的问题记录一下。 在爬虫去重这个问题上，单纯使用md5这样的完全比较hash算法不是很精确，因为不可能两个网页完全一样，都会有些稍微的更改，这样我们就需要记录并计算两篇文章的相似性进行查重。文章相似性计算应该属于NLP中的内容，由于我是个机器学习小白，所以我尽量使用工程化的思路来解决这个问题。 二. 最小编辑距离这种方法可能是最简单的方法了，编辑距离（Edit Distance），又称Levenshtein距离，是指两个字串之间，由一个转成另一个所需的最少编辑操作次数。许可的编辑操作包括： 将一个字符替换成另一个字符 插入一个字符 删除一个字符。 一般来说，编辑距离越小，两个串的相似度越大。然后拿编辑距离去除以两者之间的最大长度，意味着只要变动这么多就可以从A变成B，所以不用变动的字符便代表了相似度。这种方法实现比较简单，在&lt;编程之美&gt;上也有过介绍，主要是通过递归来实现。 两个字符串A，B的编辑距离最大是他们的长度之和，每次操作之后可能有如下6种情况： 删除A的第一个字符计算A[2….lenA]，B[1….lenB] 删除B的第一个字符计算A[1….lenA]，B[2….lenB] 修改A的第一个字符计算A[2….lenA]，B[2….lenB] 修改B的第一个字符计算A[2….lenA]，B[2….lenB] 插入A的第一个字符到B计算A[1….lenA]，B[2….lenB] 插入B的第一个字符到A计算A[2….lenA]，B[1….lenB] 我们可以将上述的6中情况合并成3种： 一步操作后计算A[2….lenA]，B[1….lenB] 一步操作后计算A[1….lenA]，B[2….lenB] 一步操作后计算A[2….lenA]，B[2….lenB] 然后每次获得三者中最小的一个再+1进行迭代即可。这种方法最简单也是最容易实现，但是可能每来一篇文章都需要和文档库中的文章进行比较，而且文章过长的话，递归深度也很大。 三. simhash1. TF-IDF TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。TF-IDF加权的各种形式常被搜索引擎应用，作为文件与用户查询之间相关程度的度量或评级。除了TF-IDF以外，因特网上的搜索引擎还会使用基于链接分析的评级方法，以确定文件在搜寻结果中出现的顺序。 这是摘自百度百科的解释，简而言之就是一个词语在一篇文章中出现的次数越多说明他越重要，一个词在文档库中出现的次数越少说明他越有代表性，利用这两个值进行一个权值相乘的计算，就能代表一个词语对于一个文章的重要程度。 2. simhash的实现simhash是谷歌发明的算法，据说很nb，可以将一个文档转换成64位的字节，然后我们可以通过判断两个字节的汉明距离就知道是否相似了。 在信息论中，两个等长字符串之间的汉明距离是两个字符串对应位置的不同字符的个数。换句话说，它就是将一个字符串变换成另外一个字符串所需要替换的字符个数。例如： 1011101 与 1001001 之间的汉明距离是 2。 首先我们来计算SimHash： 提取文档关键词得到[word,weight]这个一个数组。（举例 [美国，4]） 用hash算法将word转为固定长度的二进制值的字符串[hash(word),weight]。（举例 [100101，4]） word的hash从左到右与权重相乘，如果为1则乘以1 ，如果是0则曾以-1。（举例4,-4,-4,4,-4,4） 接着计算下个数，直到将所有分词得出的词计算完，然后将每个词第三步得出的数组中的每一个值相加。（举例美国和51区，[4,-4,-4,4,-4,4]和[5 -5 5 -5 5 5]得到[9 -9 1 -1 1 9]） 对第四步得到的数组中每一个值进行判断，如果＞0记为1，如果＜0记为0。（举例[101011]） 第四步得出的就是这个文档的SimHash。 这样我们就能将两个不同长度的文档转换为同样长度的SimHash值，so，我们现在可以计算第一个文档的值和第二个文档的汉明距离（一般&lt;3就是相似度高的）。 SimHash本质上是局部敏感性的hash（如果是两个相似的句子，那么只会有部分不同），和md5之类的不一样。 正因为它的局部敏感性，所以我们可以使用海明距离来衡量SimHash值的相似度。 如果想要小数形式的可以这么做：1 - 汉明距离 / 最长关键词数组长度。 引用 &lt;编程之美&gt;https://blog.csdn.net/chinafire525/article/details/78686876]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[同步异步和阻塞非阻塞的区别]]></title>
    <url>%2F2018%2F03%2F10%2F%E5%90%8C%E6%AD%A5%E5%BC%82%E6%AD%A5%E5%92%8C%E9%98%BB%E5%A1%9E%E9%9D%9E%E9%98%BB%E5%A1%9E%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[吐槽早上五点半胃疼醒了，去医院噼里啪啦检查了一天花了1500啥也没检查出来，继续疼。在排队的时候刷到知乎上的这个问题感觉似懂非懂，这里总结一下看到的几个比较好的解释。 一. I/O模型 阻塞式I/O 非阻塞式I/O I/O复用（select，poll，epoll…） 信号驱动式I/O（SIGIO） 异步I/O（POSIX的aio_系列函数） 一个输入操作一般有两个不同的阶段： 等待数据准备好 从内核到进程拷贝数据 对于一个套接字上的输入操作，第一步通常涉及等待数据从网络中到达。当所有等待分组到达时，它被复制到内核中的某个缓冲区。第二步就是把数据从内核缓冲区复制到应用程序缓冲区。 1. 阻塞式I/O模型默认情况下，所有套接字都是阻塞的。下面我们以阻塞套接字的recvfrom的的调用图来说明阻塞。 标红的这部分过程就是阻塞，直到阻塞结束recvfrom才能返回。 2.非阻塞式I/O以下这句话很重要：进程把一个套接字设置成非阻塞是在通知内核，当所请求的I/O操作非得把本进程投入睡眠才能完成时，不要把进程投入睡眠，而是返回一个错误。看看非阻塞的套接字的recvfrom操作如何进行,可以看出recvfrom总是立即返回。 3.I/O多路复用虽然I/O多路复用的函数也是阻塞的，但是其与以上两种还是有不同的，I/O多路复用是阻塞在select，epoll这样的系统调用之上，而没有阻塞在真正的I/O系统调用如recvfrom之上。 4.信号驱动式I/O见得少，直接给出原图 5. 异步I/O这类函数的工作机制是告知内核启动某个操作，并让内核在整个操作（包括将数据从内核拷贝到用户空间）完成后通知我们。如图所示，注意红线标记处说明在调用时就可以立马返回，等函数操作完成会通知我们。 区别前四种I/O模型都是同步I/O操作，他们的区别在于第一阶段，而他们的第二阶段是一样的：在数据从内核复制到应用缓冲区期间（用户空间），进程阻塞于recvfrom调用。相反，异步I/O模型在这两个阶段都要处理。书上也有一副很好的图解释了 同步I/O操作：导致请求进程阻塞，直到I/O操作完成 异步I/O操作：不导致请求进程阻塞 阻塞，非阻塞：进程/线程要访问的数据是否就绪，进程/线程是否需要等待； 同步，异步：访问数据的方式，同步需要主动读写数据，在读写数据的过程中还是会阻塞；异步只需要I/O操作完成的通知，并不主动读写数据，由操作系统内核完成数据的读写。 这里有个来自知乎的例子：老张爱喝茶，废话不说，煮开水。出场人物：老张，水壶两把（普通水壶，简称水壶；会响的水壶，简称响水壶）。 老张把水壶放到火上，立等水开。（同步阻塞）老张觉得自己有点傻 老张把水壶放到火上，去客厅看电视，时不时去厨房看看水开没有。（同步非阻塞）老张还是觉得自己有点傻，于是变高端了，买了把会响笛的那种水壶。水开之后，能大声发出嘀~~~~的噪音。 老张把响水壶放到火上，立等水开。（异步阻塞）老张觉得这样傻等意义不大 老张把响水壶放到火上，去客厅看电视，水壶响之前不再去看它了，响了再去拿壶。（异步非阻塞） 老张觉得自己聪明了。所谓同步异步，只是对于水壶而言。普通水壶，同步；响水壶，异步。虽然都能干活，但响水壶可以在自己完工之后，提示老张水开了。这是普通水壶所不能及的。同步只能让调用者去轮询自己（情况2中），造成老张效率的低下。所谓阻塞非阻塞，仅仅对于老张而言。立等的老张(被挂起)，阻塞；看电视的老张，非阻塞。情况1和情况3中老张就是阻塞的，媳妇喊他都不知道。虽然3中响水壶是异步的，可对于立等的老张没有太大的意义。所以一般异步是配合非阻塞使用的，这样才能发挥异步的效用。 二. IO复用这里一开始可能看不出来IO复用比之前阻塞IO有哪些好处，这里也给出一个在知乎上看的解释： 假设你是一个机场的空管， 你需要管理到你机场的所有的航线， 包括进港，出港， 有些航班需要放到停机坪等待，有些航班需要去登机口接乘客。你会怎么做?最简单的做法，就是你去招一大批空管员，然后每人盯一架飞机， 从进港，接客，排位，出港，航线监控，直至交接给下一个空港，全程监控。那么问题就来了：很快你就发现空管塔里面聚集起来一大票的空管员，交通稍微繁忙一点，新的空管员就已经挤不进来了。 空管员之间需要协调，屋子里面就1, 2个人的时候还好，几十号人以后 ，基本上就成菜市场了。空管员经常需要更新一些公用的东西，比如起飞显示屏，比如下一个小时后的出港排期，最后你会很惊奇的发现，每个人的时间最后都花在了抢这些资源上。 现实上我们的空管同时管几十架飞机稀松平常的事情， 他们怎么做的呢？他们用这个东西 这个东西叫flight progress strip. 每一个块代表一个航班，不同的槽代表不同的状态，然后一个空管员可以管理一组这样的块（一组航班），而他的工作，就是在航班信息有新的更新的时候，把对应的块放到不同的槽子里面。这个东西现在还没有淘汰哦，只是变成电子的了而已。。是不是觉得一下子效率高了很多，一个空管塔里可以调度的航线可以是前一种方法的几倍到几十倍。如果你把每一个航线当成一个Sock(I/O 流), 空管当成你的服务端Sock管理代码的话.第一种方法就是最传统的多进程并发模型 (每进来一个新的I/O流会分配一个新的进程管理。)第二种方法就是I/O多路复用 (单个线程，通过记录跟踪每个I/O流(sock)的状态，来同时管理多个I/O流 。)其实“I/O多路复用”这个坑爹翻译可能是这个概念在中文里面如此难理解的原因。所谓的I/O多路复用在英文中其实叫 I/O multiplexing. 如果你搜索multiplexing啥意思，基本上都会出这个图：于是大部分人都直接联想到”一根网线，多个sock复用” 这个概念，包括上面的几个回答， 其实不管你用多进程还是I/O多路复用， 网线都只有一根好伐。多个Sock复用一根网线这个功能是在内核＋驱动层实现的。重要的事情再说一遍： I/O multiplexing 这里面的 multiplexing 指的其实是在单个线程通过记录跟踪每一个Sock(I/O流)的状态(对应空管塔里面的Fight progress strip槽)来同时管理多个I/O流. 发明它的原因，是尽量多的提高服务器的吞吐能力。 是不是听起来好拗口，看个图就懂了在同一个线程里面， 通过拨开关的方式，来同时传输多个I/O流。 顺便提一句： select和poll都只返回可用集合，线程不安全，区别是select有1024个连接的限制 epoll 现在是线程安全的，返回具体可用的连接，现在不仅告诉你sock组里面数据，还会告诉你具体哪个sock有数据，你不用自己去找了。 引用 https://www.zhihu.com/question/19732473UNIX网络编程(卷一)https://www.zhihu.com/question/32163005]]></content>
      <categories>
        <category>计算机基础</category>
      </categories>
      <tags>
        <tag>UNIX</tag>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[类加载机制]]></title>
    <url>%2F2018%2F03%2F06%2F%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[一. 虚拟机类加载机制1. 概述:把描述类的数据从class文件加载到内存，对数据进行校验、转换和初始化，最终形成可被虚拟机使用的java类型。 2. 过程 加载、验证、准备、初始化和卸载这五个阶段的顺序是确定的，类的加载过程必须按照这种顺序按部就班的“开始”（仅仅指的是开始，而非执行或者结束，因为这些阶段通常都是互相交叉的混合进行，通常会在一个阶段执行的过程中调用或者激活另一个阶段），而解析阶段则不一定（它在某些情况下可以在初始化阶段之后再开始，这是为了支持Java语言的运行时绑定。 验证、准备、解析称为连接Linking阶段。 加载 验证 准备 初始化 卸载 需要按顺序开始 3. 初始化有且只有5种情况必须立刻初始化的： 使用new实例化对象，读取或设置一个类的静态字段，调用静态方法(不包括常量) 使用java.lang.reflect包对类进行反射调用，如果没有进行类初始化，则需要先触发其初始化 当初始化一个类的时候，如果发现其父类还没初始化，要先触发其父类的初始化 虚拟机启动时，需要指定一个要执行的主类 java7动态语言支持，一个java.lang.invoke.MethodHandle实例最后的解析结果REF_getStatic、 REF_putStatic、 REF_invokeStatic方法句柄，并且方法句柄对应的类没有进行过初始化，需要先触发其初始化。(没用过不是很理解) 以上五种引用类的方法称为主动引用,除此之外都是被动引用。比如： 通过子类调用父类的静态字段。 通过数组定义来引用类 接口初始化接口的加载过程与类的加载过程稍有不同。接口中不能使用static{}块。当一个接口在初始化时，并不要求其父接口全部都完成了初始化，只有真正在使用到父接口时（例如引用接口中定义的常量）才会初始化。 12345678910111213class superClass&#123; static &#123; System.out.println("123"); &#125;&#125;public class Demo &#123; public static void main(String[] args) &#123; superClass[] a = new superClass[2]; &#125;&#125;///会触发一个superClass一维数组类的初始化 引用常量 123456789101112131415161718192021222324package org.fenixsoft.classloading;/** * 被动使用类字段演示三：* 常量在编译阶段会存入调用类的常量池中，本质上没有直接引用到定义常量的类，因此不会触发定义常量的类的初始化。 **/public class ConstClass &#123; static &#123; System.out.println("ConstClass init!"); &#125; public static final String HELLOWORLD = "hello world";&#125;/** * 非主动使用类字段演示 **/public class NotInitialization &#123; public static void main(String[] args) &#123; System.out.println(ConstClass.HELLOWORLD); &#125;&#125; PS：接口的不同，在于第三种，不要求其父接口全部完成了初始化，只有在使用的时候才会初始化。 二. 类加载过程1. 加载 通过一个全限定名来获取定义此类的二进制字节流。 将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构 在内存中生成代表这个类的java.lang.Class对象 加载阶段即可以使用系统提供的类加载器在完成，也可以由用户自定义的类加载器来完成。加载阶段与连接阶段的部分内容(如一部分字节码文件格式验证动作)是交叉进行的，加载阶段尚未完成，连接阶段可能已经开始。 总结就是加载类的二进制资源，转为相应的方法区数据结构生成Class对象 2. 验证（连接阶段）确保Class文件的字节流中包含的信息符合当前虚拟机的要求，并且不会危害虚拟机。 文件格式验证 验证字节流是否符合Class文件格式的规范，并且能被当前版本的虚拟机处理。 元数据验证 对字节码描述的信息进行语义分析，以保证其描述的信息符合Java语言规范的要求。 字节码验证 主要工作是进行数据流和控制流分析，保证被校验类的方法在运行时不会做出危害虚拟机安全的行为 符号引用验证 发生在虚拟机将符号引用转化为直接引用的时候，这个转化动作将在“解析阶段”中发生。使用-Xverify:none关闭大部分反复使用的代码验证 3. 准备（连接阶段）正式为类变量分配内存并设置类变量的初始值。仅仅包括类变量（static）不包括实例变量，这里的初始值是0，而不是实际值。1public static int value = 123; value=123在类构造器的方法中，在初始化阶段才会执行。若在字段属性表中存在ConstantValue属性，那么在准备阶段就会初始化为所指定的值1public staic final int value = 123; 在准备阶段就会将value赋值为123 4. 解析（连接阶段）将常量池符号引用替换为直接引用 符号引用：内存布局无关，不一定已经加载到内存中 直接引用：内存布局相关，如果有那么一定在内存中存在 符号引用： 类和接口的全限定名 字段的名称和描述符 方法的名称和描述符 5. 初始化真正开始执行类中定义的java代码，初始化阶段执行类构造器自动收集类变量的赋值和静态语句块，按照出现顺序决定，优先执行父类12345678910111213public class Demo &#123; static &#123; i = 20;//可以赋值 System.out.println(i);//不能引用 &#125; static int i = 10; public static void main(String[] args) &#123; System.out.println(Demo.i); &#125;&#125; 加载顺序可以参考这个图 三. 类加载器（热部署）只有同一个类加载器加载出来的类，才是真正意义上的相等。 启动类加载器bootstrap 负责将存放在\lib目录或者-Xbootclasspath参数所指定的路径中的类库加载到虚拟机内存中。 扩展类加载器extension \lib\ext目录or java.ext.dirs系统变量指定路径的类库 应用程序类加载器application（系统类加载器） 加载classpath指定类库 12345graph BT扩展类加载器--&gt;启动类加载器应用程序类加载器--&gt;扩展类加载器自定义类加载器--&gt;应用程序类加载器自定义类加载器--&gt;应用程序类加载器 双亲委派模型除了启动类加载器外，所有的类加载器都有自己的父类加载器，而且是通过组合而不是继承的关系来实现。这样可以保证应用程序的稳定。比如所有的java.lang.object都是通过启动类加载器加载rt.jar中的object实现的 破坏双亲委派模型实现热部署每一个程序模块都有一个自己的类加载器，当需要更换一个模块时，就把模块连同类加载器一起换掉以实现代码的热替换。]]></content>
      <categories>
        <category>jvm</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GC和内存分配策略]]></title>
    <url>%2F2018%2F03%2F05%2FGC%E5%92%8C%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5%2F</url>
    <content type="text"><![CDATA[一. 概述由于程序计数器、虚拟机栈和本地方法栈都是跟线程相关的，栈中的栈帧随着方法的进入和退出进行着出栈和入栈的操作，当方法结束的时候，这部分内存也会跟着回收，所以一般gc讨论的都是方法区和堆。 GC对象的判断1. 引用计数法给对象添加一个计数器，有人引用就加1，引用失效就减1，任何时刻计数器为0的对象就不可能再被使用了。 2. 可达性分析算法判断当前对象与GC Root是否可达 可作为GC Roots的对象： 虚拟机栈（栈帧中的本地变量表）中的引用对象 方法区中类静态属性引用的对象 方法区中常量引用的对象 本地方法栈中JNI(Native方法）引用的对象 说的通俗一点就是：方法运行时，方法中引用的对象；类的静态变量引用的对象；类中常量引用的对象；Native方法中引用的对象。在网上看老外说的更具体分成了下面这些： System Class Class loaded by bootstrap/system class loader. For example, everything from the rt.jar like java.util.* . JNI Local Local variable in native code, such as user defined JNI code or JVM internal code. JNI Global Global variable in native code, such as user defined JNI code or JVM internal code. Thread Block Object referred to from a currently active thread block. Thread A started, but not stopped, thread. Busy Monitor Everything that has called wait() or notify() or that is synchronized. For example, by calling synchronized(Object) or by entering a synchronized method. Static method means class, non-static method means object. Java Local Local variable. For example, input parameters or locally created objects of methods that are still in the stack of a thread. Native Stack In or out parameters in native code, such as user defined JNI code or JVM internal code. This is often the case as many methods have native parts and the objects handled as method parameters become GC roots. For example, parameters used for file/network I/O methods or reflection. Finalizable An object which is in a queue awaiting its finalizer to be run. Unfinalized An object which has a finalize method, but has not been finalized and is not yet on the finalizer queue. Unreachable An object which is unreachable from any other root, but has been marked as a root by MAT to retain objects which otherwise would not be included in the analysis. Java Stack Frame A Java stack frame, holding local variables. Only generated when the dump is parsed with the preference set to treat Java stack frames as objects. 13.Unknown An object of unknown root type. Some dumps, such as IBM Portable Heap Dump files, do not have root information. For these dumps the MAT parser marks objects which are have no inbound references or are unreachable from any other root as roots of this type. This ensures that MAT retains all the objects in the dump. 3. 引用的分类在java1.2之后对引用的概念进行了扩充，有了4种引用 强引用：程序代码中普遍存在的类似1Object o = new Object(); 只要强引用还在，GC就不会回收掉被引用的对象。 软引用： 描述一些还有用但是非必须的对象。在系统将要发生内存溢出异常之前，将会把这些对象列进回收范围之中进行第二次回收，如果此时内存还不够才会oom 如果一个对象只具有软引用，则内存空间足够，垃圾回收器就不会回收它；如果内存空间不足了，就会回收这些对象的内存。只要垃圾回收器没有回收它，该对象就可以被程序使用。软引用可用来实现内存敏感的高速缓存。 弱引用： 也是描述非必须对象，强度比弱引用还要弱，被弱引用关联的对象只能生存到下一次垃圾收集发生之前。当gc工作无论当时内存是否足够都会回收掉只被弱引用关联的对象。 虚引用： 最弱的引用关系。一个对象是否有虚引用完全不会对其生存周期构成影响，也无法通过虚引用来获取一个对象实例。设置他的目的就是在被GC回收的时候收到一个系统通知 4. 可达性分析若一个对象的引用类型有多个，那到底如何判断它的可达性呢？其实规则如下： 单条引用链的可达性以最弱的一个引用类型来决定；多条引用链的可达性以最强的一个引用类型来决定； 我们假设图2中引用①和③为强引用，⑤为软引用，⑦为弱引用，对于对象5按照这两个判断原则，路径①-⑤取最弱的引用⑤，因此该路径对对象5的引用为软引用。同样，③-⑦为弱引用。在这两条路径之间取最强的引用，于是对象5是一个软可及对象 5. 回收方法区废弃常量和无用类的回收 回收废弃常量类似回收对象，比如一个字符串常量，没有被任何String对象使用，如果这时发生了内存回收，而且有必要的话，会被系统清理出常量池 废弃类的判断要同时满足下面3个条件： 该类的所有实例都已经被回收，也就是Java堆中不存在该类的任何实例 加载该类的ClassLoader已经被回收 该类对应java.lang.Class对象没有在任何地方被引用，无法在任何地方通过反射方位该类的方法 在java8中由于永久代的废弃，类信息保存在本地内存中，所以废弃类导致oom溢出的情况得到了很大的改善。 二. 垃圾收集算法1. 标记清除算法对象真正死亡要经历2次标记过程 对象在进行可达性分析后，没有与GC ROOTS相连接的引用链，那么它会被第一次标记，并进行一次筛选，筛选条件是此对象是否有必要执行的finalize（）方法。以下是2种没有必要执行finalize（）方法的情形。 对象没有覆盖finalize（） finalize（）方法已经被虚拟机调用过。因为finalize（）方法只会被虚拟机调用一次。 如果要执行finalize（）方法，则会将对象加入F——QUEUE的队列中，并且会在稍后由jvm自动建立的、低优先级的Finalizer线程去执行。这里的执行只是会去触发这个方法并不能承诺会等待方法运行结束。因为如果一个对象在finalize（）方法中执行缓慢，或者发生了死循环，会导致F-Queue中其他对象处于永久等待，甚至导致整个内存回收系统崩溃。对象可以在finalize（）方法中重新和引用链建立关联，这样就可以逃脱回收。 GC对F-Queue中的对象进行第二次小规模的标记。也就是说如果没有执行finalize（）方法进入F-Queue那么直接就被删除了。finalize()是对象逃脱死亡的最后机会，但是finalize()是对c++的妥协，他的运行代价高，不确定性大，使用try——finally能更好的完成他需要执行的工作，所以不推荐使用它。 缺点： 效率问题，标记和清楚2个过程的效率都不高 空间问题，标记清除之后会长生大量的不连续碎片，碎片太多在分配较大对象的时候，无法找到足够的连续内存不得不触发领一次垃圾收集动作 2. 复制算法复制算法(新生代 GC)基于大多数新生对象(98%)都会在GC中被收回的假设。新生代的GC 使用复制算法。 在GC前To 幸存区(survivor)保持清空,对象保存在 Eden 和 From 幸存区(survivor)中，GC运行时,Eden中的幸存对象被复制到 To 幸存区(survivor)。针对 From 幸存区(survivor)中的幸存对象，会考虑对象年龄,如果年龄没达到阀值(tenuring threshold)，对象会被复制到To 幸存区(survivor)。如果达到阀值对象被复制到老年代。复制阶段完成后，Eden 和From 幸存区中只保存死对象，可以视为清空。如果在复制过程中To 幸存区被填满了，剩余的对象会被复制到老年代中。最后 From 幸存区和 To幸存区会调换下名字，在下次GC时，To 幸存区会成为From 幸存区。当survivor空间不够的时候会进行分配担保，把对象保存到老年代。 优点：运行高效，实现简单 缺点：会浪费一定的内存，对象存活率较高会产生过多的复制 上图演示GC过程，黄色表示死对象，绿色表示剩余空间，红色表示幸存对象 3. 标记-整理算法(老年代)由于老年代的对象存活时间久，使用复制算法将进行大量复制操作，效率很低，而且其存在时间久，很可能出现大面积的存活对象，这样在极端情况下100%内存的对象都存活，就还需要额外的空间分配担保。 标记-整理算法类似之前的标记清楚算法，但是他不会直接清除可回收对象，而是将存活对象都移动到一段，再直接清理边界以外的内存。 4. HostSpot的实现1. 枚举GCRoot通过OopMap数据结构，在类加载的时候，就把对象内什么偏移量上是什么类型的数据计算出来了。 2. 安全点会导致OopMap内容变化的指令非常多，所以只会在特定的位置记录这些信息，这样的位置称为安全点。 比如：方法调用、循环跳转、异常跳转等。 在这个点, 所有GC Root的状态都是已知并且heap里的对象是一致的; 在这个点进行GC时, 所有的线程都需要block住, 这就是(STW)Stop The World. 在安全点终端所有线程的两种方法 抢先式中断：所有线程中断，如果有的线程不在安全点上就恢复它，让其执行到安全点再中断，但是几乎没有jvm采用这种方式。 主动式中断：在和安全点重合的地方设置一个轮询标志，让线程执行的时候主动去轮询这个标志，如果中断标志为真，就自己中断挂起。3. 安全区域safe region针对于处于Sleep状态或者blocked状态的线程，指的是在一段代码片段之中引用关系不会发生变化，在这片区域的任何地方GC都是安全的。当线程执行到安全区域的时候首先会标识自己进入了安全区域，这样在这段时间内都可以进行gc，当要离开安全区域时，会检查是否已经完成了gcroot的枚举，如果完成了就继续执行，否则就等待。 5. 垃圾收集器连线代表可搭配使用 1.Serial收集器（Client模式默认新生代收集器，复制算法，单线程）在进行垃圾回收的时候，暂停所有正在工作的线程，直到结束。优点：简单高效。 2. ParNew收集器（新生代，复制，多线程）ParNew就是Serial的多线程版本，除了多线程外几乎一致。Server模式下的首选新生代收集器，只有他和Serial能和cms配合。使用-XX:+UseParNewGC开关来控制，使用-XX:ParallelGCThreads来设置执行内存回收的线程数。 3. Parallel Scavenge收集器（新生代，复制,多线程,吞吐量优先）吞吐量=运行用户代码时间/(运行用户代码时间+gc时间) Parallel Scavenge收集器不像别的收集器关注的是减少用户停顿时间，而是吞吐量。 -XX:+UseParallelGC开关控制 -XX：MaxGCPauseMillis控制最大gc停顿时间 -XX:GCTimeRatio设置吞吐量 -XX:+UseAdaptiveSizePolicy,自动的动态调整停顿时间和吞吐量，GC的自适应调节策略，适合新手。 4. Serial Old收集器（老年代，标记-整理，单线程）是Serial收集器的老年代版本 1.5之前和Parallel Scavenge搭配使用或者作为cms的备案 5. Parallel Old收集器（老年代， 标记-整理，多线程）Parallel Scavenge的老年代版本，适合注重吞吐量和cpu资源敏感的场合 6. CMS收集器（老年代，标记-清理，多线程）以获取最短回收停顿时间为目标的收集器。 过程： 初始标记：标记一下GCROOT能关联到的对象stw 并发标记：GCROOT Tracing 重新标记：修正并发标记期间产生变动的引用stw 并发清除在初始标记和重新标记的时候需要stop the world 优点： 并行，停顿小。 缺点： cpu资源敏感： 因为并发导致吞吐量降低，随着cpu数量的下降对程序影响越大 无法处理浮动垃圾： 因为并发清理是并发执行的，所以此时还是会产生心的垃圾，称之为浮动垃圾。这部分垃圾只能等待下一次的GC。 这里产生一个问题就是老年代不能等到完全的利用，需要一部分用来存储浮动垃圾，如果预留的内存无法满足需要就会产生“ConcurrentModeFailure”，这是会启用Serial Old收集器进行gc。 使用标记-清除算法产生大量内存碎片 7. G1收集器特点： 缩短stop the world停顿时间，通过并发的方式将停顿并发执行从而取消停顿。 分代收集，不需要其他收集器单独管理整个gc堆，采用不同方式处理新对象和老对象 不会产生碎片。 可预测的停顿 G1收集的范围是整个新生代和老年代，但是他是将整个堆划分为多个大小相等的独立区域（Region），它会优先回收价值最大的Region，保证在有限时间获取最高的收集效率。根据用户所期望的GC停顿时间来指定回收计划。 初始标记 标记GC ROOTs能直接关联到的对象，耗时短 并发标记 可达性分析，耗时长，但是可以和用户线程并发 最终标记 需要停顿，但是可以并行执行 筛选回收 对回收价值和成本进行排序 其他收集器的范围都是整个新生代或者老年代，G1自己管理整个，他将heap的内存布局划分成多个大小相等的独立区域(region)，虽然还保留新生代和老年代的概念。 垃圾堆积的价值：回收所获得的空间大小以及回收所需时间的经验值的关系维护一个优先队列，优先回收价值最大的region g1和cms区别CMS收集器：是基于标记清除算法实现的，一般就是初始标记，并发标记，重新标记，并发清除，目的是实现最短的响应回收时间。保证系统的响应时间，减少垃圾收集时的停顿时间G1收集器：他的过程是初始标记、并发标记、最终标记、筛选回收，基于标记整理算法实现，以吞吐量优先，保证保证吞吐量的。 内存分配机制新生代与老年代为了优化垃圾回收的性能，将堆分为了新生代和老年代 优点： 是简化了新生对象的分配（只在新生代分配内存）， 是可以针对不同区域使用更有效的垃圾回收算法。 新生代通过广泛研究面向对象实现的应用，发现一个共同特点：很多对象的生存时间都很短。同时研究发现，新生对象很少引用生存时间长的对象。结合这2个特点，很明显 GC 会频繁访问新生对象。在新生代中，GC可以快速标记回收”死对象”，而不需要扫描整个Heap中的存活一段时间的”老对象”。 SUN/Oracle 的HotSpot JVM 又把新生代进一步划分为3个区域：一个相对大点的区域，称为”伊甸园区(Eden)”；两个相对小点的区域称为”From 幸存区(survivor)”和”To 幸存区(survivor)”。按照规定,新对象会首先分配在 Eden 中(如果新对象过大，会直接分配在老年代中)。在GC中，Eden 中的对象会被移动到survivor中，直至对象满足一定的年纪(定义为熬过GC的次数),会被移动到老年代。 新生代GC(Minor GC):发生在新生代的垃圾收集动作，频繁，回收速度快 老年代GC(Full GC/ Major GC)：发生在老年代的GC，一般比Minor GC慢10倍对象优先在Eden分配当Eden不够时，发起一次Minor GC大对象直接进入老年代比如很长的数组和字符串 —XX：PretenureSizeThreshold参数，零大于这个值的对象直接在老年代分配 长期存活的对象进入老年代经过一次Minor GC后存活进入Survivor的对象设置为1岁，每经过1次Minor GC加一岁，达到一定岁数进入老年代。 动态对象年龄判定survivor空间中相同年龄所有对象大小的总和大于单个survivor空间的一半，年龄大于等于该年龄的对象就可以直接进入老年代 空间分配担保在Minor Gc前检查老年代的连续空间大于新生代对象总大小，if （true）则这次Minor GC是安全的。 只要老年代的连续空间大于新生代对象总大小或者历次晋升的平均大小就会尽心Minor GC 否则 full GC。 参考地址 http://ifeve.com/useful-jvm-flags-part-5-young-generation-garbage-collection/]]></content>
      <categories>
        <category>jvm</category>
      </categories>
      <tags>
        <tag>内存调优</tag>
        <tag>gc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java内存区域]]></title>
    <url>%2F2018%2F03%2F04%2Fjava%E5%86%85%E5%AD%98%E5%8C%BA%E5%9F%9F%2F</url>
    <content type="text"><![CDATA[一、运行时数据区域 1. 程序计数器（线程私有）可以看做当前线程所执行的字节码的行号指示器，为了线程切换后能恢复到正确的执行位置，每条线程都需要一个独立的程序计数器，他们之间互不影响独立存储，若线程执行的是一个java方法，则记录的时候是正在执行的虚拟机字节码指令的地址；若执行的是native方法，则为空。 2. Java虚拟机栈（线程私有）java方法执行的内存模型，方法在执行的时候都会创建一个栈帧（stack frame),方法从调用直至执行完成的过程，就对应一个栈帧在虚拟机栈中入栈到出栈的过程。马士兵说的栈就是这里的虚拟机栈，或者说是虚拟机栈中的局部变量表部分。 局部变量表存放编译期可知的各种基本数据类型、对象引用和returnAddress类型。 局部变量表的内存在编译器完成分配，运行期间不会改变局部变量表的大小。 3. 本地方法栈作用同上，针对native房阿发 4. java堆（线程共享）存放对象实例，所有的对象实例和数组都要在堆上分配。不需要连续的内存，可以选择固定大小或者可扩展 5. 方法区non-heap（线程共享，永生代）方法区是java堆的逻辑部分（可能因为都是线程共享的？），存储已经被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。jvm规范将他归为堆的逻辑部分，但是他的别名叫Non-Heap。不需要连续的内存，可以选择固定大小或者可扩展 永生代是java虚拟机在HotSpot上的实现，在去永生代之后，方法区作为逻辑概念仍然存在，只不过是通过元空间的形式实现。 6. 运行时常量池是方法区的一部分，Class文件中也有常量池，用于存放编译期生成的各种字面量和符号引用，这部分内容将在类加载后进入方法区的运行时常量池中存放。 字面量：如文本字符串、声明为final的常量等 符号引用： 类和接口的全限定名 字段的名称和描述符 方法的名称和描述符 运行时常量池还具有动态性，并非预置在Class文件中的常量池才能从编译器产生进入运行时常量池，运行期间也可以，String类的intern（）。 7. 直接内存nio通过native函数库直接分配堆外内存，再通过一个存储在java堆中的directbytebuffer对象作为这块内存的引用进行操作，避免在java堆和native堆中来回复制数据从而提高性能。 二、对象的深入了解1. 对象创建过程当遇到new指令之后 类检查：检查是否是已解释的类（查符号引用），是否已经初始化（否则先执行类加载） 分配内存：根据使用的GC机制使用不同的分配方法 堆中内存规整，使用“指针碰撞”（一边是空闲的，一边是使用的，通过指针移动划分） 内存不规整，使用“空闲列表”（维护一个查询列表） 解决临界资源（内存）的问题 进行同步处理，CAS（乐观锁）+重试。CAS：通过3个值，内存值V，旧的预期值A，要修改的新值B。当且仅当预期值A和内存值V相同时，将内存值V修改为B，否则什么都不做。 本地线程分配缓冲TLAB：每个线程在java堆中预先分配一小块内存。 内存分配完后将内存初始化为0，若使用TLAB则在TLAB分配时进行，这样保证对象的实例字段在为初始值的时候就可以直接使用。 设置对象头 虚拟机的实例化完成 new指令后接着执行方法按照程序员意愿进行初始化。 2. 对象的内存布局对象分为对象头、实例数据、对齐填充 对象头： 存储对象自身的运行时数据（哈希码，gc分代年龄，锁状态标志，线程持有的锁、偏向线程ID、偏向时间戳，会根据状态服用自己的存储空间。 类型指针，指向类元数据的指针，确定这个对象是哪个类的实例，但是非必须。若为数组类型，还要记录数组长度。 实例数据:程序代码中所定义的各种类型的字段。 对齐补充: 因为对象大小是8字节的整数倍 3. 对象的访问定位java程序通过栈上的reference数据来操作堆上的具体对象，有2种主流实现方法 句柄 会在堆中划分一个句柄池，reference中存储的就是句柄的地址，句柄包含了指向实例数据的指针和类型数据的指针（类描述信息） 优点：是存储了稳定的句柄地址，GC后对象移动后只要修改句柄中的实例数据指针就好，reference不用修改 直接指针(HotSpot默认) reference存储的直接就是对象地址，对象中包括实例数据和存储类型数据的指针，优点是减少了一次指针定位的开销，加快了速度 4. 内存异常实战首先明确两个概念，内存溢出和内存泄露。 内存溢出 out of memory：是指程序在申请内存时，没有足够的内存空间供其使用，出现out of memory；比如申请了一个integer,但给它存了long才能存下的数，那就是内存溢出。 内存泄漏：指你向系统申请分配内存进行使用(new)，可是使用完了以后却不归还(delete)，结果你申请到的那块内存你自己也不能再访问（也许你把它的地址给弄丢了），而系统也不能再次将它分配给需要的程序。 比如各种连接没有close掉，在之前的版本中出现大量的字符串没有gc掉 memory leak会最终会导致out of memory。 1. 堆溢出通过jvm args： -XX:+HeapDumpOnOutOfMemoryError ：可以让虚拟机在出现内存溢出异常的时候dump当前的内存堆转储快照。通过Eclipse Memory Analyzer对文件进行分析。 -Xms20m：设置堆的最小内存值为20mb-Xmx20m：设置堆的最大内存值为20mb 2. 栈溢出-Xss设置栈内存 由于os分配给进程内存是有限制的，比如32位的windows是2gb，而jvm提供参数控制堆和方法区的最大值，所以剩余内存为2gb-Xmx-最大方法区容量，程序计数器可以忽略不计，所以若jvm的也忽略不计，那剩下的内存就由本地方法栈和虚拟机栈瓜分。如果建立过多线程导致内存溢出，在不能减少线程数或更换高位虚拟机的情况下只能通过减少最大堆和栈容量来换取更多的线程。 3.1 方法区和运行时常量池溢出String.intern()是一个Native方法，如果字符串常量池中包含等于此String对象的字符串，则返回代表池中这个字符串的String对象，否则将String对象包含的字符串添加到常量池中并且返回这个字符串的String对象。这里推荐一篇介绍JDK6和JDK7中String.inter()区别的帖子https://tech.meituan.com/in_depth_understanding_string_intern.html/12345678910111213141516/** * VM Args：-XX:PermSize=10M -XX:MaxPermSize=10M * @author zzm */public class RuntimeConstantPoolOOM &#123; public static void main(String[] args) &#123; // 使用List保持着常量池引用，避免Full GC回收常量池行为 List&lt;String&gt; list = new ArrayList&lt;String&gt;(); // 10MB的PermSize在integer范围内足够产生OOM了 int i = 0; while (true) &#123; list.add(String.valueOf(i++).intern()); &#125; &#125;&#125; 这段代码在1.6中会因为常量池溢出，而在1.7中会继续执行，就是因为去永久代，具体细节稍后会详细讨论。 一个有意思的例子1234567891011public class RuntimeConstantPoolOOM &#123; public static void main(String[] args) &#123; public static void main(String[] args) &#123; String str1 = new StringBuilder("中国").append("钓鱼岛").toString(); System.out.println(str1.intern() == str1); String str2 = new StringBuilder("ja").append("va").toString(); System.out.println(str2.intern() == str2); &#125; &#125;&#125; 在1.6中会出现2个false；在1.7中会得到一个true一个false； 这里有2个问题： 在1.6中intern（）会把首次遇到的字符串复制到常量池，返回的是常量池中字符串的实例引用，但是由StringBuilder创建的字符串实例却会出现在堆上，所以是false； 问题出现了，有些方法会在堆中创建重复的对象而不直接引用常量池。 知乎高人这么理解的难道每创建一个新实例就去常量池里查找一下，这部分开销怎么办？ 在1.7中该方法不会再复制实例，只是在常量池中记录首次出现的实例引用，所以，是同一个实例。但是！！，在jvm中，类似“java”、“int”这样的好像是字符串常量，就算用户不创建，他也会存在，可能是因为在初始化jvm的源码中用到的，所以使用的时候要注意。 3.2 java8新特性，去永久代在JDK8之前的HotSpot虚拟机中，类的元数据和常量池存放在一个叫做永久代的区域，所谓永久代，只是jvm方法区这个概念在hotspot虚拟机中的一种实现而已，在别的虚拟机实现中，并没有永久代这个概念。方法区是java虚拟机规范去中定义的一种概念上的区域，具有什么功能。由于永久代的存在，也确实引发了一些内存泄露的问题，永久代中的元数据可能会随着每一次Full GC发生而进行移动。并且为永久代设置空间大小也是很难确定的，因为这其中有很多影响因素，比如类的总数，常量池的大小和方法数量等。在java8中取消了永久代，方法区作为概念区域仍然存在，原先的永久代中的类的元信息会被放入本地内存(native memory)即元空间metaspace,类的静态变量和内部字符串会被放入堆中，这样可以加载多少类的元数据就不在由MaxPermSize控制, 而由系统的实际可用空间就是系统可用内存空间来控制。 4. 本机直接内存溢出directmemory导致的内存溢出明显特征就是heap dump文件没有明显异常，若oom后的dump文件很小，程序直接或间接使用了NIO就可能是这个的原因]]></content>
      <categories>
        <category>jvm</category>
      </categories>
      <tags>
        <tag>内存区域</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IOC]]></title>
    <url>%2F2018%2F03%2F03%2FIOC%2F</url>
    <content type="text"><![CDATA[这几天一直在看IOC的相关内容，无奈IOC实在内容太多，自己感觉有点消化不良，尤其是在IOC初始化和依赖注入的源码中有一系列名字超长的类，读了两天感觉自己一脸懵逼，这里简单的写一下自己的总结吧。 一. 概述1. 概念IOC即控制反转，也有叫依赖注入的，对于这二者是否有区别，维基百科上说依赖注入是Martin Fowler这个大神给IOC提出来的新名字。所谓控制反转，也就是把合作对象的引用或依赖关系的控制权反转交给IOC容器。 2. 注入方式最常见的注入方式有三种： 构造器注入 setter注入 自动装配 接口注入 2.1 构造器注入这种方式的注入是指带有参数的构造函数注入，看下面的例子，我创建了两个成员变量SpringDao和User，但是并未设置对象的set方法，所以就不能支持第一种注入方式，这里的注入方式是在SpringAction的构造函数中注入，也就是说在创建SpringAction对象时要将SpringDao和User两个参数值传进来：12345678910111213141516public class SpringAction &#123; //注入对象springDao private SpringDao springDao; private User user; public SpringAction(SpringDao springDao,User user)&#123; this.springDao = springDao; this.user = user; System.out.println("构造方法调用springDao和user"); &#125; public void save()&#123; user.setName("卡卡"); springDao.save(user); &#125; &#125; 在XML文件中同样不用的形式，而是使用标签，ref属性同样指向其它标签的name属性：12345678&lt;!--配置bean,配置后该类由spring管理--&gt; &lt;bean name="springAction" class="com.bless.springdemo.action.SpringAction"&gt; &lt;!--(2)创建构造器注入,如果主类有带参的构造方法则需添加此配置--&gt; &lt;constructor-arg ref="springDao"&gt;&lt;/constructor-arg&gt; &lt;constructor-arg ref="user"&gt;&lt;/constructor-arg&gt; &lt;/bean&gt; &lt;bean name="springDao" class="com.bless.springdemo.dao.impl.SpringDaoImpl"&gt;&lt;/bean&gt; &lt;bean name="user" class="com.bless.springdemo.vo.User"&gt;&lt;/bean&gt; 2.2 setter注入这是最简单的注入方式，假设有一个SpringAction，类中需要实例化一个SpringDao对象，那么就可以定义一个private的SpringDao成员变量，然后创建SpringDao的set方法（这是ioc的注入入口）：12345678910111213public class SpringAction &#123; //注入对象springDao private SpringDao springDao; //一定要写被注入对象的set方法 public void setSpringDao(SpringDao springDao) &#123; this.springDao = springDao; &#125; public void ok()&#123; springDao.ok(); &#125; &#125; 随后编写spring的xml文件，中的name属性是class属性的一个别名，class属性指类的全名，因为在SpringAction中有一个公共属性Springdao，所以要在标签中创建一个标签指定SpringDao。标签中的name就是SpringAction类中的SpringDao属性名，ref指下面，这样其实是spring将SpringDaoImpl对象实例化并且调用SpringAction的setSpringDao方法将SpringDao注入：123456&lt;!--配置bean,配置后该类由spring管理--&gt; &lt;bean name="springAction" class="com.bless.springdemo.action.SpringAction"&gt; &lt;!--(1)依赖注入,配置当前类中相应的属性--&gt; &lt;property name="springDao" ref="springDao"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;bean name="springDao" class="com.bless.springdemo.dao.impl.SpringDaoImpl"&gt;&lt;/bean&gt; 2.3 自动装配 组建扫描（component scanning）：Spring会自动发现应用上下文中锁创建的bean 自动装载（autowiring）：Spring自动满足bean之间的依赖这两个分别对应了注解@Component和@Autowiring @Component：这个注解表明该类会作为组件类，并告知Spring要为这个类创建bean@AutoWiring：可以用在构造方法或set方法上，表明注入一个依赖 2.4 接口注入这种一般就是通过简单工厂或者工厂方法来实现注入，没用过就不写了。 二. 核心类1. BeanFactory和ApplicationContext在Spring的IOC容器中主要就是这两个分支，BeanFactory是最基础的IOC容器，是所有容器的父接口，他只实现了最基础功能，相当于屌丝版，而ApplicationContext是实现了BeanFactory的高富帅版，高级IoC容器，除了基本的IoC容器功能外，支持不同信息源、访问资源、支持事件发布等功能。 继承了以下接口： ListableBeanFactory：继承自BeanFactory，在此基础上，添加了containsBeanDefinition、getBeanDefinitionCount、getBeanDefinitionNames等方法。 HierarchicalBeanFactory：继承自BeanFactory，在此基础之上，添加了getParentBeanFactory、containsLocalBean这两个方法。 AutoWireCapableBeanFactory：继承自BeanFactory MessageSource：用于获取国际化信息 ApplicationEventPublisher：因为ApplicationContext实现了该接口，因此spring的ApplicationContext实例具有发布事件的功能。 2.Resource在Spring内部，针对于资源文件有一个统一的接口Resource表示。其主要实现类有ClassPathResource、FileSystemResource、UrlResource、ByteArrayResource、ServletContextResource和InputStreamResource。Resource接口中主要定义有以下方法： exists()：用于判断对应的资源是否真的存在。 isReadable()：用于判断对应资源的内容是否可读。需要注意的是当其结果为true的时候，其内容未必真的可读，但如果返回false，则其内容必定不可读。 isOpen()：用于判断当前资源是否代表一个已打开的输入流，如果结果为true，则表示当前资源的输入流不可多次读取，而且在读取以后需要对它进行关闭，以防止内存泄露。该方法主要针对于InputStreamResource，实现类中只有它的返回结果为true，其他都为false。 getURL()：返回当前资源对应的URL。如果当前资源不能解析为一个URL则会抛出异常。如ByteArrayResource就不能解析为一个URL。 getFile()：返回当前资源对应的File。如果当前资源不能以绝对路径解析为一个File则会抛出异常。如ByteArrayResource就不能解析为一个File。 getInputStream()：获取当前资源代表的输入流。除了InputStreamResource以外，其它Resource实现类每次调用getInputStream()方法都将返回一个全新的InputStream。 实现类： ClassPathResource可用来获取类路径下的资源文件。假设我们有一个资源文件test.txt在类路径下，我们就可以通过给定对应资源文件在类路径下的路径path来获取它，new ClassPathResource(“test.txt”)。 FileSystemResource可用来获取文件系统里面的资源。我们可以通过对应资源文件的文件路径来构建一个FileSystemResource。FileSystemResource还可以往对应的资源文件里面写内容，当然前提是当前资源文件是可写的，这可以通过其isWritable()方法来判断。FileSystemResource对外开放了对应资源文件的输出流，可以通过getOutputStream()方法获取到。 UrlResource可用来代表URL对应的资源，它对URL做了一个简单的封装。通过给定一个URL地址，我们就能构建一个UrlResource。 ByteArrayResource是针对于字节数组封装的资源，它的构建需要一个字节数组。 ServletContextResource是针对于ServletContext封装的资源，用于访问ServletContext环境下的资源。ServletContextResource持有一个ServletContext的引用，其底层是通过ServletContext的getResource()方法和getResourceAsStream()方法来获取资源的。 InputStreamResource是针对于输入流封装的资源，它的构建需要一个输入流。 3.ResourceLoader通过上面介绍的Resource接口的实现类，我们就可以使用它们各自的构造函数创建符合需求的Resource实例。但是在Spring中提供了ResourceLoader接口，用于实现不同的Resource加载策略，即将不同Resource实例的创建交给ResourceLoader来加载，这也是ApplicationContext等高级容器中使用的策略。 接口中有两个主要的方法： getResource()：在ResourceLoader接口中，主要定义了一个方法：getResource()，它通过提供的资源location参数获取Resource实例，该实例可以是ClasPathResource、FileSystemResource、UrlResource等，但是该方法返回的Resource实例并不保证该Resource一定是存在的，需要调用exists方法判断。 getResourceByPath：这个方法被声明为protected，所以在它的子类中基本都重写了这个方法。 4. BeanDefinition一个BeanDefinition描述了一个bean的实例，包括属性值，构造方法参数值和继承自它的类的更多信息。这个东西会贯穿整个IOC的初始化 三. Bean的注入1. IOC初始化上面说到了BeanDefinition会伴随整个IOC初始化的过程初始化，其实整个IOC容器的初始化过程大致分为以下四个步骤： Resource定位过程 BeanDefinition载入 BeanDefinition解析 BeanDefinition注册 最终配置的bean以BeanDefinition的数据与结构存在于IOC容器之中，这个过程不涉及bean的依赖注入，也不产生任何bean。 1234567891011121314151617181920public class FileSystemXmlApplicationContext extends AbstractXmlApplicationContext &#123; //核心构造器 public FileSystemXmlApplicationContext(String[] configLocations, boolean refresh, ApplicationContext parent) throws BeansException &#123; super(parent); setConfigLocations(configLocations); if (refresh) &#123; refresh();//Ioc容器的refresh()过程，是个非常复杂的过程，但不同的容器实现这里都是相似的，因此基类中就将他们封装好了 &#125; &#125; //通过构造一个FileSystemResource对象来得到一个在文件系统中定位的BeanDefinition //采用模板方法设计模式，具体的实现用子类来完成 @Override protected Resource getResourceByPath(String path) &#123; if (path != null &amp;&amp; path.startsWith("/")) &#123; path = path.substring(1); &#125; return new FileSystemResource(path); &#125;&#125; 这里我们主要看一下这部分12345678public FileSystemXmlApplicationContext(String[] configLocations, boolean refresh, ApplicationContext parent) throws BeansException &#123; super(parent); setConfigLocations(configLocations); if (refresh) &#123; refresh();//Ioc容器的refresh()过程，是个非常复杂的过程，但不同的容器实现这里都是相似的，因此基类中就将他们封装好了 &#125; &#125; 这里我们可以看到调用了一个 refresh()，这个方法在父类AbstractApplicationContext中已经封装好了。它详细描述了整个ApplicationContext的初始化过程，比如BeanFactory的更新、MessageSource和PostProcessor的注册等。这里看起来像是对ApplicationContext进行初始化的模版或执行提纲，这个执行过程为Bean的生命周期管理提供了条件。 refresh为初始化IoC容器的入口，但是具体的资源定位还是在XmlBeanDefinitionReader读入BeanDefinition时完成，loadBeanDefinitions() 加载BeanDefinition的载入。由于源码分析过于冗长我就直接介绍一下每一步的大致思路，如果想看具体的分析可以参考http://www.cnblogs.com/ITtangtang/p/3978349.html，这篇文章比较详细。这里直接给出一个流程性的总结： 首先需要获得一个IOC容器才能操作其控制的Bean，对于IOC容器的初始化来说，他通过一个refresh()函数作为开头，首先判断是否已经创建了BeanFactory，如果创建了则销毁关闭该BeanFactory，接着会创建相应的读取器(Reader),通过相应loadBeanDefinitions函数获取资源定位 对xml的资源文件的加载将他们转换为一个Document对象进行处理，载入过程实际上就是Resource对象转换成Document对象的过程，也就是一个XML文件解析的过程，Spring中使用的是JAXP解析，生成的Document文件就是org.w3c.dom.Document中的Document文件； 就是解析相应的Document对象，这个没有什么好说的类似于用DOM解析xml文件 向IoC容器注册解析的BeanDefiniton ，完成前面的步骤用户定义的BeanDefiniton已经在IoC容器里建立相应的数据结构和表示，但不能直接使用，需要进行注册，简单来说就是通过一个ConcurrentHashMap存储。 2. 依赖注入假设我们已经完成了IoC容器的初始化，首先要注意到的一点，Spring中的依赖注入是lazy-loading，即用户第一次向容器索要Bean，调用相应的getBean()函数，但是也能通过设置Bean的lazy-init属性来控制预实例化过程，这个预实例化在初始化容器时完成Bean的依赖注入。 他的主要流程也就下面这几个步骤： AbstractBeanFactory中的getBean方法来获取Bean： 在缓存中查找，如果存在则直接返回，否则开始下一步创建Bean AbstractAutowireCapableBeanFactory类中创建Bean，这个类中有几个关键方法： createBean方法：创建容器指定的Bean实例对象的入口 同时还对创建的Bean实例对象进行初始化处理比如init-method、后置处理器等，然后调用下面的两个方法创建实例并注入依赖 2. createBeanInstance方法：创建Bean的Java实例对象，分两种情况： 对于使用工厂方法和自动装配特性的bean的实例化：则调用对应的工厂方法或者参数匹配的构造方法即可完成实例化对象的工作否则调用默认的无参构造器进行实例化即SimpleInstantiationStrategy的instantiate方法: 1. 使用Java的反射技术 2. 使用CGLIB populateBean方法：实例化之后，根据属性类型决定是否需要解析，最后通过BeanWrapperImpl类完成对属性的注入，对属性的类型也要进行判断： 属性值类型不需要转换时，不需要解析属性值，直接准备进行依赖注入 属性值需要进行类型转换时，如对其他对象的引用等，首先需要解析属性值，然后对解析后的属性值进行依赖注入。解析过程由BeanDefinitionValueResolver类的setPropertyValue方法完成 BeanWrapperImpl对Bean属性的依赖注入： 对于集合类属性，将其属性值解析为目标类型的集合后直接赋值给属性。 对于非集合类型的属性，大量使用了JDK的反射和内省机制，通过属性的getter方法(reader method)获取指定属性注入以前的值，同时调用属性的setter方法(writer method)为属性设置注入后的值。 总结一下就是先判断是否有缓存，有的话直接取，没有就创建实例，接着会判断bean是否能被实例化，以及实例化这个bean的方法，根据相应策略来创建bean，之后填充属性完成创建并返回。 3. 循环依赖循环依赖就是循环引用，就是两个或多个Bean相互之间的持有对方，比如CircleA引用CircleB，CircleB引用CircleC，CircleC引用CircleA，则它们最终反映为一个环。此处不是循环调用，循环调用是方法之间的环调用。循环调用是无法解决的，除非有终结条件，否则就是死循环，最终导致内存溢出错误。Spring容器循环依赖包括构造器循环依赖和setter循环依赖，那Spring容器如何解决循环依赖呢？首先让我们来定义循环引用类：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package cn.javass.spring.chapter3.bean; public class CircleA &#123; private CircleB circleB; public CircleA() &#123; &#125; public CircleA(CircleB circleB) &#123; this.circleB = circleB; &#125; public void setCircleB(CircleB circleB) &#123; this.circleB = circleB; &#125; public void a() &#123; circleB.b(); &#125; &#125; package cn.javass.spring.chapter3.bean; public class CircleB &#123; private CircleC circleC; public CircleB() &#123; &#125; public CircleB(CircleC circleC) &#123; this.circleC = circleC; &#125; public void setCircleC(CircleC circleC) &#123; this.circleC = circleC; &#125; public void b() &#123; circleC.c(); &#125; &#125; package cn.javass.spring.chapter3.bean; public class CircleC &#123; private CircleA circleA; public CircleC() &#123; &#125; public CircleC(CircleA circleA) &#123; this.circleA = circleA; &#125; public void setCircleA(CircleA circleA) &#123; this.circleA = circleA; &#125; public void c() &#123; circleA.a(); &#125; &#125; 1.构造器循环依赖表示通过构造器注入构成的循环依赖，此依赖是无法解决的，只能抛出BeanCurrentlyInCreationException异常表示循环依赖。 如在创建CircleA类时，构造器需要CircleB类，那将去创建CircleB，在创建CircleB类时又发现需要CircleC类，则又去创建CircleC，最终在创建CircleC时发现又需要CircleA；从而形成一个环，没办法创建。Spring容器将每一个正在创建的Bean 标识符放在一个“当前创建Bean池”中，Bean标识符在创建过程中将一直保持在这个池中，因此如果在创建Bean过程中发现自己已经在“当前创建Bean池”里时将抛出BeanCurrentlyInCreationException异常表示循环依赖；而对于创建完毕的Bean将从“当前创建Bean池”中清除掉。这个调用会是这样的流程： Spring容器创建“circleA” Bean，首先去“当前创建Bean池”查找是否当前Bean正在创建，如果没发现，则继续准备其需要的构造器参数“circleB”，并将“circleA” 标识符放到“当前创建Bean池”； Spring容器创建“circleB” Bean，首先去“当前创建Bean池”查找是否当前Bean正在创建，如果没发现，则继续准备其需要的构造器参数“circleC”，并将“circleB” 标识符放到“当前创建Bean池”；S pring容器创建“circleC” Bean，首先去“当前创建Bean池”查找是否当前Bean正在创建，如果没发现，则继续准备其需要的构造器参数“circleA”，并将“circleC” 标识符放到“当前创建Bean池”； 到此为止Spring容器要去创建“circleA”Bean，发现该Bean 标识符在“当前创建Bean池”中，因为表示循环依赖，抛出BeanCurrentlyInCreationException。 1.setter循环依赖对于setter注入造成的依赖是通过Spring容器提前暴露刚完成构造器注入但未完成其他步骤（如setter注入）的Bean来完成的，而且只能解决单例作用域的Bean循环依赖。具体步骤如下： Spring容器创建单例“circleA” Bean，首先根据无参构造器创建Bean，并暴露一个“ObjectFactory ”用于返回一个提前暴露一个创建中的Bean，并将“circleA” 标识符放到“当前创建Bean池”；然后进行setter注入“circleB”； Spring容器创建单例“circleB” Bean，首先根据无参构造器创建Bean，并暴露一个“ObjectFactory”用于返回一个提前暴露一个创建中的Bean，并将“circleB” 标识符放到“当前创建Bean池”，然后进行setter注入“circleC”； Spring容器创建单例“circleC” Bean，首先根据无参构造器创建Bean，并暴露一个“ObjectFactory ”用于返回一个提前暴露一个创建中的Bean，并将“circleC” 标识符放到“当前创建Bean池”，然后进行setter注入“circleA”；进行注入“circleA”时由于提前暴露了“ObjectFactory”工厂从而使用它返回提前暴露一个创建中的Bean； 最后在依赖注入“circleB”和“circleA”，完成setter注入。 对于“prototype”作用域Bean，Spring容器无法完成依赖注入，因为“prototype”作用域的Bean，Spring容器不进行缓存，因此无法提前暴露一个创建中的Bean。 四. 参考 http://paine1690.github.io/2017/01/02/spring/Spring%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90(2)%E4%BE%9D%E8%B5%96%E6%B3%A8%E5%85%A5%E7%9A%84%E5%AE%9E%E7%8E%B0/http://blog.battcn.com/2018/01/17/spring/spring-4/#morehttp://www.cnblogs.com/ITtangtang/p/3978349.htmlhttp://jinnianshilongnian.iteye.com/blog/1415278]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>源码分析</tag>
        <tag>IOC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AOP]]></title>
    <url>%2F2018%2F02%2F28%2FAOP%2F</url>
    <content type="text"><![CDATA[引入AOP（Aspect Oriented Programming），即面向切面编程，可以说是OOP（Object Oriented Programming，面向对象编程）的补充和完善。OOP引入封装、继承、多态等概念来建立一种对象层次结构，用于模拟公共行为的一个集合。不过OOP允许开发者定义纵向的关系，但并不适合定义横向的关系，例如日志功能。日志代码往往横向地散布在所有对象层次中，而与它对应的对象的核心功能毫无关系对于其他类型的代码，如安全性、异常处理和透明的持续性也都是如此，这种散布在各处的无关的代码被称为横切（cross cutting），在OOP设计中，它导致了大量代码的重复，而不利于各个模块的重用。 AOP技术恰恰相反，它利用一种称为”横切”的技术，剖解开封装的对象内部，并将那些影响了多个类的公共行为封装到一个可重用模块，并将其命名为”Aspect”，即切面。所谓”切面”，简单说就是那些与业务无关，却为业务模块所共同调用的逻辑或责任封装起来，便于减少系统的重复代码，降低模块之间的耦合度，并有利于未来的可操作性和可维护性。 使用”横切”技术，AOP把软件系统分为两个部分：核心关注点和横切关注点。业务处理的主要流程是核心关注点，与之关系不大的部分是横切关注点。横切关注点的一个特点是，他们经常发生在核心关注点的多处，而各处基本相似，比如权限认证、日志、事物。AOP的作用在于分离系统中的各种关注点，将核心关注点和横切关注点分离开来。 一.术语1.横切关注点软件开发中，散布于应用中多处的功能被称为横切关注点，如事务、日志、安全。 2.advice(通知） what when切面的工作被称为通知，他定义了切面的工作是什么和何时工作 5种类型通知: 前置通知before 后置通知after(不关心方法的输出是什么) 返回通知after-returning(方法成功执行) 异常通知after-throwing(方法返回异常) 环绕通知around(包裹被通知方法，之前和之后都自定义行为) 3.join point连接点应用执行过程中，能够插入切面的所有“点”（时机），使用通知的时机，即触发通知的事件方法 4.poincut 切点 where如果说通知定义切面的“什么”和“何时”，那么切点就定义了“何处”。切点的定义会匹配通知所要织入的一个或多个连接点。我们通常会使用明确的类和方法名称来指定这些切点，或是利用正则表达式定义匹配的类和方法名称模式来指定这些切点。 5.切面aspect = advice + poincut切面是通知和切点的结合，通知和切点共同定义了关于切面的全部内容—-它是什么，在何时和何处完成其功能 通过 代理 来实现切面，代理类包裹了目标bean，先拦截对被通知方法的调用，再转发给真正的bean，在调用bean之前，先执行切面逻辑。直到需要被代理的bean时，才创建代理对象。正因为spring会在运行时才创建代理对象，所以不需要特殊的编译器织入springAOP切面。 定义切面@AspectJ ：表明该类不仅是一个pojo还是以个切面 @Pointcut 在一个@Aspect切面内定义可重用的切点 @Pointcut（“execution（ concert.Performance.perform(..))”)public void performance() {}将切点依附于performance（）方法，可以在定义切点表达式的时候用performance（）代替execution（ concert.Performance.perform(..)) 环绕通知1234567891011121314151617@Aspectpublic class Audience &#123; @Pointcut("excution(** concert.Performance.perform(..))") public void performance() &#123;&#125; @Around("performance()") public void watchPerformance(ProceedingJoinPoint jp) &#123; try &#123; System.out.println("Silencing cell phones"); System.out.pringln("Taking seats"); jp.proceed(): Sytem.out.println("CLAP CLAP CLAP!!!"); &#125; catch (THrowable e) &#123; SYstem.out.println("Demanding a refund"); &#125; &#125;&#125; 如果不调用proceed（）方法，那么会阻塞被通知方法的调用。可以不调用proceed（）方法，也可以多次调用proceed（）方法，这样的场景就是实现重试逻辑，也就是在被通知方法失败后，进行重复尝试。 切点拦截参数1234567891011@Pointcut("excution(** concert.Performance.perform(..)) &amp;&amp; args(trackNumber") public void trackPlayed(int trackNumber) &#123;&#125; @Before("trackPlayed(trackNumber)") public void countTrack(int trackNumber) &#123; int currentCount = getPlayCount(trackNumber); trackCounts.put(trackNumber, currentCount + 1); &#125; public int getPlayCount(int trackNumber) &#123; return trackCounts.containsKey(trackNumber) ? trackCounts.get(trackNumber) : 0; &#125; 切点定义中的参数与切点方法中的参数名一样就可以实现从命名切点到通知方法的参数转移。 6.Introduction引入向现有的类添加新方法和属性。 给bean添加一个新的功能，需要一个引入代理去实现相关的功能。12345@Aspect public class EncoreableIntroducer &#123; @DeclareParents(value="concert.Performance+", defualtImpl=DefaultEncoreable.class) public static Encoreable ecoreable;&#125; value指定了哪种类型的bean要引入该接口。在这里是所有实现Performance的类型。加号表示Performance的所有子类而不是本身 defaultImplement指定了为引入功能提供实现的类 实现类 @DeclareParents注解所标注的静态属性指明了要 引入的接口。 7.织入把切面应用到目标对象并创建新的代理对象的过程。在目标生命周期有多个点可以进行织入： 编译期 类加载期 运行期 二.spring对AOP的支持Spring默认采取的动态代理机制实现AOP，当动态代理不可用时（代理类无接口）会使用CGlib机制。但Spring的AOP有一定的缺点，第一个只能对方法进行切入，不能对接口，字段，静态代码块进行切入（切入接口的某个方法，则该接口下所有实现类的该方法将被切入）。第二个同类中的互相调用方法将不会使用代理类。因为要使用代理类必须从Spring容器中获取Bean。第三个性能不是最好的 基于代理的经典SpringAOP 纯POJO切面 @AspectJ注解驱动的切面 注入式AspectJ切面 由于Spring AOP构建在动态代理基础之上，所以其对AOP的支持局限于方法拦截。他不支持字段连接点和构造器连接点。 直到应用需要被代理的bean时，spring才会创建代理对象。如果使用的是ApplicationContext的话，在ApplicationContext从BeanFactory中加载所有bean的时候，Spring才会创建被dialing的对象。 动态代理不修改原来的对象就能增加一系列新的功能 jdk自带动态代理 Java在JDK1.3后引入的动态代理机制，使我们可以在运行期,目标类加载后动态的创建代理类 CGlib 使用动态字节码生成技术实现AOP原理是在运行期间目标字节码加载后，生成目标类的子类，将切面逻辑加入到子类中，所以使用Cglib实现AOP不需要基于接口。 JDK中动态代理实现流程在java的动态代理机制中，有两个重要的类和接口，一个是 InvocationHandler(Interface)、另一个则是 Proxy(Class)，这一个类和接口是实现我们动态代理所必须用到的。 使用方法： 创建调用处理器（InvocationHandler)，在其中传入被代理对象 在调用处理器的invoke方法中加入代理逻辑，并调用被代理对象的被代理方法 通过Proxy.newProxyInstance方法获得代理对象 12345678910111213141516171819202122232425262728293031class DynamicProxyHandler implements InvocationHandler &#123; private Object proxied; public DynamicProxyHandler(Object proxied) &#123; this.proxied = proxied; &#125; public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; System.out.println("**** proxy: " + proxy.getClass() + ", method: " + method + ", args: " + args); if(args != null) for(Object arg : args) System.out.println(" " + arg); return method.invoke(proxied, args); &#125;&#125;class SimpleDynamicProxy &#123; public static void consumer(Interface iface) &#123; iface.doSomething(); iface.somethingElse("bonobo"); &#125; public static void main(String[] args) &#123; RealObject real = new RealObject(); Interface proxy = (Interface)Proxy.newProxyInstance( Interface.class.getClassLoader(), new Class[]&#123; Interface.class &#125;, new DynamicProxyHandler(real)); consumer(proxy)； &#125; Proxy.newProxyInstance方法有三个参数，分别是类加载器，一个希望该代理实现的接口列表以及传入他的被代理对象的调用处理器。 通过 Proxy.newProxyInstance创建的代理对象是在jvm运行时动态生成的一个对象，它并不是我们的InvocationHandler类型，也不是我们定义的那组接口的类型，而是在运行是动态生成的一个对象，并且命名方式都是这样的形式，以$开头，proxy为中，最后一个数字表示对象的标号。 总体逻辑大概是：通过InvocationHandler来包装被代理的方法，再根据（InvocationHandler）和需要代理的接口生成相应的代理对象，通过将相应的调用转发到代理对象，从而实现功能的包装。 由于在Proxy.newProxyInstance中，所有的方法都是通过h.invoke()实现的，这样就让所有的接口方法都被包裹上代理逻辑，也就是说当执行被代理操作的时候在代理对象内部，实际上是使用invoke()方法将请求转发给了调用处理器进行操作。 静态代理既然说了动态代理，那我们就顺便提一下静态代理，虽然他没有动态代理那么好。1234567891011121314151617181920212223242526272829303132333435363738package test;public interface Subject &#123; public void doSomething(); &#125;package test;public class RealSubject implements Subject &#123; public void doSomething() &#123; System.out.println( "call doSomething()" ); &#125; &#125; package test;public class SubjectProxy implements Subject&#123; Subject subimpl = new RealSubject(); public void doSomething() &#123; //可以在这里加入代理逻辑 subimpl.doSomething(); //可以在这里加入代理逻辑 &#125;&#125;package test;public class TestProxy &#123; public static void main(String args[]) &#123; Subject sub = new SubjectProxy(); sub.doSomething(); &#125;&#125; 刚开始我会觉得SubjectProxy定义出来纯属多余，直接实例化实现类完成操作不就结了吗？后来随着业务庞大，你就会知道，实现proxy类对真实类的封装对于粒度的控制有着重要的意义。但是静态代理这个模式本身有个大问题，如果类方法数量越来越多的时候，代理类的代码量是十分庞大的。所以引入动态代理来解决此类问题。 动态代理相对于静态的好处： Proxy类的代码量被固定下来，不会因为业务的逐渐庞大而庞大； 可以实现AOP编程，实际上静态代理也可以实现，总的来说，AOP可以算作是代理模式的一个典型应用； 解耦，通过参数就可以判断真实类，不需要事先实例化，更加灵活多变。这也是为什么Spring这么受欢迎的一个原因，Spring容器代替工厂，Spring AOP代替JDK动态代理，让面向切面编程更容易实现。在Spring的帮助下轻松添加，移除动态代理，且对源代码无任何影响。]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>源码分析</tag>
        <tag>aop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashMap源码解析]]></title>
    <url>%2F2018%2F02%2F24%2FHashMap%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[〇.简介Java为数据结构中的映射定义了一个接口java.util.Map，此接口主要有四个常用的实现类，分别是HashMap、Hashtable、LinkedHashMap和TreeMap，类继承关系如下图所示： 下面针对各个实现类的特点做一些说明： HashMap：它根据键的hashCode值存储数据，大多数情况下可以直接定位到它的值，因而具有很快的访问速度，但遍历顺序却是不确定的。 HashMap最多只允许一条记录的键为null，允许多条记录的值为null。HashMap非线程安全，即任一时刻可以有多个线程同时写HashMap，可能会导致数据的不一致。如果需要满足线程安全，可以用 Collections的synchronizedMap方法使HashMap具有线程安全的能力，或者使用ConcurrentHashMap。 Hashtable：Hashtable是遗留类，很多映射的常用功能与HashMap类似，不同的是它承自Dictionary类，并且是线程安全的，任一时间只有一个线程能写Hashtable，并发性不如ConcurrentHashMap，因为ConcurrentHashMap引入了分段锁。Hashtable不建议在新代码中使用，不需要线程安全的场合可以用HashMap替换，需要线程安全的场合可以用ConcurrentHashMap替换。 LinkedHashMap：LinkedHashMap是HashMap的一个子类，保存了记录的插入顺序，在用Iterator遍历LinkedHashMap时，先得到的记录肯定是先插入的，也可以在构造时带参数，按照访问次序排序。 TreeMap：TreeMap实现SortedMap接口，能够把它保存的记录根据键排序，默认是按键值的升序排序，也可以指定排序的比较器，当用Iterator遍历TreeMap时，得到的记录是排过序的。如果使用排序的映射，建议使用TreeMap。在使用TreeMap时，key必须实现Comparable接口或者在构造TreeMap传入自定义的Comparator，否则会在运行时抛出java.lang.ClassCastException类型的异常。 对于上述四种Map类型的类，要求映射中的key是不可变对象。不可变对象是该对象在创建后它的哈希值不会被改变。如果对象的哈希值发生变化，Map对象很可能就定位不到映射的位置了。 在JDK8中HashMap的实现由原先的数组加链表，也就是通过链地址法，解决哈希冲突，变成了数组加链表加红黑树，如图所示 当链表长度大于8时，链表会变成红黑树，这样做的目的是为了改进之前由于hash函数选择不好导致链表过长的查询瓶颈，之后会具体介绍。 一. 关键参数及构造方法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111/* ---------------- Fields -------------- */ //存储节点的table数组，第一次使用的时候初始化，必要时resize，长度总是2的幂 transient Node&lt;K,V&gt;[] table; //缓存entrySet，用于keySet() and values() transient Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet; //容器中元素的个数 transient int size; //每次扩容和更改map结构的计数器 transient int modCount; //阈值，当实际大小超过阈值时，会进行扩容 int threshold; //装载因子 final float loadFactor; //默认的初始容量，必须是2的幂次，出于优化考虑，默认16 static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; //默认的最大容量 static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; //默认的装载因子，在无参构造器中默认设为该值 static final float DEFAULT_LOAD_FACTOR = 0.75f; //阈值，当链表中节点数大于该阈值后就会转变成红黑树 static final int TREEIFY_THRESHOLD = 8; //与上一个阈值相反，当小于这个阈值后转变回链表 static final int UNTREEIFY_THRESHOLD = 6; // 看源码注释里说是：树的最小的容量，至少是 4 x TREEIFY_THRESHOLD = 32 然后为了避免(resizing 和 treeification thresholds) 设置成64 static final int MIN_TREEIFY_CAPACITY = 64; //基本哈希容器节点 实现Map.Entry接口 static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash;//不可变的哈希值，由关键字key得来 final K key;//不可变的关键字 V value; Node&lt;K,V&gt; next; Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return value; &#125; public final String toString() &#123; return key + "=" + value; &#125; public final int hashCode() &#123;//Node对象的哈希值，关键字key的hashCode()与值value的hashCode()做异或运算 return Objects.hashCode(key) ^ Objects.hashCode(value); &#125; public final V setValue(V newValue) &#123; V oldValue = value; value = newValue; return oldValue; &#125; public final boolean equals(Object o) &#123;//对象相同或同类型且key-value均相同，则返回true if (o == this) return true; if (o instanceof Map.Entry) &#123; Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;)o; if (Objects.equals(key, e.getKey()) &amp;&amp; Objects.equals(value, e.getValue())) return true; &#125; return false; &#125; &#125; /* * 构造函数 */ public HashMap(int initialCapacity, float loadFactor) &#123;//给定初始容量和装载因子，构造一个空的HashMap if (initialCapacity &lt; 0) throw new IllegalArgumentException("Illegal initial capacity: " + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException("Illegal load factor: " + loadFactor); this.loadFactor = loadFactor; this.threshold = tableSizeFor(initialCapacity);//根据指定的容量计算容量，因为必须是2的幂次，虽然将该值赋给threshold，但表示的依然是容量，到时候会重新计算阈值 &#125; public HashMap(int initialCapacity) &#123;//指定初始容量，和默认装载因子0.75构造空HashMap this(initialCapacity, DEFAULT_LOAD_FACTOR); &#125; public HashMap() &#123;//无参，使用默认的初始容量16,和装载因子0.75构造空的HashMap this.loadFactor = DEFAULT_LOAD_FACTOR; &#125; public HashMap(Map&lt;? extends K, ? extends V&gt; m) &#123;//构造一个和给定Map映射相同的HashMap，默认装载因子，初始空间以足够存放给定Map中的映射为准 this.loadFactor = DEFAULT_LOAD_FACTOR; putMapEntries(m, false); &#125; &#125; 注释中给出了相应解释，下面再着重介绍几个参数。 1. table数组也就是我们之前所说的HashMap实现中的数组，通过对key计算hash后得到相应数组的index，数组中存储着相同index的链表首结点或者红黑树的根节点，结点类型就是上面代码中的Node 2. 容量， 装载因子，阈值threshold = loadFactor * capcity 由于这样的一个对应关系，这三个变量在HashMap中只有threshold和loadFactor这两个是明确给出来的。在给出初始容量和装载因子的构造函数中我们可以发现，threshold被作为了初始化的容量变量，他将在第一次调用resize()中被使用。123456789101112public HashMap(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException("Illegal initial capacity: " + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException("Illegal load factor: " + loadFactor); this.loadFactor = loadFactor; this.threshold = tableSizeFor(initialCapacity);&#125; 容量的默认值是16，装载因子默认为0.75, 至于为什么要有装载因子这个设定，而不是在table数组满了再扩容，文档中是这么说的 As a general rule, the default load factor (.75) offers a good tradeoff between time and space costs. Higher values decrease the space overhead but increase the lookup cost (reflected in most of the operations of the HashMap class, including get and put). The expected number of entries in the map and its load factor should be taken into account when setting its initial capacity, so as to minimize the number of rehash operations. If the initial capacity is greater than the maximum number of entries divided by the load factor, no rehash operations will ever occur. 意思总结差不多就是这是时间和空间均衡后的决定。我们也知道hashmap是用空间换时间，0.75这个装载因子能在二者之间达到一个比较好的平衡。反正就是我们不要乱改就好了。 二. 关键方法1.hash()在对hashCode()计算hash时具体实现是这样的：1234static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; 他的执行过程如图所示：其实就是将本身的hashcode的高16位和低16位做了一个异或操作。至于为什么要这么做呢？这里牵扯到了table数组中index的计算。 1index = (n - 1) &amp; hash n为数组的长度，而table长度n为2的幂，而计算table数组下标的时候，举个例子，加入n=16，那么n-1=15的二进制表示就是0x0000 1111，可以看出，任何一个2的幂次减1后二进制肯定都是这种形式，它的意义在于，任何一个值和它做&amp;操作，得到的结构肯定都在0~(n-1)之间，也就是说计算出来的下标值肯定数组的合法下标，这种方式由于使用了位运算比单纯的取模更快。 但问题也来了，设计者认为这方法很容易发生碰撞。为什么这么说呢？不妨思考一下，在n - 1为15(0x1111)时，其实散列真正生效的只是低4bit的有效位，当然容易碰撞了。 因此，设计者想了一个顾全大局的方法(综合考虑了速度、作用、质量)，就是把高16bit和低16bit异或了一下。设计者还解释到因为现在大多数的hashCode的分布已经很不错了，就算是发生了碰撞也用O(logn)的tree去做了。仅仅异或一下，既减少了系统的开销，也不会造成的因为高位没有参与下标的计算(table长度比较小时)，从而引起的碰撞。 2.putput方法的流程如下： 如果table数组为空，那么调用resize()方法新建数组 对key的hashCode()做hash，然后再计算index; 如果没碰撞直接放到bucket里； 如果碰撞了，放在以链表的形式存在buckets后； 如果碰撞导致链表过长(大于等于TREEIFY_THRESHOLD)，就把链表转换成红黑树； 如果节点已经存在就替换old value(保证key的唯一性) 如果bucket满了(超过load factor*current capacity)，就要resize12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true); &#125; /* * 实现Map.put以及相关方法 * 向map中加入个节点 * 没有分析onlyIfAbsent和evict */ final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K, V&gt;[] tab;//指向table数组 Node&lt;K, V&gt; p;//对应下标中的第一个节点，为null说明没有碰撞，不为null代表链表第一个元素或红黑树根节点 int n, i;//n为table数组的长度，2的幂次; i表示对应的下标index if ((tab = table) == null || (n = tab.length) == 0) // 如果table为空即第一次添加元素，则进行初始化 n = (tab = resize()).length; /* * 计算下标，根据hash与n计算index * 公式:i = (n - 1) &amp; hash; */ // p=table[i]; 对应下标中的第一个节点 if ((p = tab[i = (n - 1) &amp; hash]) == null) // p为null说明没有碰撞， tab[i] = newNode(hash, key, value, null);//直接新建一个节点加入就可以了 else &#123;// p不为null，说明有碰撞 Node&lt;K, V&gt; e;//e，代表map中与给定key值相同的节点 K k;//代表e的key // p的关键字与要加入的关键字相同，则p就是要找的e if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; // 如果p的类型是红黑树，则向红黑树中查找e else if (p instanceof TreeNode) e = ((TreeNode&lt;K, V&gt;) p).putTreeVal(this, tab, hash, key, value); // 否则就是链表 else &#123; for (int binCount = 0;; ++binCount) &#123;//遍历链表查找e，如果找不到就新建一个 if ((e = p.next) == null) &#123;// 如果next为null，说明没有找到 p.next = newNode(hash, key, value, null);// 那么新创建一个节点 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // 加入节点后如果超出树形化阈值 treeifyBin(tab, hash);// 则转换为红黑树 break; &#125; if (e.hash == hash &amp;&amp; // 找到关键字相同的节点，退出循环 ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; if (e != null) &#123; //e不为null，说明原来存在对应的key，那么返回原来的值 V oldValue = e.value;// 保留原来的值，用于返回 if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; //说明新插入了一个节点，返回null ++modCount; if (++size &gt; threshold) // 超过临界值，则resize resize(); afterNodeInsertion(evict); return null; &#125; 3. get() 和 containsKey()方法这两个方法是最常用的，都是根据给定的key值，一个获取对应的value，一个判断是否存在于Map中，在内部这两个方法都会调用一个finall方法，就是getNode()，也就是查找对应key值的节点。 getNode方法的大致过程： table里的第一个节点，直接命中； 如果有冲突，则遍历链表或二叉树去查找相同节点 查找节点时先判断hash值是否相等 如果hash值相等，再判断key值是否相等 判断key值相等时，用==或equals或，整个判断条件为： (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k))))12345678910111213141516171819202122232425262728293031public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value; &#125; public boolean containsKey(Object key) &#123; return getNode(hash(key), key) != null; &#125; /* * 实现Map.get以及相关方法 */ final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; //指向table数组 Node&lt;K,V&gt; first, e; //first为table[index]，即所在数组下标中第一个节点；e用于遍历节点 int n; K k;//n为table的长度，k用于指向节点的key if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp;//首先必须保证table数组不为空 (first = tab[(n - 1) &amp; hash]) != null) &#123;//计算下标，保证数组下标中第一个节点不为null不然就肯定找不到直接返回null if (first.hash == hash &amp;&amp; // 先检查第一个节点hash值是否相等 ((k = first.key) == key || (key != null &amp;&amp; key.equals(k))))//再判断key，如果相等直接返回 return first; if ((e = first.next) != null) &#123; //第一个不符合，就从下一个开始找 if (first instanceof TreeNode)//红黑树 O(logn) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); do &#123;//不然就是链表O(n) if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null; &#125; 4. resize()从put函数我们不难看出，当table数组为空，或者当加入某个元素后超过阈值，都会调用resize()进行扩容，他的目的就在于将链表和红黑树分散，使得碰撞分散，提高查询效率。简单来说就是下面的步骤： 将容量和阈值扩大两倍，如果超过最大值就使用最大值最为新的容量和阈值 新建一个大小为新容量的table，然后将之前的结点放进去 这里有一个很有意思的小技巧，还记得我们的index是怎么计算的吗？1index = (n - 1) &amp; hash 假设我们的容量没有超标，由于容量都是2的幂，这里的n扩大2倍，相当于在原来的n-1的基础上高位增加了一个1，说白了就是多取了一位的hash。如图所示所以，元素的位置要么是在原位置，要么是在原位置再移动2次幂的位置，因此元素在重新计算hash之后，因为n变为2倍，那么n-1的mask范围在高位多1bit(红色)，因此新的index就会发生这样的变化： 因此，我们在扩充HashMap的时候，不需要重新计算hash，只需要看看原来的hash值新增的那个bit是1还是0就好了，是0的话索引没变，是1的话索引变成“原索引+oldCap”。可以看看下图为16扩充为32的resize示意图：这个设计确实非常的巧妙，既省去了重新计算hash值的时间，而且同时，由于新增的1bit是0还是1可以认为是随机的，因此resize的过程，均匀的把之前的冲突的节点分散到新的bucket了。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; /* * step1: 先根据容量和阈值确定新的容量和阈值 */ //case1: 如果table已经被初始化，说明不是第一次加入元素 if (oldCap &gt; 0) &#123; if (oldCap &gt;= MAXIMUM_CAPACITY) &#123;//如果table的容量已经达到最大值，那么就不再扩容了，碰撞也没办法 threshold = Integer.MAX_VALUE;//那么扩大阈值到最大值 return oldTab;//原来的table不变 &#125; //不然的话table的容量扩大2倍，newCap = oldCap &lt;&lt; 1 大部分情况下肯定都是这种情况 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; //阈值也扩大2倍 &#125; //case2: table没有被初始化，但是阈值大于0，说明在构造函数中指定了容量，但是容量存在阈值那个变量上 else if (oldThr &gt; 0) newCap = oldThr;//那么将阈值设置为table的容量，下面还会重新计算阈值 //case3: table和阈值都没有初始化，说明是无参构造函数 else &#123; newCap = DEFAULT_INITIAL_CAPACITY;//使用默认的初始容量 newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);//计算默认的阈值，threshold=load_factor*capacity &#125; //重新计算阈值 if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; /* * step2: 更新阈值和新容量的table */ threshold = newThr; @SuppressWarnings(&#123;"rawtypes","unchecked"&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; /* * step3: 将原来table中元素，加入到新的table中 */ if (oldTab != null) &#123; for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; //e = oldTab[j] if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; if (e.next == null) //e所在位置没有哈希冲突，只有一个元素，直接计算 newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) //e所在位置是一颗红黑树 ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123;// e所在位置是一个链表，则遍历链表 // 根据e.hash &amp; oldCap) == 0，确定放入lo还是hi两个链表 // 其实就是判断e.hash是否大于oldCap // lo和hi两个链表放分别放在在newTab[j]和newTab[j + oldCap] Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab; &#125; 三. 性能探讨1.hashcode()HashMap的查询效率我们理论上看做是O(1),这是在没有发生冲突的情况下，但是当发生冲突较严重的时候，我们会浪费很多的时间在链表的查询或者红黑树的查询，以至于退化为O(n)或者O(logn),所以当作为key的类型，其Hashcode()函数的设计尤为重要。 2. Key的要求由上一条可知，一个作为key的类型，首先需要有一个设计良好的Hashcode函数，其次我们发现，在get函数中，我们首先判断hashcode相等，再判断equals()或者==来判断是否为同一个对象，因为hashcode相等的两个对象不一定相等，由此可见作为key的另一个条件时重写了equals方法。最后还有一个隐藏条件，key需要为不可变对象比如String，什么叫不可变对象呢？不可变对象就是创建后状态不能修改的对象，因为只有这样才能确保hashcode不发生变化，才能保证能找到相应的key，总结起来就是一下三个： 重写hashcode（） 重写equals（） 不可变对象 四. 引用 https://tech.meituan.com/java-hashmap.htmlhttp://paine1690.github.io/2016/11/12/Java/JDK/HashMap%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/http://yikun.github.io/2015/04/01/Java-HashMap%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>源码分析</tag>
        <tag>集合</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[缓存穿透、缓存雪崩、hot key]]></title>
    <url>%2F2018%2F02%2F03%2F%E7%BC%93%E5%AD%98%E7%A9%BF%E9%80%8F%E3%80%81%E7%BC%93%E5%AD%98%E9%9B%AA%E5%B4%A9%E3%80%81hot-key%2F</url>
    <content type="text"><![CDATA[在做排行榜的时候，对缓存的更新频率产生了一定的疑问，在网上也看了不少博客对这方面的介绍，这里对看到的知识做个总结。 缓存穿透缓存穿透是指查询一个一定不存在的数据，由于缓存是不命中时要查询数据库，并且出于容错考虑，如果从存储层查不到数据则不写入缓存，这将导致这个不存在的数据每次请求都要到存储层去查询，失去了缓存的意义。在流量大时，可能DB就挂掉了，要是有人利用不存在的key频繁攻击我们的应用，这就是漏洞 解决方法1. 空值缓存这是一个比较简单暴力的方法，如果一个查询返回的数据为空（不管是数据不存在，还是系统故障），我们仍然把这个空结果进行缓存，但它的过期时间会很短，最长不超过五分钟。 缺点 空值做了缓存，意味着缓存层中存了更多的键，需要更多的内存空间 ( 如果是攻击，问题更严重 )，比较有效的方法是针对这类数据设置一个较短的过期时间，让其自动剔除。 缓存层和存储层的数据会有一段时间窗口的不一致，可能会对业务有一定影响。例如过期时间设置为 5 分钟，如果此时存储层添加了这个数据，那此段时间就会出现缓存层和存储层数据的不一致，此时可以利用消息系统或者其他方式清除掉缓存层中的空对象。 2. Bloom FilterBloom Filter是一个占用空间很小、效率很高的随机数据结构，它由一个bit数组和一组Hash算法构成。可用于判断一个元素是否在一个集合中，查询效率很高（1-N，最优能逼近于1）。在很多场景下，我们都需要一个能迅速判断一个元素是否在一个集合中。譬如： 网页爬虫对URL的去重，避免爬取相同的URL地址； 反垃圾邮件，从数十亿个垃圾邮件列表中判断某邮箱是否垃圾邮箱（同理，垃圾短信）； 缓存击穿，将已存在的缓存放到布隆中，当黑客访问不存在的缓存时迅速返回避免缓存及DB挂掉。 原理初始化状态是一个全为0的bit数组 为了表达存储N个元素的集合，使用K个独立的函数来进行哈希运算。x1，x2……xk为k个哈希算法。如果集合元素有N1，N2……NN，N1经过x1运算后得到的结果映射的位置标1，经过x2运算后结果映射也标1，已经为1的报错1不变。经过k次散列后，对N1的散列完成。依次对N2，NN等所有数据进行散列，最终得到一个部分为1，部分位为0的字节数组。当然了，这个字节数组会比较长，不然散列效果不好。 那么怎么判断一个外来的元素是否已经在集合里呢，譬如已经散列了10亿个垃圾邮箱，现在来了一个邮箱，怎么判断它是否在这10亿里面呢？很简单，就拿这个新来的也依次经历x1，x2……xk个哈希算法即可。在任何一个哈希算法譬如到x2时，得到的映射值有0，那就说明这个邮箱肯定不在这10亿内。如果是一个黑名单对象，那么可以肯定的是所有映射都为1，肯定跑不了它。也就是说是坏人，一定会被抓。那么误伤是为什么呢，就是指一些非黑名单对象的值经过k次哈希后，也全部为1，但它确实不是黑名单里的值，这种概率是存在的，但是是可控的。 至于具体实现，可以直接调用com.google.guava中的BloomFilter，就不赘述了。 缓存雪崩平时我们设定一个缓存的过期时间时，可能有一些会设置1分钟啊，5分钟这些，并发很高时可能会出在某一个时间同时生成了很多的缓存，并且过期时间都一样，这个时候就可能引发一当过期时间到后，这些缓存同时失效，请求全部转发到DB，DB可能会压力过重。 解决方法 将缓存失效时间分散开，比如我们可以在原有的失效时间基础上增加一个随机值，比如1-5分钟随机，这样每一个缓存的过期时间的重复率就会降低，就很难引发集体失效的事件。 加锁或者队列的方式保证缓存的单线 程（进程）写，从而避免失效时大量的并发请求落到底层存储系统上。 热key重建开发人员使用缓存 + 过期时间的策略既可以加速数据读写，又保证数据的定期更新，这种模式基本能够满足绝大部分需求。但是有两个问题如果同时出现，可能就会对应用造成致命的危害： 当前 key 是一个热点 key( 例如一个热门的娱乐新闻），并发量非常大。 重建缓存不能在短时间完成，可能是一个复杂计算，例如复杂的 SQL、多次IO、多个依赖等。 在缓存失效的瞬间，有大量线程来重建缓存 ( 如下图)，造成后端负载加大，甚至可能会让应用崩溃。 要解决这个问题也不是很复杂，但是不能为了解决这个问题给系统带来更多的麻烦，所以需要制定如下目标： 减少重建缓存的次数 数据尽可能一致 较少的潜在危险 解决方案1. 互斥锁此方法只允许一个线程重建缓存，其他线程等待重建缓存的线程执行完，重新从缓存获取数据即可。 这种方案思路比较简单，但是存在一定的隐患，如果构建缓存过程出现问题或者时间较长，可能会存在死锁和线程池阻塞的风险，但是这种方法能够较好的降低后端存储负载并在一致性上做的比较好。 2. 不设置超时时间“永远不过期”包含两层意思： 从缓存层面来看，确实没有设置过期时间，所以不会出现热点key过期后产生的问题，也就是“物理”不过期。 从功能层面来看，为每个value设置一个逻辑过期时间，当发现超过逻辑过期时间后，会使用单独的线程去构建缓存。 这种方案由于没有设置真正的过期时间，实际上已经不存在热点 key 产生的一系列危害，但是会存在数据不一致的情况，同时代码复杂度会增大。 http://mp.weixin.qq.com/s/TBCEwLVAXdsTszRVpXhVughttp://blog.csdn.net/tianyaleixiaowu/article/details/74721877http://blog.csdn.net/zeb_perfect/article/details/54135506https://zhuanlan.zhihu.com/p/26151305]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>缓存</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis排行榜]]></title>
    <url>%2F2018%2F02%2F03%2Fredis%E6%8E%92%E8%A1%8C%E6%A6%9C%2F</url>
    <content type="text"><![CDATA[设计思路因为看了一段时间的redis，准备动手做一个小demo，做一个排行榜，正好加在之前的未完成的新闻门户里面。关于排行榜他有一些跟排行榜本身相关的要求比如： 排行精确性如果一个排行榜的结果关系到用户的权益问题，这个时候一个排行榜的精确性就需要非常高，比如一个运营同学进行了根据微博转发数量的营销活动，这个时候微博转发数量的排行榜就需要非常精确，否则会影响用户权益的分发。 排行榜实时性游戏和社交互动的结合是目前的趋势，对于热门游戏的排行是用户的关注重点，在这部分用户中对于排行的实时性有很高的要求，如果一个用户升级了自己的装备和能力，而自己的排名一直没有更新，那这个用户一定要非常伤心抛弃这个游戏了。所以通过离线计算等平台来构建一个非实时的排行榜系统就不太适合这样的模型。 海量数据排行海量数据是目前的一个趋势，比如对于淘宝全网商品的一个排行，这个榜单将会是一个亿级别的，所以我们设计的榜单也需要具备弹性伸缩能力，同时在对海量数据进行排行的时候拥有一定的实时性。 实现方法目的是要实现一个热点新闻排行榜的话，毫无疑问，使用的是redis内置的zset这种数据结构，他可以根据score自动产生rank比较方便。我们将评论或者点赞数超过200的认为是热门文章 由于文章是从别的地方爬过来的，所以只有评论数没有点赞数，设置初始化分数为： score = 发布时间毫秒数 + 432 * 评论数 而问题就在排行榜更新的频率，更新过快，缓存效果不好，会产生类似重建热key的问题（下一篇文章要讲一下），但是频率过慢又不能达到实时性，所以正如之前所说的，要根据排行榜自身的要求制定一个适合的更新策略： 针对自身的这个项目需求，我想实现的是一个热点新闻排行榜，他的时效性要求并不是很高，所以通过分析网易新闻的爬取量，对爬到的每个新闻建立一个news：id的hash进行初始化，类似关系型数据库中的一条字段，并设置一周后过期自动删除，排行榜肯定是用zset的，但是为了不刷新过快，再建立一个time：的zset缓存最近一个星期的文章，设置一周过期，每周一次定时维护time：，从time: 删除时间超过一个星期的文章，并重置score：，由于爬虫每隔6小时更新一次，且新闻量相对较小，所以对time：的频繁读写是可以容忍的，再维护一个score：的zset简历news和score的映射，所以总的来说就是 爬取新闻，建立新的hash(news:id),设置过期时间为一周，并加入zset(time：) 每周执行一次更新，删除zset：中过期的任务，对未过期的任务分数进行更新。 score： zset news：id 分数 time： zset news：id 时间 news：id hash voted 投票数 title xxx url xxx 这里介绍一个在网上看到的实时排行榜的设计策略，其思路类似于维护一个小顶堆： 第一次访问的时候，查数据库，查整个表查出topN（使用sql排序），丢给redis(使用sorted set数据类型)。 排序在redis，redis自动排序。以后的用户访问：均访问redis。 只要每次积分变化判断的时候拿topN的最后一个判别，大于最后一名，则整个user丢进redis排序。效率性能再优化：用户积分变动的时候，（守护线程）服务器预存一下变化的数量。。到一定量再通知。 再往下去设定一个小距离为阈值。比如现在第50名的积分是100，那80分一下的应该就没必要扔给redis了吧？ 注意：这个排行榜的用户是会不断增加的，比如1亿用户，如果刚开始只有前50，后5千万人的积分大于第50名，那么就会往redis加入这个用户的信息。（虽然看起来要存很多，其实一亿用户怎么存也就1G左右的内存，简单暴力优雅方案了）1234567891011121314151617181920@Autowiredprivate RedisDao redisDao;private final int ONE_WEEK_IN_SECONDS = 7 * 86400;@Scheduled(cron = "0 0 0 1/7 * ?")public void updataRank() &#123; redisDao.zRemRangeByRank("score:", 0, -1); long cutOff = System.currentTimeMillis() / 1000 - ONE_WEEK_IN_SECONDS; Set&lt;TypedTuple&lt;Object&gt;&gt; set = redisDao.zRangeWithScores("time:", 0, -1); for (TypedTuple&lt;Object&gt; o: set) &#123; //如果过期直接删除，否则计算结果 BigDecimal db = new BigDecimal(o.getScore().toString());db.toPlainString(); if (Long.valueOf(db.toPlainString()) &lt; cutOff) &#123; redisDao.zrem("time:", o.getValue()); &#125; else &#123; redisDao.zadd("score:", o.getValue(), Double.valueOf(o.getScore().toString()) + 432 * Double.valueOf(redisDao.hget(o.getValue().toString(), "voted").toString())); &#125; &#125;&#125; 成品效果如图（忽略我这个丑陋的前端)： 新闻爬虫2.0由于这次修改也涉及到了之前爬取数据的爬虫，索性就把爬虫也一并进行了修改，对整个爬虫进行了重构，使用多线程对爬虫进行优化，具体步骤如下： 将爬虫分为两个部分，使用生产者和消费者模式，将redis作为任务队列，生产者爬虫爬取新闻url，消费者爬虫根据新闻url爬取具体信息。使用2个redis集合存储已爬新闻和未爬新闻，作为简单去重。 完整代码请参考： 新闻门户代码：https://github.com/MoriatyC/OmegaNews 爬虫代码：https://github.com/MoriatyC/nethard]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>缓存</tag>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis数据安全与性能保障]]></title>
    <url>%2F2018%2F01%2F29%2Fredis%E6%95%B0%E6%8D%AE%E5%AE%89%E5%85%A8%E4%B8%8E%E6%80%A7%E8%83%BD%E4%BF%9D%E9%9A%9C%2F</url>
    <content type="text"><![CDATA[一.持久化1. 快照： 将存在于某一时刻的所有数据都写入硬盘里面方法： 客户端通过向redis发送bgsave命令（创建子进程） 客户端通过向redis发送save命令，但是会阻塞其他命令，所以只有内存不够，或者不怕阻塞的时候才可以用。但是不要创建子进程，不会导致redis停顿，并且由于没有子进程抢资源所以比bgsave快。 设置了save选项：比如 save 60 10000，表示从最近一次创建快照之后开始算起，当有60s内有10000次写入的时候就会触发bgsave命令，可以有多个save配置，任意一个满足即可。 通过shutdown接收到关闭请求时，或者接收到标准的term信号，执行save命令 当一个redis服务器连接另一个redis服务器，想对方发送sync时，若主服务器没执行bgsave，或者并非刚刚执行完，那么主服务器就会执行bgsave。 缺点：当redis、系统或者硬件中的一个发生崩溃，将丢失最近一次创建快照后的数据。TIPS: 将开发环境尽可能的模拟生产环境以得到正确的快照生成速率配置。 2. AOF：在执行写命令时，将被执行的写命令复制到硬盘里面使用appendonlyyes配置选项打开，下图是appendfsync配置选项。 选项目 同步频率 always 每个写操作都要同步写入，严重降低redis速度损耗硬盘寿命 everysec 每秒执行一次，将多个写入同步，墙裂推荐 no 让os决定，不稳定，不知道会丢失多少数据 自动配置aof重写： auto-aof-rewrite-percentage 100 auto-aof-rrewrite-min-size 64当启用aof持久化之后，当aof文件体积大于64mb并且体积比上一次大了100%，就会执行bgrewriteaof命令。 缺点：1.aof文件过大，2. 文件过大导致还原事件过长。但是可以对其进行重写压缩。 二. 复制就像之前所说当一个从服务器连接一个主服务器的时候，主服务器会创建一个快照文件并将其发送到从服务器。 在配置中包含slaveof host port选项指定主服务器，启动时候会先执行aof或者快照文件。 也可以通过发送flaveof no one命令来终止复制操作，通过slaveof host port命令来开始复制一个主服务器，会直接执行下面的连接操作。 步骤 主服务器操作 从服务器操作 1 （等待命令） 连接主服务器，发送sync命令 2 开始执行bgsave，并使用缓冲区记录bgsave之后执行的所有写命令 根据配置选项决定使用现有数据处理客户端请求还是返回错误 3 Bgsave执行完毕，向从服务器发快照，并在发送期间继续用缓冲区记录写命令 丢弃所有旧数据，载入快照文件 4 快照发送完毕，向从服务器发送缓冲区里的写命令 完成快照解释，开始接受命令 5 缓冲区存储的写命令发送完毕：从现在起每执行一个写命令都发给从服务器 执行主服务器发来的所有存储在缓冲区里的写；并接受执行主服务器发来的写命令 三. 处理故障系统验证快照和aof文件 redis-check-aof redis-check-dump 检查aof和快照文件的状态，在有需要的情况下对aof文件进行修复。 更换新的故障主服务器假设A为主服务器，B为从服务器，当机器A发生故障的时候，更换服务器的步骤如下：首先向机器B发送一个save命令，将这个快照文件发送给机器C，在C上启动Redis，让B成为C的从服务器。 将从服务器升级为主服务器将从服务器升级为主服务器，为升级后的主服务器创建从服务器。 redis事务四. 事务multi: 标记一个事务块的开始。 事务块内的多条命令会按照先后顺序被放进一个队列当中，最后由 EXEC 命令原子性(atomic)地执行。 exec: 执行所有事务块内的命令。 假如某个(或某些) key 正处于 WATCH 命令的监视之下，且事务块中有和这个(或这些) key 相关的命令，那么 EXEC 命令只在这个(或这些) key 没有被其他命令所改动的情况下执行并生效，否则该事务被打断(abort)。 redis的事务包裹在multi命令和exec命令之中，在jedis中通过如下实现12345678910111213141516171819202122232425262728293031323334 public class RedisJava extends Thread&#123; static Response&lt;String&gt; ret; Jedis conn = new Jedis("localhost"); @Override public void run() &#123; Transaction t = conn.multi(); t.incr("notrans:"); Response&lt;String&gt; result1 = t.get("notrans:"); try &#123; Thread.sleep(1L); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; t.incrBy("notrans:", -1); t.exec(); String foolbar = result1.get(); System.out.println(foolbar); &#125; public static void main(String[] args) &#123; Jedis conn = new Jedis("localhost"); Thread t1 = new RedisJava(); Thread t2 = new RedisJava(); Thread t3 = new RedisJava(); t1.start(); t2.start(); t3.start(); &#125;&#125; wathc：监视一个(或多个) key ，如果在事务执行之前这个(或这些) key 被其他命令所改动，那么事务将被打断。 unwatch：取消 WATCH 命令对所有 key 的监视。如果在执行 WATCH 命令之后， EXEC 命令或 DISCARD 命令先被执行了的话，那么就不需要再执行 UNWATCH 了。的监视，因此这两个命令执行之后，就没有必要执行 UNWATCH 了。 discard :取消事务，放弃执行事务块内的所有命令。取消watch，清空任务队列。如果正在使用 WATCH 命令监视某个(或某些) key，那么取消所有监视，等同于执行命令 UNWATCH 。 一个简单的商品买卖demo如下： key type inventory：id set market zset user:id hash 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859public boolean listItem( Jedis conn, String itemId, String sellerId, double price) &#123; String inventory = "inventory:" + sellerId; String item = itemId + '.' + sellerId; long end = System.currentTimeMillis() + 5000; while (System.currentTimeMillis() &lt; end) &#123; conn.watch(inventory); if (!conn.sismember(inventory, itemId))&#123; conn.unwatch(); return false; &#125; Transaction trans = conn.multi(); trans.zadd("market:", price, item); trans.srem(inventory, itemId); List&lt;Object&gt; results = trans.exec(); // null response indicates that the transaction was aborted due to // the watched key changing. if (results == null)&#123; continue; &#125; return true; &#125; return false; &#125;public boolean purchaseItem( Jedis conn, String buyerId, String itemId, String sellerId, double lprice) &#123; String buyer = "users:" + buyerId; String seller = "users:" + sellerId; String item = itemId + '.' + sellerId; String inventory = "inventory:" + buyerId; long end = System.currentTimeMillis() + 10000; while (System.currentTimeMillis() &lt; end)&#123; conn.watch("market:", buyer); double price = conn.zscore("market:", item); double funds = Double.parseDouble(conn.hget(buyer, "funds")); if (price != lprice || price &gt; funds)&#123; conn.unwatch(); return false; &#125; Transaction trans = conn.multi(); trans.hincrBy(seller, "funds", (int)price); trans.hincrBy(buyer, "funds", (int)-price); trans.sadd(inventory, itemId); trans.zrem("market:", item); List&lt;Object&gt; results = trans.exec(); // null response indicates that the transaction was aborted due to // the watched key changing. if (results == null)&#123; continue; &#125; return true; &#125; 总结：相比于一般关系型数据库的悲观锁，redis的事务是典型的乐观锁，没有对事务进行封锁，以避免客户端运行过慢造成长时间的阻塞 非事务型流水线使用流水线，减少通信次数提高性能，以jedis为例，对比使用和没使用流水线的函数方法调用次数：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public void updateTokenPipeline(Jedis conn, String token, String user, String item) &#123; long timestamp = System.currentTimeMillis() / 1000; Pipeline pipe = conn.pipelined(); pipe.multi(); pipe.hset("login:", token, user); pipe.zadd("recent:", timestamp, token); if (item != null)&#123; pipe.zadd("viewed:" + token, timestamp, item); pipe.zremrangeByRank("viewed:" + token, 0, -26); pipe.zincrby("viewed:", -1, item); &#125; pipe.exec();&#125;//对比没有使用流水线的方法public void updateToken(Jedis conn, String token, String user, String item) &#123; long timestamp = System.currentTimeMillis() / 1000; conn.hset("login:", token, user); conn.zadd("recent:", timestamp, token); if (item != null) &#123; conn.zadd("viewed:" + token, timestamp, item); conn.zremrangeByRank("viewed:" + token, 0, -26); conn.zincrby("viewed:", -1, item); &#125;&#125;//测试函数如下 public void benchmarkUpdateToken(Jedis conn, int duration) &#123; try&#123; @SuppressWarnings("rawtypes") Class[] args = new Class[]&#123; Jedis.class, String.class, String.class, String.class&#125;; Method[] methods = new Method[]&#123; this.getClass().getDeclaredMethod("updateToken", args), this.getClass().getDeclaredMethod("updateTokenPipeline", args), &#125;; for (Method method : methods)&#123; int count = 0; long start = System.currentTimeMillis(); long end = start + (duration * 1000); while (System.currentTimeMillis() &lt; end)&#123; count++; method.invoke(this, conn, "token", "user", "item"); &#125; long delta = System.currentTimeMillis() - start; System.out.println( method.getName() + ' ' + count + ' ' + (delta / 1000) + ' ' + (count / (delta / 1000))); &#125; &#125;catch(Exception e)&#123; throw new RuntimeException(e); &#125;&#125; 运行结果如图所示，在本地运行性能提升大概17.8倍。 tips：可以使用redis-benchmark工具进行性能测试。 五. References 《Redis实战》]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>nosql</tag>
        <tag>持久化</tag>
        <tag>事务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线程池(二)]]></title>
    <url>%2F2018%2F01%2F18%2F%E7%BA%BF%E7%A8%8B%E6%B1%A0(%E4%BA%8C)%2F</url>
    <content type="text"><![CDATA[性能问题饥饿死锁如果线程池中的任务依赖于之后提交的子任务，当线程池不够大的时候，很容易造成饥饿死锁。所以最好在线程池中加入的是同类型的独立任务。 运行时间较长的任务如果线程运行时间较长也会影响任务的相应性，同样造成不好的体验，所以api有很多方法都带有一个限时版本。 线程池大小线程池的大小需要分析计算环境，资源预算和任务特性。 一般来说在知道了系统中有多少个cpu和内存的基础下，任务类型是最为重要的。 对于计算密集型的任务线程池大小为cpu数+1，以实现尽可能的满载利用率 对于i/o密集型，由于线程不会一直执行，所以规模更大。这里给出一个《Java并发变成实战》这本书提出的一个公式 最佳线程数目 = （线程等待时间/线程CPU时间之比 + 1） CPU数目cpu利用率 即等待时间越长，需要更多的线程。当我们不需要一个那么精准的线程数目时，也可以用这个公式 最佳线程数目 = 2N+1(N为CPU数目) 是否使用线程池就一定比使用单线程高效呢？答案是否定的，比如Redis就是单线程的，但它却非常高效，基本操作都能达到十万量级/s。从线程这个角度来看，部分原因在于： 多线程带来线程上下文切换开销，单线程就没有这种开销 锁 当然“Redis很快”更本质的原因在于：Redis基本都是内存操作，这种情况下单线程可以很高效地利用CPU。而多线程适用场景一般是：存在相当比例的IO和网络操作。 扩展线程池ThreadPoolExecutor提供了几个可以在子类化中该学的方法： beforeExecute afterExecute terminated 如果beforeExecute抛出一个RuntimeException，那么任务将不被执行，并且afterExecute也不会被调用。 但是无论人物从run中正常返回还是抛出一个异常返回，afterExecute都会被调用，如果任务在完成后带有一个Error，那么久不会调用。 所有任务都已经完成并且所有工作者线程也已经关闭后，terminated会被调用。 给出一个demo，他通过这一些列方法来统计任务执行并添加日志。123456789101112131415161718192021222324252627282930313233343536373839public class TimingThreadPool extends ThreadPoolExecutor &#123; public TimingThreadPool() &#123; super(1, 1, 0L, TimeUnit.SECONDS, null); &#125; private final ThreadLocal&lt;Long&gt; startTime = new ThreadLocal&lt;Long&gt;(); private final Logger log = Logger.getLogger("TimingThreadPool"); private final AtomicLong numTasks = new AtomicLong(); private final AtomicLong totalTime = new AtomicLong(); protected void beforeExecute(Thread t, Runnable r) &#123; super.beforeExecute(t, r); log.fine(String.format("Thread %s: start %s", t, r)); startTime.set(System.nanoTime()); &#125; protected void afterExecute(Runnable r, Throwable t) &#123; try &#123; long endTime = System.nanoTime(); long taskTime = endTime - startTime.get(); numTasks.incrementAndGet(); totalTime.addAndGet(taskTime); log.fine(String.format("Thread %s: end %s, time=%dns", t, r, taskTime)); &#125; finally &#123; super.afterExecute(r, t); &#125; &#125; protected void terminated() &#123; try &#123; log.info(String.format("Terminated: avg time=%dns", totalTime.get() / numTasks.get())); &#125; finally &#123; super.terminated(); &#125; &#125;&#125; 引用 http://ifeve.com/how-to-calculate-threadpool-size/《Java并发编程实战》]]></content>
      <categories>
        <category>并发</category>
      </categories>
      <tags>
        <tag>容器</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线程池(一)]]></title>
    <url>%2F2018%2F01%2F17%2F%E7%BA%BF%E7%A8%8B%E6%B1%A0(%E4%B8%80)%2F</url>
    <content type="text"><![CDATA[概述这几天准备深入学习有关并发的知识，所以先简单复习了一下JDK自带的并发包，其中首先比较重要的一个就是线程池了。 为什么不无限的创造线程？主要基于以下几个原因： 线程生命周期的开销非常高 资源消耗 稳定性 所谓物极必反，线程的创建和销毁和需要一定的时间，如果所创建的线程工作时间还不如创建销毁的时间长那是得不偿失的，并且当线程创建过多也会对内存造成一定的负担甚至溢出，并且对GC也是极大的消耗，由于存在一定数额的活跃线程也提高了响应性。 线程池根据《阿里巴巴Java开发手册》中对线程创建的要求 【强制】线程资源必须通过线程池提供，不允许在应用中自行显式创建线程 由此可见，在正式生产环境中，线程池是唯一的创建线程的方法。而JDK对线程池也有强大的支持。 根据《手册》中的另一点要求 【强制】线程池不允许使用 Executors 去创建，而是通过 ThreadPoolExecutor 的方式，这样的处理方式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险。说明： Executors 返回的线程池对象的弊端如下：1） FixedThreadPool 和 SingleThreadPool:允许的请求队列长度为 Integer.MAX_VALUE，可能会堆积大量的请求，从而导致 OOM。2） CachedThreadPool 和 ScheduledThreadPool:允许的创建线程数量为 Integer.MAX_VALUE， 可能会创建大量的线程，从而导致 OOM 虽然Executor为我们提供了很多方便的工厂方法，比如newSingleThreadExecutor(),也有Executors为我们很好的实现了这些工厂方法，但是手动实现ThreadPoolExecutor能让我们对线程池有更深的了解和控制。所以接下来让我们来介绍一下ThreadPoolExecutor这个类。 原理一个最常见的ThreadPoolExecutor构造函数如下 1234567ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) corePoolSize： 活动线程数 maximumPoolSize： 线程池上限 keepAliveTime： 当线程池中线程数超过corePoolSize后，完成工作后的线程存活时间 unit： 单位其余的几个参数我们会在后面着重介绍。 这里介绍一下ThreadPoolExecutor的核心工作原理 123456789101112131415int c = ctl.get(); if (workerCountOf(c) &lt; corePoolSize) &#123; if (addWorker(command, true)) return; c = ctl.get(); &#125; if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123; int recheck = ctl.get(); if (! isRunning(recheck) &amp;&amp; remove(command)) reject(command); else if (workerCountOf(recheck) == 0) addWorker(null, false); &#125; else if (!addWorker(command, false)) reject(command); workderCountOf()获得当前线程池线程总数，若小于corePoolSize，则直接将任务通过addWorker()方法执行，否则在workQueue.offer()进入等待队列，若进入失败，则任务直接交给线程池，若线程池达到了maximumPoolSize则提交失败执行拒绝策略 任务队列BlockingQueue：接口，阻塞队列，数据共享通道任务队列的作用在于，当线程池中线程数达到corePoolSize的时候，接下来的任务将进入这个队列进行等待，等待执行。 简单原理服务线程（获取队列信息并处理的线程）在队列为空时进行读等待，有新的消息进入队列后自动唤醒，反之，当队列满时进行写等待直到有消息出队。 不同于常用的offer()和poll()方法，这里我们使用take()和put()方法进行读写。我们以ArrayBlockingQueue的为例子,其中包括了这几个控制对象123final ReentrantLock lick;private final Condition notEmpty;private final Condition notFull; 就拿take()来说1234567891011public E take() throws InterruptedException &#123; final ReentrantLock lock = this.lock; lock.lockInterruptibly(); try &#123; while (count == 0) notEmpty.await(); return dequeue(); &#125; finally &#123; lock.unlock(); &#125;&#125; 当队列为空时，他会在notEmpty上进行等待，在线程等待时，若有新的元素插入，线程就会被唤醒12345678910private void enqueue(E x) &#123; // assert lock.getHoldCount() == 1; // assert items[putIndex] == null; final Object[] items = this.items; items[putIndex] = x; if (++putIndex == items.length) putIndex = 0; count++; notEmpty.signal();&#125; 常用实现 SynchronousQueue(直接提交队列): 一个零容量队列，每个插入操作要对应一个删除操作。提交的任务不会被真实保存，其实就是将新任务交给了线程执行。 ArrayBlockingQueue(有界任务队列): 这里就会用到线程池中另一个参数maximumPoolSize, 若当前线程池中线程小于corePoolSize则直接在线程池中增加线程，若大于，则加入该任务队列，若队列满则继续加入线程池，若线程池中数目多余maximumPoolSize则执行拒绝策略。 LinkedBlockingQueue(无界任务队列)：如果未指定容量，那么容量将等于 Integer.MAX_VALUE。只要插入元素不会使双端队列超出容量，每次插入后都将动态地创建链接节点。 PriorityBlockingQueue(优先任务队列)： 一个特殊的无界任务队列，前面两者都是按FIFO的顺序执行，而这个是可以按照优先级执行。拒绝策略JDK内置拒绝策略如下 AboerPolicy(默认）：直接抛出异常，阻止系统正常工作。 CallerRunsPolicy: 只要线程池未关闭，该策略直接在调用者线程中，运行当前被丢弃的任务。（伪丢弃，但是任务提交线程性能大幅度下降） DiscardOledestPolicy:和名字一样，丢弃最老的一个请求，也就是即将被执行的一个任务，并尝试再次提交当前任务。 DiscardPolicy: 丢弃无法处理的任务，不给任何处理。 异常堆栈首先给出一个例子：123456789101112131415161718192021222324252627public class Main implements Runnable&#123; int a, b; public Main(int a, int b) &#123; this.a = a; this.b = b; &#125; @Override public void run() &#123; int ret = a / b; System.out.println(ret); &#125; public static void main(String[] args) &#123; ThreadPoolExecutor pools = new ThreadPoolExecutor(0, Integer.MAX_VALUE, 0L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;()); for (int i = 0; i &lt; 5; i++) &#123; pools.submit(new Main(100, i)); &#125; &#125;&#125;/* 结果如下100335025*/ 可以发现，其中一个显然的异常除数为0不见了，我们可以通过将submit方法改为execute方法来打印部分异常信息，但是我们仍然不能发现他的调用线程在哪儿。这里我们通过扩展线程池给出一种解决办法。1234567891011121314151617181920212223242526272829303132333435363738394041424344package first_maven;import java.util.concurrent.BlockingQueue;import java.util.concurrent.Future;import java.util.concurrent.SynchronousQueue;import java.util.concurrent.ThreadPoolExecutor;import java.util.concurrent.TimeUnit;public class Main extends ThreadPoolExecutor&#123; public Main(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue) &#123; super(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue); &#125; @Override public void execute(Runnable task) &#123; super.execute(wrap(task, clientTrace(), Thread.currentThread().getName())); &#125; @Override public Future&lt;?&gt; submit(Runnable task) &#123; return super.submit(wrap(task, clientTrace(), Thread.currentThread().getName())); &#125; private Exception clientTrace() &#123; return new Exception("Client stack trace"); &#125; private Runnable wrap(final Runnable task, final Exception clientStack, String clientThreadName) &#123; return new Runnable() &#123; @Override public void run() &#123; try &#123; task.run(); &#125; catch(Exception e) &#123; clientStack.printStackTrace(); System.out.println(" 1212"); throw e; &#125; &#125; &#125;; &#125;&#125; 我们通过扩展ThreadPoolExecutor，将要执行的Runnable进行包装，通过手动创建异常，获取当前主线程的调用堆栈，从而得到线程池的调用信息，并打印相应的运行异常，这样我们就可以追踪到完整的异常信息。 总结在使用多线程的时候，要通过ThreadPoolExecutor来手动创建，根据当前任务的需求分配相应的线程池大小和阻塞队列以及拒绝策略，这样才能知根知底。 五. References 《实战Java高并发程序设计》 《阿里巴巴Java开发手册》]]></content>
      <categories>
        <category>并发</category>
      </categories>
      <tags>
        <tag>容器</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[工厂模式]]></title>
    <url>%2F2018%2F01%2F13%2F%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[这两天正在看关于多线程的一些内容，看到线程池的时候发现它的实现使用了工厂模式，之前对工厂模式的了解不深，只是知道他是根据需要创建对象的，索性就开个支线，找了本书看了看关于工厂模式的一些知识，书中讲的也比较有意思，以下是一些心得。 概述对于设计模式来说，模式本身固然重要，但是模式设计的思想也同样很有味道，其中带来的一些OO的原则更是我们平时写代码需要注意的地方。而对于OO的设计原则其中有一个重要的思想就是将固定与变化分开，也就是简单的策略模式，将变化抽象，针对同一个接口，有各自的实现。 但是对于创建对象来说，java中只有new这一种方法，这就不可避免的要将代码写死，这又是我们不想看到的事情，由于硬编码带来的一系列拓展上的不便，使我们无法针对接口编程。就好像当我们使用集合的使用都会这么写： 1List&lt;T&gt; list = new XXXList&lt;&gt;(); 因为这种针对接口的编程给了我们更多的自由。那么有没有一种灵活的方式创建对象，那就是工厂模式，所有的工厂模式都是针对对象的创建。 一.简单工厂首先声明一下，简单工厂不是一种设计模式，只是一种习惯而已。他将动态的创建对象这一过程与固定的使用对象的代码分隔开。我们结合一个简单的例子来说：1234567891011121314151617181920212223242526272829303132333435363738394041class PizzaStore &#123; SimplePizzaFactory factory; public PizzaStore(SimplePizzaFactory factory) &#123; this.factory = factory; &#125; public Pizza orderPizza(String type) &#123; Pizza pizza; pizza = factory.createPizza(type); pizza.prepare(); pizza.bake(); pizza.cut(); pizza.box(); return pizza; &#125;&#125;public class SimplePizzaFactory &#123; public Pizza createPizza(String type) &#123; Pizza pizza = null; if (type.equals("cheese")) &#123; pizza = new CheesePizza(); &#125; else if (type.equals("pepperoni")) &#123; pizza = new PepperoniPizza(); &#125; else if (type.equals("clam")) &#123; pizza = new ClamPizza(); &#125; else if (type.equals("veggie")) &#123; pizza = new VeggiePizza(); &#125; return pizza; &#125;&#125; 在这个例子中，我们所需要创建的对象是Pizza，但在这里我们通过一个factory代替了以往的new关键字来创建对象，而这样的好处也是显而易见的，在这个服务中，变化的是Pizza的种类，而处理Pizza 的流程是固定的。我们只需根据需要传入所需的factory，就能实现创建对象与使用对象的解耦。 我们通过定义一个工厂类，将创建对象的操作通过这个类来进行，当对象种类增加时，我们只需要修改工厂类，就是所谓类对修改关闭，对扩展开放。二. 工厂方法在上一个例子中，我们在PizzaStore中创建简单工厂对象，通过简单工厂创建对象，这不免让代码失去了一点弹性，让我们进一步抽象，将创建对象的方法进一步封装，形成一个抽象基类，让每个子类去各自实现自己所需的创建对象的方法。提高代码的可扩展性。下面给出例子：12345678910111213141516171819202122232425262728abstract class PizzaStore &#123; abstract Pizza createPizza(String item); public Pizza orderPizza(String type) &#123; Pizza pizza = createPizza(type); System.out.println("--- Making a " + pizza.getName() + " ---"); pizza.prepare(); pizza.bake(); pizza.cut(); pizza.box(); return pizza; &#125;&#125;public class ChicagoPizzaStore extends PizzaStore &#123; Pizza createPizza(String item) &#123; if (item.equals("cheese")) &#123; return new ChicagoStyleCheesePizza(); &#125; else if (item.equals("veggie")) &#123; return new ChicagoStyleVeggiePizza(); &#125; else if (item.equals("clam")) &#123; return new ChicagoStyleClamPizza(); &#125; else if (item.equals("pepperoni")) &#123; return new ChicagoStylePepperoniPizza(); &#125; else return null; &#125;&#125; 在上面的代码中，对象的创建只给出了一个抽象方法，而具体的实现，则有子类自由选择决定，这样极大的丰富了代码的选择性和扩展性。基类实际上并不知道他持有的是什么对象，他主要负责持有对象后的一系列固定流程操作。 定义了一个创建对象的接口，但由子类决定要实例化的类是哪一个。工厂方法让类把实例化推迟到子类。 对比工厂方法和简单工厂原本有一个对象负责所有具体类的实例化，而在工厂方法中则由一些子类来负责实例化。工厂方法用来处理对象的创建，并将行为封装在子类，这样基类的代码就和子类的对象创建完全解耦。 三. 抽象工厂当你需要创建的对象也依赖了一系列可变对象，那么就需要工厂模式中的最后一种方式–抽象工厂。我们首先给出抽象工厂的定义： 提供一个借口，用于创建相关或依赖对象的家族，而不需要明确指定具体类。让我们再用Pizza来举例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public class ChicagoPizzaStore extends PizzaStore &#123; protected Pizza createPizza(String item) &#123; Pizza pizza = null; PizzaIngredientFactory ingredientFactory = new ChicagoPizzaIngredientFactory(); if (item.equals("cheese")) &#123; pizza = new CheesePizza(ingredientFactory); pizza.setName("Chicago Style Cheese Pizza"); &#125; else if (item.equals("veggie")) &#123; pizza = new VeggiePizza(ingredientFactory); pizza.setName("Chicago Style Veggie Pizza"); &#125; else if (item.equals("clam")) &#123; pizza = new ClamPizza(ingredientFactory); pizza.setName("Chicago Style Clam Pizza"); &#125; else if (item.equals("pepperoni")) &#123; pizza = new PepperoniPizza(ingredientFactory); pizza.setName("Chicago Style Pepperoni Pizza"); &#125; return pizza; &#125;&#125;class ChicagoPizzaIngredientFactory implements PizzaIngredientFactory &#123; public Dough createDough() &#123; return new ThickCrustDough(); &#125; public Sauce createSauce() &#123; return new PlumTomatoSauce(); &#125; public Cheese createCheese() &#123; return new MozzarellaCheese(); &#125; public Veggies[] createVeggies() &#123; Veggies veggies[] = &#123; new BlackOlives(), new Spinach(), new Eggplant() &#125;; return veggies; &#125; public Pepperoni createPepperoni() &#123; return new SlicedPepperoni(); &#125; public Clams createClam() &#123; return new FrozenClams(); &#125;&#125; PizzaStore和之前一样，这里就不重复了，和之前不一样的是在子类的createPizza方法中，我们不是简单的返回对象，而是根据创建对象所依赖的成员的不同，也进行了“个性化定制”。 其本质上其实也是用工厂方法对依赖对象进行创建。四. 抽象工厂与抽象方法的比较 工厂方法使用继承，把对象的创建委托给子类，子类实现工厂方法来创建对象，并将实例化延迟到子类。 抽象工厂使用组合，对象的创建被实现在工厂接口所暴露出来的方法中。 抽象工厂创建相关的对象家族，并让他们集合起来，而不需要依赖他们的具体类 五. References 《Head First 设计模式》]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>工厂</tag>
        <tag>OO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库索引]]></title>
    <url>%2F2017%2F11%2F27%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[索引在存储引擎层实现，所以我们先介绍一下不同的数据库引擎。 一.数据库引擎的类型1. InnoDBMySQL的默认事务型引擎，使用mvcc来支持高并发，实现了四个标准的隔离级别，默认级别是rr，通过间隙锁策略防止幻读。 InnoDB表基于聚簇索引建立的，数据存储在表空间中，可以将每个表的数据和索引存放在单独的文件中，支持热备份，其他的引擎都不支持。他的索引结构和其他存储引擎有很大不同，他的二级索引中必须包含主键 2. MyISAMMysql5.1及之前版本的默认引擎，有大量特性，包括全文索引、压缩、空间函数，不支持事务和行级锁，崩溃后无法安全恢复。 MyISAM会将表存储在两个文件中：数据文件和索引文件。 InnoDB和MyISAM的区别 InnoDB支持事务，MyISAM不支持 InnoDB是聚集索引，数据文件是和索引绑在一起的，必须要有主键，通过主键索引效率很高。但是辅助索引需要两次查询，先查询到主键，然后再通过主键查询到数据。因此，主键不应该过大，因为主键太大，其他索引也都会很大。而MyISAM是非聚集索引，数据文件是分离的，索引保存的是数据文件的物理地址。主键索引和辅助索引是独立的。 3.Memory如果需要快速的访问数据，并且这些数据不会被修改，重启后丢失也没有关系，那么使用Memory表很有用，他比MyISAM快一个数量级，因为索欧数据都保存在内存中。重启后结构保留，数据丢失。 二.索引的类型1.B-Tree索引实际上在很多引擎中使用的是B+树进行优化比如innoDB，之后再写一篇详细介绍B-树索引的文章，这里先简单介绍一下，他的工作原理，存储引擎不需要进行权标扫描来获取数据，而是从根节点开始搜索，根节点存了指向子节点的指针，根据这些指针向下层搜索，通过比较节点页的值和要查找的值，可以找到合适的指针进入下一层。 索引对多个值进行排序依据的是CREATE TABLE语句中定义索引时列的顺序 可以使用B-Tree索引的查询类型适用于全键值、键值范围、或者键前缀查找，假设索引建立在姓、名、出生日期上，实际使用如下所示 全值匹配： 匹配Cuba Allen、生日是1970-01-01的人 匹配最左前缀：可以用于查找姓为Allen的人，即只用第一列 匹配列前缀：匹配姓开头为J的 匹配范围值：查找姓在Allen和Jack之间的人 精确匹配某一列并范围匹配另一列：查找姓为Allen名字是K开头的 优点 大大减少了服务器需要扫描的数据量 可以帮助服务器避免排序和临时表 可以将随机io编程顺序io限制 若果不按引的最左列开始查找，则无法使用：比如无法找到特定生日的人活着名字叫Bill的人，要从索引的最左列开始，所以也无法找找姓氏以某个字母结尾的人 不能跳过索引：即使无法找到姓为Smith并且在某个特定日期出生的人 如果查询中有某个范围查询，那么他右边的所有列都无法使用索引优化查找：例如where last_name = “Smith” and first_name like “J%” and dob = ‘1990-01-01’,由于第二列是范围查询，所以第三列作废 Tips 对于BLOB、TEXT或者很长的VARCHAR类型的列，必须使用前缀索引，因为MySQL不允许索引这些列的完整长度。 选择合适的索引列顺序 2.哈希索引只有精确匹配索引所有列的查询才有效，对于每一行数据，都会对所有的索引列计算一个hashcode，哈希索引将所有的哈希码存储在索引中，同时在哈希表中保存指向每个数据行的指针 Innodb有一个功能叫做自适应哈希索引，当innodb注意到某些索引值使用的很频繁的时候，会在内存中基于B-Tree索引之上再创建一个哈希索引。 缺陷 索引只包含哈希值和行指针不存储字段值，无法通过索引值来避免读行 不是按索引值的顺序存储的，无法排序 不支持部分索引列匹配查找 只支持等值比较 访问哈希索引的数据很快，除非有很多冲突，冲突多的时候维护代价很大 3.空间数据索引(R_Tree)4. 全文索引InnoDB和MyISAM中B-Tree实现区别聚簇索引和非聚簇索引聚簇索引的顺序就是数据的物理存储顺序，而对非聚簇索引的索引顺序与数据物理排列顺序无关其实就是一个存储的是具体数据，一个存储了物理地址。正是聚簇索引的顺序就是数据的物理存储顺序，所以一个表最多只能有一个聚簇索引，因为物理存储只能有一个顺序。 InnoDB和MyISAM的数据分布对比下面分别是聚簇索引和非聚簇索引的存储结构图 总结： InnoDB是聚簇索引，通过主键引用被索引的值，数据文件和索引文件在一起，MyISAM是非聚簇索引，通过物理地址索引被索引的值，数据文件和索引文件在两个文件中 InnoDB的二级索引需要包含主键，MyISAM不需要，仍然只需要存储地址，他的主键索引有唯一性要求，二级索引没有 InnoDB索引保存了原格式文件，MyISAM使用了前缀压缩 后记对于聚簇索引和非聚簇索引的区别，应该从他们存储的区别以及相对应的主键索引和二级索引来说。用查字典来举例的话，聚簇索引类似用拼音检索，物理顺序和逻辑顺序一致，非聚簇索引类似于偏旁部首查找，通过偏旁部首找到页码，也就是相应的物理地址指针。正由于一个字典只有一个排列顺序，所以一个表只有一个聚簇索引。对于聚簇索引对应的主索引和二级索引，他的二级索引包含主键列，在查询的时候需要查询2次，先要查询到主键列，再根据这个值去聚簇索引中查找。对于非聚簇索引的主键索引和二级索引相差不大，存储的都是相应的物理地址。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>索引</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库隔离级别]]></title>
    <url>%2F2017%2F11%2F26%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%2F</url>
    <content type="text"><![CDATA[READ UNCOMMITTED(未提交读)事务中的修改，即使没有提交，对其他事务也都是可见的，即读取了一个未提交的修改（脏读） READ COMMITTED(提交读)大多数数据库的默认隔离级别(mysql不是)，满足ACID中的隔离性定义：一个事务开始时，只能看见已经提交的事务所做的修改。有点不能理解，在网上看到了一个例子： singo拿着工资卡去消费，系统读取到卡里确实有2000元，而此时她的老婆也正好在网上转账，把singo工资卡的2000元转到另一账户，并在 singo之前提交了事务，当singo扣款时，系统检查到singo的工资卡已经没有钱，扣款失败，singo十分纳闷，明明卡里有钱，为 何…… 出现上述情况，即我们所说的不可重复读 ，两个并发的事务，“事务A：singo消费”、“事务B：singo的老婆网上转账”，事务A事先读取了数据，事务B紧接了更新了数据，并提交了事务，而事务A再次读取该数据时，数据已经发生了改变。 这里事务A两次读取到的数据都是已经提交的，不管是首次读取的2000，还是在B事务提交之后读取到的0，所以这个级别有时候也叫不可重复读。 当隔离级别设置为Read committed 时，避免了脏读，但是可能会造成不可重复读。 REPEATABLE READ(可重复度， mysql默认级别)该级别保证同一个事务多次读取的结果是一致的，在同一个事务里，SELECT的结果是事务开始时时间点的状态，因此，同样的SELECT操作读到的结果会是一致的。用上面的例子说就是当singo拿着工资卡去消费时，一旦系统开始读取工资卡信息（即事务开始），singo的老婆就不可能对该记录进行修改，也就是singo的老婆不能在此时转账。但是这个级别会产生幻读，所谓幻读，指的是当某个事务在读取某个范围内的记录时，另一个事务又在该范围内插入新纪录，当前事务再次读取该范围的记录时会产生幻行。 幻读和不可重复读的区别幻读是指当事务不是独立执行时发生的一种现象，例如第一个事务对一个表中的数据进行了修改，这种修改涉及到表中的全部数据行。同时，第二个事务也修改这个表中的数据，这种修改是向表中插入一行新数据。那么，以后就会发生操作第一个事务的用户发现表中还有没有修改的数据行，就好象发生了幻觉一样。 不可重复读强调的是对同一个数据的修改得到两个不同的结果，幻读强调的是对结果集的插入或删除操作，产生了新的结果集。 Serializable (序列化)Serializable 是最高的事务隔离级别，强制事务串行执行，简单来说就是读取的每一行数据都加锁，同时代价也花费最高，性能很低，一般很少使用，在该级别下，事务顺序执行，不仅可以避免脏读、不可重复读，还避免了幻像读。 间隙锁（Next-Key锁）当我们用范围条件而不是相等条件检索数据，并请求共享或排他锁时，InnoDB会给符合条件的已有数据记录的索引项加锁；对于键值在条件范围内但并不存在的记录，叫做“间隙（GAP)”，InnoDB也会对这个“间隙”加锁，这种锁机制就是所谓的间隙锁（Next-Key锁）,另外一方面，是为了满足其恢复和复制的需要。 举例来说，假如emp表中只有101条记录，其empid的值分别是 1,2,…,100,101，下面的SQL： Select * from emp where empid &gt; 100 for update;是一个范围条件的检索，InnoDB不仅会对符合条件的empid值为101的记录加锁，也会对empid大于101（这些记录并不存在）的“间隙”加锁。 InnoDB使用间隙锁的目的，一方面是为了防止幻读，以满足相关隔离级别的要求， √: 可能出现 ×: 不会出现 隔离级别 脏读 不可重复读 幻读 加锁读 Read uncommitted √ √ √ × Read committed × √ √ × Repeatable read × × √ × Serializable × × × √ 最后提一句InnoDB采用两阶段锁协议，只有在commit和rollback的时候才会释放锁并且是同一时间释放，并且这里说的锁都是隐式加锁，InnoDB会根据隔离级别在需要的时候自动加锁。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>事务</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[为什么String是不可变对象(译)]]></title>
    <url>%2F2017%2F11%2F23%2F%E4%B8%BA%E4%BB%80%E4%B9%88String%E6%98%AF%E4%B8%8D%E5%8F%AF%E5%8F%98%E5%AF%B9%E8%B1%A1(%E8%AF%91)%2F</url>
    <content type="text"><![CDATA[原文:Why String is immutable in Java ? String是Java中的一个不可变类。所谓不可变，简单来说就是其对象不能被修改。实例中的所有信息在初始化的时候就已经具备，并且不能被修改。不可变类有很多优点。这篇文章简要说明了为什么String被设计为不可变类。关于其好的回答应该建立在对内存模型、同步和数据结构等的理解之上。 1. 字符串池的需求字符串池是一个位于方法区的特殊区域。当一个字符串被创建的时候，如果该字符串已经存在于字符串池中，那么直接返回该字符串的引用，而不是创建一个新的字符串。下边的代码将只会创建一个字符串对象：12String s1 = "abcd";String s2 = "abcd"; s1和s2都指向同一个字符串对象。如果String不是不可变的，那么修改s1的字符串对象同样也会导致s2的内容发生变化。 2. 缓存Hashcode字符串的hashcode在Java中经常被用到。例如，在一个HashMap中。其不可变性保证了hashcode（哈希值）总是保持不变，从而不用担心因hashcode变化导致的缓存问题。那就意味着，不用每次在其使用的时候计算其hashcode，从而更加高效。在String类中，有如下代码：1private int hash; //用来缓存hash code 3. 简化其他对象的使用为了理解这一点，请看下边的代码：123456HashSet&lt;String&gt; set = new HashSet&lt;String&gt;();set.add(new String(&quot;a&quot;));set.add(new String(&quot;b&quot;));set.add(new String(&quot;c&quot;));for (String a : set) a.value = &quot;a&quot;; 这个例子中，如果String是可变的，也就是说set中的值是可变的，这会影响到set的设计（set包含不重复的元素）。当然这个例子是有问题的，在String类中是不存在value这个属性的。（ps：个人觉得应该是没有可以直接访问的value，毕竟String中value数组是可以通过反射访问的，不知道，不知道这个老外是怎么个意思） 4.安全性字符串在许多的java类中都用作参数，例如网络连接，打开文件等等。如果字符串是可变的，一个连接或文件就会被修改从而导致严重的错误。可变的字符串也会导致在使用反射时导致严重的问题，因为参数是字符串形式的。举例如下：1234567boolean connect(String s) &#123; if (!isSecure(s)) &#123; throw new SecurityException(); &#125; // 如果s内的值被修改，则会导致出现问题 doSomethind(s); &#125; （虽然略牵强，但是也有一定道理） 5. 不可变的对象本身就是线程安全的不可变的对象，可以在多个线程间自由共享。从而免除了进行同步的麻烦。 总之， String被设计为不可变的类，是出于性能和安全性的考虑，这也是其他所有不可变类应用的初衷。 6. 引用 http://blog.csdn.net/get_set/article/details/49926511]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>源码分析</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
</search>
