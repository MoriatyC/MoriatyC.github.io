<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[AOP]]></title>
    <url>%2F2018%2F02%2F28%2FAOP%2F</url>
    <content type="text"><![CDATA[引入AOP（Aspect Oriented Programming），即面向切面编程，可以说是OOP（Object Oriented Programming，面向对象编程）的补充和完善。OOP引入封装、继承、多态等概念来建立一种对象层次结构，用于模拟公共行为的一个集合。不过OOP允许开发者定义纵向的关系，但并不适合定义横向的关系，例如日志功能。日志代码往往横向地散布在所有对象层次中，而与它对应的对象的核心功能毫无关系对于其他类型的代码，如安全性、异常处理和透明的持续性也都是如此，这种散布在各处的无关的代码被称为横切（cross cutting），在OOP设计中，它导致了大量代码的重复，而不利于各个模块的重用。 AOP技术恰恰相反，它利用一种称为”横切”的技术，剖解开封装的对象内部，并将那些影响了多个类的公共行为封装到一个可重用模块，并将其命名为”Aspect”，即切面。所谓”切面”，简单说就是那些与业务无关，却为业务模块所共同调用的逻辑或责任封装起来，便于减少系统的重复代码，降低模块之间的耦合度，并有利于未来的可操作性和可维护性。 使用”横切”技术，AOP把软件系统分为两个部分：核心关注点和横切关注点。业务处理的主要流程是核心关注点，与之关系不大的部分是横切关注点。横切关注点的一个特点是，他们经常发生在核心关注点的多处，而各处基本相似，比如权限认证、日志、事物。AOP的作用在于分离系统中的各种关注点，将核心关注点和横切关注点分离开来。 一.术语1.横切关注点软件开发中，散布于应用中多处的功能被称为横切关注点，如事务、日志、安全。 2.advice(通知） what when切面的工作被称为通知，他定义了切面的工作是什么和何时工作 5种类型通知: 前置通知before 后置通知after(不关心方法的输出是什么) 返回通知after-returning(方法成功执行) 异常通知after-throwing(方法返回异常) 环绕通知around(包裹被通知方法，之前和之后都自定义行为) 3.join point连接点应用执行过程中，能够插入切面的所有“点”（时机），使用通知的时机，即触发通知的事件方法 4.poincut 切点 where如果说通知定义切面的“什么”和“何时”，那么切点就定义了“何处”。切点的定义会匹配通知所要织入的一个或多个连接点。我们通常会使用明确的类和方法名称来指定这些切点，或是利用正则表达式定义匹配的类和方法名称模式来指定这些切点。 5.切面aspect = advice + poincut切面是通知和切点的结合，通知和切点共同定义了关于切面的全部内容—-它是什么，在何时和何处完成其功能 通过 代理 来实现切面，代理类包裹了目标bean，先拦截对被通知方法的调用，再转发给真正的bean，在调用bean之前，先执行切面逻辑。直到需要被代理的bean时，才创建代理对象。正因为spring会在运行时才创建代理对象，所以不需要特殊的编译器织入springAOP切面。 定义切面@AspectJ ：表明该类不仅是一个pojo还是以个切面 @Pointcut 在一个@Aspect切面内定义可重用的切点 @Pointcut（“execution（ concert.Performance.perform(..))”)public void performance() {}将切点依附于performance（）方法，可以在定义切点表达式的时候用performance（）代替execution（ concert.Performance.perform(..)) 环绕通知1234567891011121314151617@Aspectpublic class Audience &#123; @Pointcut("excution(** concert.Performance.perform(..))") public void performance() &#123;&#125; @Around("performance()") public void watchPerformance(ProceedingJoinPoint jp) &#123; try &#123; System.out.println("Silencing cell phones"); System.out.pringln("Taking seats"); jp.proceed(): Sytem.out.println("CLAP CLAP CLAP!!!"); &#125; catch (THrowable e) &#123; SYstem.out.println("Demanding a refund"); &#125; &#125;&#125; 如果不调用proceed（）方法，那么会阻塞被通知方法的调用。可以不调用proceed（）方法，也可以多次调用proceed（）方法，这样的场景就是实现重试逻辑，也就是在被通知方法失败后，进行重复尝试。 切点拦截参数1234567891011@Pointcut("excution(** concert.Performance.perform(..)) &amp;&amp; args(trackNumber") public void trackPlayed(int trackNumber) &#123;&#125; @Before("trackPlayed(trackNumber)") public void countTrack(int trackNumber) &#123; int currentCount = getPlayCount(trackNumber); trackCounts.put(trackNumber, currentCount + 1); &#125; public int getPlayCount(int trackNumber) &#123; return trackCounts.containsKey(trackNumber) ? trackCounts.get(trackNumber) : 0; &#125; 切点定义中的参数与切点方法中的参数名一样就可以实现从命名切点到通知方法的参数转移。 6.Introduction引入向现有的类添加新方法和属性。 给bean添加一个新的功能，需要一个引入代理去实现相关的功能。12345@Aspect public class EncoreableIntroducer &#123; @DeclareParents(value="concert.Performance+", defualtImpl=DefaultEncoreable.class) public static Encoreable ecoreable;&#125; value指定了哪种类型的bean要引入该接口。在这里是所有实现Performance的类型。加号表示Performance的所有子类而不是本身 defaultImplement指定了为引入功能提供实现的类 实现类 @DeclareParents注解所标注的静态属性指明了要 引入的接口。 7.织入把切面应用到目标对象并创建新的代理对象的过程。在目标生命周期有多个点可以进行织入： 编译期 类加载期 运行期 二.spring对AOP的支持Spring默认采取的动态代理机制实现AOP，当动态代理不可用时（代理类无接口）会使用CGlib机制。但Spring的AOP有一定的缺点，第一个只能对方法进行切入，不能对接口，字段，静态代码块进行切入（切入接口的某个方法，则该接口下所有实现类的该方法将被切入）。第二个同类中的互相调用方法将不会使用代理类。因为要使用代理类必须从Spring容器中获取Bean。第三个性能不是最好的 基于代理的经典SpringAOP 纯POJO切面 @AspectJ注解驱动的切面 注入式AspectJ切面 由于Spring AOP构建在动态代理基础之上，所以其对AOP的支持局限于方法拦截。他不支持字段连接点和构造器连接点。 直到应用需要被代理的bean时，spring才会创建代理对象。如果使用的是ApplicationContext的话，在ApplicationContext从BeanFactory中加载所有bean的时候，Spring才会创建被dialing的对象。 动态代理不修改原来的对象就能增加一系列新的功能 jdk自带动态代理 Java在JDK1.3后引入的动态代理机制，使我们可以在运行期,目标类加载后动态的创建代理类 CGlib 使用动态字节码生成技术实现AOP原理是在运行期间目标字节码加载后，生成目标类的子类，将切面逻辑加入到子类中，所以使用Cglib实现AOP不需要基于接口。 JDK中动态代理实现流程在java的动态代理机制中，有两个重要的类和接口，一个是 InvocationHandler(Interface)、另一个则是 Proxy(Class)，这一个类和接口是实现我们动态代理所必须用到的。 使用方法： 创建调用处理器（InvocationHandler)，在其中传入被代理对象 在调用处理器的invoke方法中加入代理逻辑，并调用被代理对象的被代理方法 通过Proxy.newProxyInstance方法获得代理对象 12345678910111213141516171819202122232425262728293031class DynamicProxyHandler implements InvocationHandler &#123; private Object proxied; public DynamicProxyHandler(Object proxied) &#123; this.proxied = proxied; &#125; public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; System.out.println("**** proxy: " + proxy.getClass() + ", method: " + method + ", args: " + args); if(args != null) for(Object arg : args) System.out.println(" " + arg); return method.invoke(proxied, args); &#125;&#125;class SimpleDynamicProxy &#123; public static void consumer(Interface iface) &#123; iface.doSomething(); iface.somethingElse("bonobo"); &#125; public static void main(String[] args) &#123; RealObject real = new RealObject(); Interface proxy = (Interface)Proxy.newProxyInstance( Interface.class.getClassLoader(), new Class[]&#123; Interface.class &#125;, new DynamicProxyHandler(real)); consumer(proxy)； &#125; Proxy.newProxyInstance方法有三个参数，分别是类加载器，一个希望该代理实现的接口列表以及他的被代理对象。 通过 Proxy.newProxyInstance创建的代理对象是在jvm运行时动态生成的一个对象，它并不是我们的InvocationHandler类型，也不是我们定义的那组接口的类型，而是在运行是动态生成的一个对象，并且命名方式都是这样的形式，以$开头，proxy为中，最后一个数字表示对象的标号。 总体逻辑大概是：通过InvocationHandler来包装被代理的方法，再根据（InvocationHandler）和需要代理的接口生成相应的代理对象，通过将相应的调用转发到代理对象，从而实现功能的包装。 由于在Proxy.newProxyInstance中，所有的方法都是通过h.invoke()实现的，这样就让所有的接口方法都被包裹上代理逻辑，也就是说当执行被代理操作的时候在代理对象内部，实际上是使用invoke()方法将请求转发给了调用处理器进行操作。 静态代理既然说了动态代理，那我们就顺便提一下静态代理，虽然他没有动态代理那么好。1234567891011121314151617181920212223242526272829303132333435363738package test;public interface Subject &#123; public void doSomething(); &#125;package test;public class RealSubject implements Subject &#123; public void doSomething() &#123; System.out.println( "call doSomething()" ); &#125; &#125; package test;public class SubjectProxy implements Subject&#123; Subject subimpl = new RealSubject(); public void doSomething() &#123; //可以在这里加入代理逻辑 subimpl.doSomething(); //可以在这里加入代理逻辑 &#125;&#125;package test;public class TestProxy &#123; public static void main(String args[]) &#123; Subject sub = new SubjectProxy(); sub.doSomething(); &#125;&#125; 刚开始我会觉得SubjectProxy定义出来纯属多余，直接实例化实现类完成操作不就结了吗？后来随着业务庞大，你就会知道，实现proxy类对真实类的封装对于粒度的控制有着重要的意义。但是静态代理这个模式本身有个大问题，如果类方法数量越来越多的时候，代理类的代码量是十分庞大的。所以引入动态代理来解决此类问题。 动态代理相对于静态的好处： Proxy类的代码量被固定下来，不会因为业务的逐渐庞大而庞大； 可以实现AOP编程，实际上静态代理也可以实现，总的来说，AOP可以算作是代理模式的一个典型应用； 解耦，通过参数就可以判断真实类，不需要事先实例化，更加灵活多变。这也是为什么Spring这么受欢迎的一个原因，Spring容器代替工厂，Spring AOP代替JDK动态代理，让面向切面编程更容易实现。在Spring的帮助下轻松添加，移除动态代理，且对源代码无任何影响。]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>源码分析</tag>
        <tag>aop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashMap源码解析]]></title>
    <url>%2F2018%2F02%2F24%2FHashMap%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[〇.简介Java为数据结构中的映射定义了一个接口java.util.Map，此接口主要有四个常用的实现类，分别是HashMap、Hashtable、LinkedHashMap和TreeMap，类继承关系如下图所示： 下面针对各个实现类的特点做一些说明： HashMap：它根据键的hashCode值存储数据，大多数情况下可以直接定位到它的值，因而具有很快的访问速度，但遍历顺序却是不确定的。 HashMap最多只允许一条记录的键为null，允许多条记录的值为null。HashMap非线程安全，即任一时刻可以有多个线程同时写HashMap，可能会导致数据的不一致。如果需要满足线程安全，可以用 Collections的synchronizedMap方法使HashMap具有线程安全的能力，或者使用ConcurrentHashMap。 Hashtable：Hashtable是遗留类，很多映射的常用功能与HashMap类似，不同的是它承自Dictionary类，并且是线程安全的，任一时间只有一个线程能写Hashtable，并发性不如ConcurrentHashMap，因为ConcurrentHashMap引入了分段锁。Hashtable不建议在新代码中使用，不需要线程安全的场合可以用HashMap替换，需要线程安全的场合可以用ConcurrentHashMap替换。 LinkedHashMap：LinkedHashMap是HashMap的一个子类，保存了记录的插入顺序，在用Iterator遍历LinkedHashMap时，先得到的记录肯定是先插入的，也可以在构造时带参数，按照访问次序排序。 TreeMap：TreeMap实现SortedMap接口，能够把它保存的记录根据键排序，默认是按键值的升序排序，也可以指定排序的比较器，当用Iterator遍历TreeMap时，得到的记录是排过序的。如果使用排序的映射，建议使用TreeMap。在使用TreeMap时，key必须实现Comparable接口或者在构造TreeMap传入自定义的Comparator，否则会在运行时抛出java.lang.ClassCastException类型的异常。 对于上述四种Map类型的类，要求映射中的key是不可变对象。不可变对象是该对象在创建后它的哈希值不会被改变。如果对象的哈希值发生变化，Map对象很可能就定位不到映射的位置了。 在JDK8中HashMap的实现由原先的数组加链表，也就是通过链地址法，解决哈希冲突，变成了数组加链表加红黑树，如图所示 当链表长度大于8时，链表会变成红黑树，这样做的目的是为了改进之前由于hash函数选择不好导致链表过长的查询瓶颈，之后会具体介绍。 一. 关键参数及构造方法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111/* ---------------- Fields -------------- */ //存储节点的table数组，第一次使用的时候初始化，必要时resize，长度总是2的幂 transient Node&lt;K,V&gt;[] table; //缓存entrySet，用于keySet() and values() transient Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet; //容器中元素的个数 transient int size; //每次扩容和更改map结构的计数器 transient int modCount; //阈值，当实际大小超过阈值时，会进行扩容 int threshold; //装载因子 final float loadFactor; //默认的初始容量，必须是2的幂次，出于优化考虑，默认16 static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; //默认的最大容量 static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; //默认的装载因子，在无参构造器中默认设为该值 static final float DEFAULT_LOAD_FACTOR = 0.75f; //阈值，当链表中节点数大于该阈值后就会转变成红黑树 static final int TREEIFY_THRESHOLD = 8; //与上一个阈值相反，当小于这个阈值后转变回链表 static final int UNTREEIFY_THRESHOLD = 6; // 看源码注释里说是：树的最小的容量，至少是 4 x TREEIFY_THRESHOLD = 32 然后为了避免(resizing 和 treeification thresholds) 设置成64 static final int MIN_TREEIFY_CAPACITY = 64; //基本哈希容器节点 实现Map.Entry接口 static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash;//不可变的哈希值，由关键字key得来 final K key;//不可变的关键字 V value; Node&lt;K,V&gt; next; Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return value; &#125; public final String toString() &#123; return key + "=" + value; &#125; public final int hashCode() &#123;//Node对象的哈希值，关键字key的hashCode()与值value的hashCode()做异或运算 return Objects.hashCode(key) ^ Objects.hashCode(value); &#125; public final V setValue(V newValue) &#123; V oldValue = value; value = newValue; return oldValue; &#125; public final boolean equals(Object o) &#123;//对象相同或同类型且key-value均相同，则返回true if (o == this) return true; if (o instanceof Map.Entry) &#123; Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;)o; if (Objects.equals(key, e.getKey()) &amp;&amp; Objects.equals(value, e.getValue())) return true; &#125; return false; &#125; &#125; /* * 构造函数 */ public HashMap(int initialCapacity, float loadFactor) &#123;//给定初始容量和装载因子，构造一个空的HashMap if (initialCapacity &lt; 0) throw new IllegalArgumentException("Illegal initial capacity: " + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException("Illegal load factor: " + loadFactor); this.loadFactor = loadFactor; this.threshold = tableSizeFor(initialCapacity);//根据指定的容量计算容量，因为必须是2的幂次，虽然将该值赋给threshold，但表示的依然是容量，到时候会重新计算阈值 &#125; public HashMap(int initialCapacity) &#123;//指定初始容量，和默认装载因子0.75构造空HashMap this(initialCapacity, DEFAULT_LOAD_FACTOR); &#125; public HashMap() &#123;//无参，使用默认的初始容量16,和装载因子0.75构造空的HashMap this.loadFactor = DEFAULT_LOAD_FACTOR; &#125; public HashMap(Map&lt;? extends K, ? extends V&gt; m) &#123;//构造一个和给定Map映射相同的HashMap，默认装载因子，初始空间以足够存放给定Map中的映射为准 this.loadFactor = DEFAULT_LOAD_FACTOR; putMapEntries(m, false); &#125; &#125; 注释中给出了相应解释，下面再着重介绍几个参数。 1. table数组也就是我们之前所说的HashMap实现中的数组，通过对key计算hash后得到相应数组的index，数组中存储着相同index的链表首结点或者红黑树的根节点，结点类型就是上面代码中的Node 2. 容量， 装载因子，阈值threshold = loadFactor * capcity 由于这样的一个对应关系，这三个变量在HashMap中只有threshold和loadFactor这两个是明确给出来的。在给出初始容量和装载因子的构造函数中我们可以发现，threshold被作为了初始化的容量变量，他将在第一次调用resize()中被使用。123456789101112public HashMap(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException("Illegal initial capacity: " + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException("Illegal load factor: " + loadFactor); this.loadFactor = loadFactor; this.threshold = tableSizeFor(initialCapacity);&#125; 容量的默认值是16，装载因子默认为0.75, 至于为什么要有装载因子这个设定，而不是在table数组满了再扩容，文档中是这么说的 As a general rule, the default load factor (.75) offers a good tradeoff between time and space costs. Higher values decrease the space overhead but increase the lookup cost (reflected in most of the operations of the HashMap class, including get and put). The expected number of entries in the map and its load factor should be taken into account when setting its initial capacity, so as to minimize the number of rehash operations. If the initial capacity is greater than the maximum number of entries divided by the load factor, no rehash operations will ever occur. 意思总结差不多就是这是时间和空间均衡后的决定。我们也知道hashmap是用空间换时间，0.75这个装载因子能在二者之间达到一个比较好的平衡。反正就是我们不要乱改就好了。 二. 关键方法1.hash()在对hashCode()计算hash时具体实现是这样的：1234static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; 他的执行过程如图所示：其实就是将本身的hashcode的高16位和低16位做了一个异或操作。至于为什么要这么做呢？这里牵扯到了table数组中index的计算。 1index = (n - 1) &amp; hash n为数组的长度，而table长度n为2的幂，而计算table数组下标的时候，举个例子，加入n=16，那么n-1=15的二进制表示就是0x0000 1111，可以看出，任何一个2的幂次减1后二进制肯定都是这种形式，它的意义在于，任何一个值和它做&amp;操作，得到的结构肯定都在0~(n-1)之间，也就是说计算出来的下标值肯定数组的合法下标，这种方式由于使用了位运算比单纯的取模更快。 但问题也来了，设计者认为这方法很容易发生碰撞。为什么这么说呢？不妨思考一下，在n - 1为15(0x1111)时，其实散列真正生效的只是低4bit的有效位，当然容易碰撞了。 因此，设计者想了一个顾全大局的方法(综合考虑了速度、作用、质量)，就是把高16bit和低16bit异或了一下。设计者还解释到因为现在大多数的hashCode的分布已经很不错了，就算是发生了碰撞也用O(logn)的tree去做了。仅仅异或一下，既减少了系统的开销，也不会造成的因为高位没有参与下标的计算(table长度比较小时)，从而引起的碰撞。 2.putput方法的流程如下： 如果table数组为空，那么调用resize()方法新建数组 对key的hashCode()做hash，然后再计算index; 如果没碰撞直接放到bucket里； 如果碰撞了，放在以链表的形式存在buckets后； 如果碰撞导致链表过长(大于等于TREEIFY_THRESHOLD)，就把链表转换成红黑树； 如果节点已经存在就替换old value(保证key的唯一性) 如果bucket满了(超过load factor*current capacity)，就要resize12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true); &#125; /* * 实现Map.put以及相关方法 * 向map中加入个节点 * 没有分析onlyIfAbsent和evict */ final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K, V&gt;[] tab;//指向table数组 Node&lt;K, V&gt; p;//对应下标中的第一个节点，为null说明没有碰撞，不为null代表链表第一个元素或红黑树根节点 int n, i;//n为table数组的长度，2的幂次; i表示对应的下标index if ((tab = table) == null || (n = tab.length) == 0) // 如果table为空即第一次添加元素，则进行初始化 n = (tab = resize()).length; /* * 计算下标，根据hash与n计算index * 公式:i = (n - 1) &amp; hash; */ // p=table[i]; 对应下标中的第一个节点 if ((p = tab[i = (n - 1) &amp; hash]) == null) // p为null说明没有碰撞， tab[i] = newNode(hash, key, value, null);//直接新建一个节点加入就可以了 else &#123;// p不为null，说明有碰撞 Node&lt;K, V&gt; e;//e，代表map中与给定key值相同的节点 K k;//代表e的key // p的关键字与要加入的关键字相同，则p就是要找的e if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; // 如果p的类型是红黑树，则向红黑树中查找e else if (p instanceof TreeNode) e = ((TreeNode&lt;K, V&gt;) p).putTreeVal(this, tab, hash, key, value); // 否则就是链表 else &#123; for (int binCount = 0;; ++binCount) &#123;//遍历链表查找e，如果找不到就新建一个 if ((e = p.next) == null) &#123;// 如果next为null，说明没有找到 p.next = newNode(hash, key, value, null);// 那么新创建一个节点 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // 加入节点后如果超出树形化阈值 treeifyBin(tab, hash);// 则转换为红黑树 break; &#125; if (e.hash == hash &amp;&amp; // 找到关键字相同的节点，退出循环 ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; if (e != null) &#123; //e不为null，说明原来存在对应的key，那么返回原来的值 V oldValue = e.value;// 保留原来的值，用于返回 if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; //说明新插入了一个节点，返回null ++modCount; if (++size &gt; threshold) // 超过临界值，则resize resize(); afterNodeInsertion(evict); return null; &#125; 3. get() 和 containsKey()方法这两个方法是最常用的，都是根据给定的key值，一个获取对应的value，一个判断是否存在于Map中，在内部这两个方法都会调用一个finall方法，就是getNode()，也就是查找对应key值的节点。 getNode方法的大致过程： table里的第一个节点，直接命中； 如果有冲突，则遍历链表或二叉树去查找相同节点 查找节点时先判断hash值是否相等 如果hash值相等，再判断key值是否相等 判断key值相等时，用==或equals或，整个判断条件为： (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k))))12345678910111213141516171819202122232425262728293031public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value; &#125; public boolean containsKey(Object key) &#123; return getNode(hash(key), key) != null; &#125; /* * 实现Map.get以及相关方法 */ final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; //指向table数组 Node&lt;K,V&gt; first, e; //first为table[index]，即所在数组下标中第一个节点；e用于遍历节点 int n; K k;//n为table的长度，k用于指向节点的key if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp;//首先必须保证table数组不为空 (first = tab[(n - 1) &amp; hash]) != null) &#123;//计算下标，保证数组下标中第一个节点不为null不然就肯定找不到直接返回null if (first.hash == hash &amp;&amp; // 先检查第一个节点hash值是否相等 ((k = first.key) == key || (key != null &amp;&amp; key.equals(k))))//再判断key，如果相等直接返回 return first; if ((e = first.next) != null) &#123; //第一个不符合，就从下一个开始找 if (first instanceof TreeNode)//红黑树 O(logn) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); do &#123;//不然就是链表O(n) if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null; &#125; 4. resize()从put函数我们不难看出，当table数组为空，或者当加入某个元素后超过阈值，都会调用resize()进行扩容，他的目的就在于将链表和红黑树分散，使得碰撞分散，提高查询效率。简单来说就是下面的步骤： 将容量和阈值扩大两倍，如果超过最大值就使用最大值最为新的容量和阈值 新建一个大小为新容量的table，然后将之前的结点放进去 这里有一个很有意思的小技巧，还记得我们的index是怎么计算的吗？1index = (n - 1) &amp; hash 假设我们的容量没有超标，由于容量都是2的幂，这里的n扩大2倍，相当于在原来的n-1的基础上高位增加了一个1，说白了就是多取了一位的hash。如图所示所以，元素的位置要么是在原位置，要么是在原位置再移动2次幂的位置，因此元素在重新计算hash之后，因为n变为2倍，那么n-1的mask范围在高位多1bit(红色)，因此新的index就会发生这样的变化： 因此，我们在扩充HashMap的时候，不需要重新计算hash，只需要看看原来的hash值新增的那个bit是1还是0就好了，是0的话索引没变，是1的话索引变成“原索引+oldCap”。可以看看下图为16扩充为32的resize示意图：这个设计确实非常的巧妙，既省去了重新计算hash值的时间，而且同时，由于新增的1bit是0还是1可以认为是随机的，因此resize的过程，均匀的把之前的冲突的节点分散到新的bucket了。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; /* * step1: 先根据容量和阈值确定新的容量和阈值 */ //case1: 如果table已经被初始化，说明不是第一次加入元素 if (oldCap &gt; 0) &#123; if (oldCap &gt;= MAXIMUM_CAPACITY) &#123;//如果table的容量已经达到最大值，那么就不再扩容了，碰撞也没办法 threshold = Integer.MAX_VALUE;//那么扩大阈值到最大值 return oldTab;//原来的table不变 &#125; //不然的话table的容量扩大2倍，newCap = oldCap &lt;&lt; 1 大部分情况下肯定都是这种情况 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; //阈值也扩大2倍 &#125; //case2: table没有被初始化，但是阈值大于0，说明在构造函数中指定了容量，但是容量存在阈值那个变量上 else if (oldThr &gt; 0) newCap = oldThr;//那么将阈值设置为table的容量，下面还会重新计算阈值 //case3: table和阈值都没有初始化，说明是无参构造函数 else &#123; newCap = DEFAULT_INITIAL_CAPACITY;//使用默认的初始容量 newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);//计算默认的阈值，threshold=load_factor*capacity &#125; //重新计算阈值 if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; /* * step2: 更新阈值和新容量的table */ threshold = newThr; @SuppressWarnings(&#123;"rawtypes","unchecked"&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; /* * step3: 将原来table中元素，加入到新的table中 */ if (oldTab != null) &#123; for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; //e = oldTab[j] if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; if (e.next == null) //e所在位置没有哈希冲突，只有一个元素，直接计算 newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) //e所在位置是一颗红黑树 ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123;// e所在位置是一个链表，则遍历链表 // 根据e.hash &amp; oldCap) == 0，确定放入lo还是hi两个链表 // 其实就是判断e.hash是否大于oldCap // lo和hi两个链表放分别放在在newTab[j]和newTab[j + oldCap] Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab; &#125; 三. 性能探讨1.hashcode()HashMap的查询效率我们理论上看做是O(1),这是在没有发生冲突的情况下，但是当发生冲突较严重的时候，我们会浪费很多的时间在链表的查询或者红黑树的查询，以至于退化为O(n)或者O(logn),所以当作为key的类型，其Hashcode()函数的设计尤为重要。 2. Key的要求由上一条可知，一个作为key的类型，首先需要有一个设计良好的Hashcode函数，其次我们发现，在get函数中，我们首先判断hashcode相等，再判断equals()或者==来判断是否为同一个对象，因为hashcode相等的两个对象不一定相等，由此可见作为key的另一个条件时重写了equals方法。最后还有一个隐藏条件，key需要为不可变对象比如String，什么叫不可变对象呢？不可变对象就是创建后状态不能修改的对象，因为只有这样才能确保hashcode不发生变化，才能保证能找到相应的key，总结起来就是一下三个： 重写hashcode（） 重写equals（） 不可变对象 四. 引用 https://tech.meituan.com/java-hashmap.htmlhttp://paine1690.github.io/2016/11/12/Java/JDK/HashMap%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/http://yikun.github.io/2015/04/01/Java-HashMap%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>源码分析</tag>
        <tag>集合</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[缓存穿透、缓存雪崩、hot key]]></title>
    <url>%2F2018%2F02%2F03%2F%E7%BC%93%E5%AD%98%E7%A9%BF%E9%80%8F%E3%80%81%E7%BC%93%E5%AD%98%E9%9B%AA%E5%B4%A9%E3%80%81hot-key%2F</url>
    <content type="text"><![CDATA[在做排行榜的时候，对缓存的更新频率产生了一定的疑问，在网上也看了不少博客对这方面的介绍，这里对看到的知识做个总结。 缓存穿透缓存穿透是指查询一个一定不存在的数据，由于缓存是不命中时要查询数据库，并且出于容错考虑，如果从存储层查不到数据则不写入缓存，这将导致这个不存在的数据每次请求都要到存储层去查询，失去了缓存的意义。在流量大时，可能DB就挂掉了，要是有人利用不存在的key频繁攻击我们的应用，这就是漏洞 解决方法1. 空值缓存这是一个比较简单暴力的方法，如果一个查询返回的数据为空（不管是数据不存在，还是系统故障），我们仍然把这个空结果进行缓存，但它的过期时间会很短，最长不超过五分钟。 缺点 空值做了缓存，意味着缓存层中存了更多的键，需要更多的内存空间 ( 如果是攻击，问题更严重 )，比较有效的方法是针对这类数据设置一个较短的过期时间，让其自动剔除。 缓存层和存储层的数据会有一段时间窗口的不一致，可能会对业务有一定影响。例如过期时间设置为 5 分钟，如果此时存储层添加了这个数据，那此段时间就会出现缓存层和存储层数据的不一致，此时可以利用消息系统或者其他方式清除掉缓存层中的空对象。 2. Bloom FilterBloom Filter是一个占用空间很小、效率很高的随机数据结构，它由一个bit数组和一组Hash算法构成。可用于判断一个元素是否在一个集合中，查询效率很高（1-N，最优能逼近于1）。在很多场景下，我们都需要一个能迅速判断一个元素是否在一个集合中。譬如： 网页爬虫对URL的去重，避免爬取相同的URL地址； 反垃圾邮件，从数十亿个垃圾邮件列表中判断某邮箱是否垃圾邮箱（同理，垃圾短信）； 缓存击穿，将已存在的缓存放到布隆中，当黑客访问不存在的缓存时迅速返回避免缓存及DB挂掉。 原理初始化状态是一个全为0的bit数组 为了表达存储N个元素的集合，使用K个独立的函数来进行哈希运算。x1，x2……xk为k个哈希算法。如果集合元素有N1，N2……NN，N1经过x1运算后得到的结果映射的位置标1，经过x2运算后结果映射也标1，已经为1的报错1不变。经过k次散列后，对N1的散列完成。依次对N2，NN等所有数据进行散列，最终得到一个部分为1，部分位为0的字节数组。当然了，这个字节数组会比较长，不然散列效果不好。 那么怎么判断一个外来的元素是否已经在集合里呢，譬如已经散列了10亿个垃圾邮箱，现在来了一个邮箱，怎么判断它是否在这10亿里面呢？很简单，就拿这个新来的也依次经历x1，x2……xk个哈希算法即可。在任何一个哈希算法譬如到x2时，得到的映射值有0，那就说明这个邮箱肯定不在这10亿内。如果是一个黑名单对象，那么可以肯定的是所有映射都为1，肯定跑不了它。也就是说是坏人，一定会被抓。那么误伤是为什么呢，就是指一些非黑名单对象的值经过k次哈希后，也全部为1，但它确实不是黑名单里的值，这种概率是存在的，但是是可控的。 至于具体实现，可以直接调用com.google.guava中的BloomFilter，就不赘述了。 缓存雪崩平时我们设定一个缓存的过期时间时，可能有一些会设置1分钟啊，5分钟这些，并发很高时可能会出在某一个时间同时生成了很多的缓存，并且过期时间都一样，这个时候就可能引发一当过期时间到后，这些缓存同时失效，请求全部转发到DB，DB可能会压力过重。 解决方法 将缓存失效时间分散开，比如我们可以在原有的失效时间基础上增加一个随机值，比如1-5分钟随机，这样每一个缓存的过期时间的重复率就会降低，就很难引发集体失效的事件。 加锁或者队列的方式保证缓存的单线 程（进程）写，从而避免失效时大量的并发请求落到底层存储系统上。 热key重建开发人员使用缓存 + 过期时间的策略既可以加速数据读写，又保证数据的定期更新，这种模式基本能够满足绝大部分需求。但是有两个问题如果同时出现，可能就会对应用造成致命的危害： 当前 key 是一个热点 key( 例如一个热门的娱乐新闻），并发量非常大。 重建缓存不能在短时间完成，可能是一个复杂计算，例如复杂的 SQL、多次IO、多个依赖等。 在缓存失效的瞬间，有大量线程来重建缓存 ( 如下图)，造成后端负载加大，甚至可能会让应用崩溃。 要解决这个问题也不是很复杂，但是不能为了解决这个问题给系统带来更多的麻烦，所以需要制定如下目标： 减少重建缓存的次数 数据尽可能一致 较少的潜在危险 解决方案1. 互斥锁此方法只允许一个线程重建缓存，其他线程等待重建缓存的线程执行完，重新从缓存获取数据即可。 这种方案思路比较简单，但是存在一定的隐患，如果构建缓存过程出现问题或者时间较长，可能会存在死锁和线程池阻塞的风险，但是这种方法能够较好的降低后端存储负载并在一致性上做的比较好。 2. 不设置超时时间“永远不过期”包含两层意思： 从缓存层面来看，确实没有设置过期时间，所以不会出现热点key过期后产生的问题，也就是“物理”不过期。 从功能层面来看，为每个value设置一个逻辑过期时间，当发现超过逻辑过期时间后，会使用单独的线程去构建缓存。 这种方案由于没有设置真正的过期时间，实际上已经不存在热点 key 产生的一系列危害，但是会存在数据不一致的情况，同时代码复杂度会增大。 http://mp.weixin.qq.com/s/TBCEwLVAXdsTszRVpXhVughttp://blog.csdn.net/tianyaleixiaowu/article/details/74721877http://blog.csdn.net/zeb_perfect/article/details/54135506https://zhuanlan.zhihu.com/p/26151305]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>缓存</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis排行榜]]></title>
    <url>%2F2018%2F02%2F03%2Fredis%E6%8E%92%E8%A1%8C%E6%A6%9C%2F</url>
    <content type="text"><![CDATA[设计思路因为看了一段时间的redis，准备动手做一个小demo，做一个排行榜，正好加在之前的未完成的新闻门户里面。关于排行榜他有一些跟排行榜本身相关的要求比如： 排行精确性如果一个排行榜的结果关系到用户的权益问题，这个时候一个排行榜的精确性就需要非常高，比如一个运营同学进行了根据微博转发数量的营销活动，这个时候微博转发数量的排行榜就需要非常精确，否则会影响用户权益的分发。 排行榜实时性游戏和社交互动的结合是目前的趋势，对于热门游戏的排行是用户的关注重点，在这部分用户中对于排行的实时性有很高的要求，如果一个用户升级了自己的装备和能力，而自己的排名一直没有更新，那这个用户一定要非常伤心抛弃这个游戏了。所以通过离线计算等平台来构建一个非实时的排行榜系统就不太适合这样的模型。 海量数据排行海量数据是目前的一个趋势，比如对于淘宝全网商品的一个排行，这个榜单将会是一个亿级别的，所以我们设计的榜单也需要具备弹性伸缩能力，同时在对海量数据进行排行的时候拥有一定的实时性。 实现方法目的是要实现一个热点新闻排行榜的话，毫无疑问，使用的是redis内置的zset这种数据结构，他可以根据score自动产生rank比较方便。我们将评论或者点赞数超过200的认为是热门文章 由于文章是从别的地方爬过来的，所以只有评论数没有点赞数，设置初始化分数为： score = 发布时间毫秒数 + 432 * 评论数 而问题就在排行榜更新的频率，更新过快，缓存效果不好，会产生类似重建热key的问题（下一篇文章要讲一下），但是频率过慢又不能达到实时性，所以正如之前所说的，要根据排行榜自身的要求制定一个适合的更新策略： 针对自身的这个项目需求，我想实现的是一个热点新闻排行榜，他的时效性要求并不是很高，所以通过分析网易新闻的爬取量，对爬到的每个新闻建立一个news：id进行初始化，并设置一周后过期自动删除，排行榜肯定是用zset的，但是为了不刷新过快，再建立一个zset缓存最近一个星期的文章，通过一个定时任务，每周一次定时维护time：，从time: 删除时间超过一个星期的文章，并重置score：，由于爬虫每隔6小时更新一次，且新闻量相对较小，所以对time：的频繁读写是可以容忍的。所以总的来说就是 爬取新闻，建立新的hash(news:id),设置过期时间为一周，并加入zset(time：) 每周执行一次更新，删除zset：中过期的任务，对未过期的任务分数进行更新。 score： zset news：id 分数 time： zset news：id 时间 news：id hash voted 投票数 title xxx url xxx 这里介绍一个在网上看到的实时排行榜的设计策略，其思路类似于维护一个小顶堆： 第一次访问的时候，查数据库，查整个表查出topN（使用sql排序），丢给redis(使用sorted set数据类型)。 排序在redis，redis自动排序。以后的用户访问：均访问redis。 只要每次积分变化判断的时候拿topN的最后一个判别，大于最后一名，则整个user丢进redis排序。效率性能再优化：用户积分变动的时候，（守护线程）服务器预存一下变化的数量。。到一定量再通知。 再往下去设定一个小距离为阈值。比如现在第50名的积分是100，那80分一下的应该就没必要扔给redis了吧？ 注意：这个排行榜的用户是会不断增加的，比如1亿用户，如果刚开始只有前50，后5千万人的积分大于第50名，那么就会往redis加入这个用户的信息。（虽然看起来要存很多，其实一亿用户怎么存也就1G左右的内存，简单暴力优雅方案了）1234567891011121314151617181920@Autowiredprivate RedisDao redisDao;private final int ONE_WEEK_IN_SECONDS = 7 * 86400;@Scheduled(cron = "0 0 0 1/7 * ?")public void updataRank() &#123; redisDao.zRemRangeByRank("score:", 0, -1); long cutOff = System.currentTimeMillis() / 1000 - ONE_WEEK_IN_SECONDS; Set&lt;TypedTuple&lt;Object&gt;&gt; set = redisDao.zRangeWithScores("time:", 0, -1); for (TypedTuple&lt;Object&gt; o: set) &#123; //如果过期直接删除，否则计算结果 BigDecimal db = new BigDecimal(o.getScore().toString());db.toPlainString(); if (Long.valueOf(db.toPlainString()) &lt; cutOff) &#123; redisDao.zrem("time:", o.getValue()); &#125; else &#123; redisDao.zadd("score:", o.getValue(), Double.valueOf(o.getScore().toString()) + 432 * Double.valueOf(redisDao.hget(o.getValue().toString(), "voted").toString())); &#125; &#125;&#125; 成品效果如图（忽略我这个丑陋的前端)： 新闻爬虫2.0由于这次修改也涉及到了之前爬取数据的爬虫，索性就把爬虫也一并进行了修改，对整个爬虫进行了重构，使用多线程对爬虫进行优化，具体步骤如下： 将爬虫分为两个部分，使用生产者和消费者模式，将redis作为任务队列，生产者爬虫爬取新闻url，消费者爬虫根据新闻url爬取具体信息。使用2个redis集合存储已爬新闻和未爬新闻，作为简单去重。 完整代码请参考： 新闻门户代码：https://github.com/MoriatyC/OmegaNews 爬虫代码：https://github.com/MoriatyC/nethard]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>缓存</tag>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis数据安全与性能保障]]></title>
    <url>%2F2018%2F01%2F29%2Fredis%E6%95%B0%E6%8D%AE%E5%AE%89%E5%85%A8%E4%B8%8E%E6%80%A7%E8%83%BD%E4%BF%9D%E9%9A%9C%2F</url>
    <content type="text"><![CDATA[一.持久化1. 快照： 将存在于某一时刻的所有数据都写入硬盘里面方法： 客户端通过向redis发送bgsave命令（创建子进程） 客户端通过向redis发送save命令，但是会阻塞其他命令，所以只有内存不够，或者不怕阻塞的时候才可以用。但是不要创建子进程，不会导致redis停顿，并且由于没有子进程抢资源所以比bgsave快。 设置了save选项：比如 save 60 10000，表示从最近一次创建快照之后开始算起，当有60s内有10000次写入的时候就会触发bgsave命令，可以有多个save配置，任意一个满足即可。 通过shutdown接收到关闭请求时，或者接收到标准的term信号，执行save命令 当一个redis服务器连接另一个redis服务器，想对方发送sync时，若主服务器没执行bgsave，或者并非刚刚执行完，那么主服务器就会执行bgsave。 缺点：当redis、系统或者硬件中的一个发生崩溃，将丢失最近一次创建快照后的数据。TIPS: 将开发环境尽可能的模拟生产环境以得到正确的快照生成速率配置。 2. AOF：在执行写命令时，将被执行的写命令复制到硬盘里面使用appendonlyyes配置选项打开，下图是appendfsync配置选项。 选项目 同步频率 always 每个写操作都要同步写入，严重降低redis速度损耗硬盘寿命 everysec 每秒执行一次，将多个写入同步，墙裂推荐 no 让os决定，不稳定，不知道会丢失多少数据 自动配置aof重写： auto-aof-rewrite-percentage 100 auto-aof-rrewrite-min-size 64当启用aof持久化之后，当aof文件体积大于64mb并且体积比上一次大了100%，就会执行bgrewriteaof命令。 缺点：1.aof文件过大，2. 文件过大导致还原事件过长。但是可以对其进行重写压缩。 二. 复制就像之前所说当一个从服务器连接一个主服务器的时候，主服务器会创建一个快照文件并将其发送到从服务器。 在配置中包含slaveof host port选项指定主服务器，启动时候会先执行aof或者快照文件。 也可以通过发送flaveof no one命令来终止复制操作，通过slaveof host port命令来开始复制一个主服务器，会直接执行下面的连接操作。 步骤 主服务器操作 从服务器操作 1 （等待命令） 连接主服务器，发送sync命令 2 开始执行bgsave，并使用缓冲区记录bgsave之后执行的所有写命令 根据配置选项决定使用现有数据处理客户端请求还是返回错误 3 Bgsave执行完毕，向从服务器发快照，并在发送期间继续用缓冲区记录写命令 丢弃所有旧数据，载入快照文件 4 快照发送完毕，向从服务器发送缓冲区里的写命令 完成快照解释，开始接受命令 5 缓冲区存储的写命令发送完毕：从现在起每执行一个写命令都发给从服务器 执行主服务器发来的所有存储在缓冲区里的写；并接受执行主服务器发来的写命令 三. 处理故障系统验证快照和aof文件 redis-check-aof redis-check-dump 检查aof和快照文件的状态，在有需要的情况下对aof文件进行修复。 更换新的故障主服务器假设A为主服务器，B为从服务器，当机器A发生故障的时候，更换服务器的步骤如下：首先向机器B发送一个save命令，将这个快照文件发送给机器C，在C上启动Redis，让B成为C的从服务器。 将从服务器升级为主服务器将从服务器升级为主服务器，为升级后的主服务器创建从服务器。 redis事务四. 事务multi: 标记一个事务块的开始。 事务块内的多条命令会按照先后顺序被放进一个队列当中，最后由 EXEC 命令原子性(atomic)地执行。 exec: 执行所有事务块内的命令。 假如某个(或某些) key 正处于 WATCH 命令的监视之下，且事务块中有和这个(或这些) key 相关的命令，那么 EXEC 命令只在这个(或这些) key 没有被其他命令所改动的情况下执行并生效，否则该事务被打断(abort)。 redis的事务包裹在multi命令和exec命令之中，在jedis中通过如下实现12345678910111213141516171819202122232425262728293031323334 public class RedisJava extends Thread&#123; static Response&lt;String&gt; ret; Jedis conn = new Jedis("localhost"); @Override public void run() &#123; Transaction t = conn.multi(); t.incr("notrans:"); Response&lt;String&gt; result1 = t.get("notrans:"); try &#123; Thread.sleep(1L); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; t.incrBy("notrans:", -1); t.exec(); String foolbar = result1.get(); System.out.println(foolbar); &#125; public static void main(String[] args) &#123; Jedis conn = new Jedis("localhost"); Thread t1 = new RedisJava(); Thread t2 = new RedisJava(); Thread t3 = new RedisJava(); t1.start(); t2.start(); t3.start(); &#125;&#125; wathc：监视一个(或多个) key ，如果在事务执行之前这个(或这些) key 被其他命令所改动，那么事务将被打断。 unwatch：取消 WATCH 命令对所有 key 的监视。如果在执行 WATCH 命令之后， EXEC 命令或 DISCARD 命令先被执行了的话，那么就不需要再执行 UNWATCH 了。的监视，因此这两个命令执行之后，就没有必要执行 UNWATCH 了。 discard :取消事务，放弃执行事务块内的所有命令。取消watch，清空任务队列。如果正在使用 WATCH 命令监视某个(或某些) key，那么取消所有监视，等同于执行命令 UNWATCH 。 一个简单的商品买卖demo如下： key type inventory：id set market zset user:id hash 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859public boolean listItem( Jedis conn, String itemId, String sellerId, double price) &#123; String inventory = "inventory:" + sellerId; String item = itemId + '.' + sellerId; long end = System.currentTimeMillis() + 5000; while (System.currentTimeMillis() &lt; end) &#123; conn.watch(inventory); if (!conn.sismember(inventory, itemId))&#123; conn.unwatch(); return false; &#125; Transaction trans = conn.multi(); trans.zadd("market:", price, item); trans.srem(inventory, itemId); List&lt;Object&gt; results = trans.exec(); // null response indicates that the transaction was aborted due to // the watched key changing. if (results == null)&#123; continue; &#125; return true; &#125; return false; &#125;public boolean purchaseItem( Jedis conn, String buyerId, String itemId, String sellerId, double lprice) &#123; String buyer = "users:" + buyerId; String seller = "users:" + sellerId; String item = itemId + '.' + sellerId; String inventory = "inventory:" + buyerId; long end = System.currentTimeMillis() + 10000; while (System.currentTimeMillis() &lt; end)&#123; conn.watch("market:", buyer); double price = conn.zscore("market:", item); double funds = Double.parseDouble(conn.hget(buyer, "funds")); if (price != lprice || price &gt; funds)&#123; conn.unwatch(); return false; &#125; Transaction trans = conn.multi(); trans.hincrBy(seller, "funds", (int)price); trans.hincrBy(buyer, "funds", (int)-price); trans.sadd(inventory, itemId); trans.zrem("market:", item); List&lt;Object&gt; results = trans.exec(); // null response indicates that the transaction was aborted due to // the watched key changing. if (results == null)&#123; continue; &#125; return true; &#125; 总结：相比于一般关系型数据库的悲观锁，redis的事务是典型的乐观锁，没有对事务进行封锁，以避免客户端运行过慢造成长时间的阻塞 非事务型流水线使用流水线，减少通信次数提高性能，以jedis为例，对比使用和没使用流水线的函数方法调用次数：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public void updateTokenPipeline(Jedis conn, String token, String user, String item) &#123; long timestamp = System.currentTimeMillis() / 1000; Pipeline pipe = conn.pipelined(); pipe.multi(); pipe.hset("login:", token, user); pipe.zadd("recent:", timestamp, token); if (item != null)&#123; pipe.zadd("viewed:" + token, timestamp, item); pipe.zremrangeByRank("viewed:" + token, 0, -26); pipe.zincrby("viewed:", -1, item); &#125; pipe.exec();&#125;//对比没有使用流水线的方法public void updateToken(Jedis conn, String token, String user, String item) &#123; long timestamp = System.currentTimeMillis() / 1000; conn.hset("login:", token, user); conn.zadd("recent:", timestamp, token); if (item != null) &#123; conn.zadd("viewed:" + token, timestamp, item); conn.zremrangeByRank("viewed:" + token, 0, -26); conn.zincrby("viewed:", -1, item); &#125;&#125;//测试函数如下 public void benchmarkUpdateToken(Jedis conn, int duration) &#123; try&#123; @SuppressWarnings("rawtypes") Class[] args = new Class[]&#123; Jedis.class, String.class, String.class, String.class&#125;; Method[] methods = new Method[]&#123; this.getClass().getDeclaredMethod("updateToken", args), this.getClass().getDeclaredMethod("updateTokenPipeline", args), &#125;; for (Method method : methods)&#123; int count = 0; long start = System.currentTimeMillis(); long end = start + (duration * 1000); while (System.currentTimeMillis() &lt; end)&#123; count++; method.invoke(this, conn, "token", "user", "item"); &#125; long delta = System.currentTimeMillis() - start; System.out.println( method.getName() + ' ' + count + ' ' + (delta / 1000) + ' ' + (count / (delta / 1000))); &#125; &#125;catch(Exception e)&#123; throw new RuntimeException(e); &#125;&#125; 运行结果如图所示，在本地运行性能提升大概17.8倍。 tips：可以使用redis-benchmark工具进行性能测试。 五. References 《Redis实战》]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>nosql</tag>
        <tag>持久化</tag>
        <tag>事务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线程池(二)]]></title>
    <url>%2F2018%2F01%2F18%2F%E7%BA%BF%E7%A8%8B%E6%B1%A0(%E4%BA%8C)%2F</url>
    <content type="text"><![CDATA[性能问题饥饿死锁如果线程池中的任务依赖于之后提交的子任务，当线程池不够大的时候，很容易造成饥饿死锁。所以最好在线程池中加入的是同类型的独立任务。 运行时间较长的任务如果线程运行时间较长也会影响任务的相应性，同样造成不好的体验，所以api有很多方法都带有一个限时版本。 线程池大小线程池的大小需要分析计算环境，资源预算和任务特性。 一般来说在知道了系统中有多少个cpu和内存的基础下，任务类型是最为重要的。 对于计算密集型的任务线程池大小为cpu数+1，以实现尽可能的满载利用率 对于i/o密集型，由于线程不会一直执行，所以规模更大。这里给出一个《Java并发变成实战》这本书提出的一个公式 最佳线程数目 = （线程等待时间/线程CPU时间之比 + 1） CPU数目cpu利用率 即等待时间越长，需要更多的线程。当我们不需要一个那么精准的线程数目时，也可以用这个公式 最佳线程数目 = 2N+1(N为CPU数目) 是否使用线程池就一定比使用单线程高效呢？答案是否定的，比如Redis就是单线程的，但它却非常高效，基本操作都能达到十万量级/s。从线程这个角度来看，部分原因在于： 多线程带来线程上下文切换开销，单线程就没有这种开销 锁 当然“Redis很快”更本质的原因在于：Redis基本都是内存操作，这种情况下单线程可以很高效地利用CPU。而多线程适用场景一般是：存在相当比例的IO和网络操作。 扩展线程池ThreadPoolExecutor提供了几个可以在子类化中该学的方法： beforeExecute afterExecute terminated 如果beforeExecute抛出一个RuntimeException，那么任务将不被执行，并且afterExecute也不会被调用。 但是无论人物从run中正常返回还是抛出一个异常返回，afterExecute都会被调用，如果任务在完成后带有一个Error，那么久不会调用。 所有任务都已经完成并且所有工作者线程也已经关闭后，terminated会被调用。 给出一个demo，他通过这一些列方法来统计任务执行并添加日志。123456789101112131415161718192021222324252627282930313233343536373839public class TimingThreadPool extends ThreadPoolExecutor &#123; public TimingThreadPool() &#123; super(1, 1, 0L, TimeUnit.SECONDS, null); &#125; private final ThreadLocal&lt;Long&gt; startTime = new ThreadLocal&lt;Long&gt;(); private final Logger log = Logger.getLogger("TimingThreadPool"); private final AtomicLong numTasks = new AtomicLong(); private final AtomicLong totalTime = new AtomicLong(); protected void beforeExecute(Thread t, Runnable r) &#123; super.beforeExecute(t, r); log.fine(String.format("Thread %s: start %s", t, r)); startTime.set(System.nanoTime()); &#125; protected void afterExecute(Runnable r, Throwable t) &#123; try &#123; long endTime = System.nanoTime(); long taskTime = endTime - startTime.get(); numTasks.incrementAndGet(); totalTime.addAndGet(taskTime); log.fine(String.format("Thread %s: end %s, time=%dns", t, r, taskTime)); &#125; finally &#123; super.afterExecute(r, t); &#125; &#125; protected void terminated() &#123; try &#123; log.info(String.format("Terminated: avg time=%dns", totalTime.get() / numTasks.get())); &#125; finally &#123; super.terminated(); &#125; &#125;&#125; 引用 http://ifeve.com/how-to-calculate-threadpool-size/《Java并发编程实战》]]></content>
      <categories>
        <category>并发</category>
      </categories>
      <tags>
        <tag>多线程</tag>
        <tag>容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线程池(一)]]></title>
    <url>%2F2018%2F01%2F17%2F%E7%BA%BF%E7%A8%8B%E6%B1%A0(%E4%B8%80)%2F</url>
    <content type="text"><![CDATA[概述这几天准备深入学习有关并发的知识，所以先简单复习了一下JDK自带的并发包，其中首先比较重要的一个就是线程池了。 为什么不无限的创造线程？主要基于以下几个原因： 线程生命周期的开销非常高 资源消耗 稳定性 所谓物极必反，线程的创建和销毁和需要一定的时间，如果所创建的线程工作时间还不如创建销毁的时间长那是得不偿失的，并且当线程创建过多也会对内存造成一定的负担甚至溢出，并且对GC也是极大的消耗，由于存在一定数额的活跃线程也提高了响应性。 线程池根据《阿里巴巴Java开发手册》中对线程创建的要求 【强制】线程资源必须通过线程池提供，不允许在应用中自行显式创建线程 由此可见，在正式生产环境中，线程池是唯一的创建线程的方法。而JDK对线程池也有强大的支持。 根据《手册》中的另一点要求 【强制】线程池不允许使用 Executors 去创建，而是通过 ThreadPoolExecutor 的方式，这样的处理方式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险。说明： Executors 返回的线程池对象的弊端如下：1） FixedThreadPool 和 SingleThreadPool:允许的请求队列长度为 Integer.MAX_VALUE，可能会堆积大量的请求，从而导致 OOM。2） CachedThreadPool 和 ScheduledThreadPool:允许的创建线程数量为 Integer.MAX_VALUE， 可能会创建大量的线程，从而导致 OOM 虽然Executor为我们提供了很多方便的工厂方法，比如newSingleThreadExecutor(),也有Executors为我们很好的实现了这些工厂方法，但是手动实现ThreadPoolExecutor能让我们对线程池有更深的了解和控制。所以接下来让我们来介绍一下ThreadPoolExecutor这个类。 原理一个最常见的ThreadPoolExecutor构造函数如下 1234567ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) corePoolSize： 活动线程数 maximumPoolSize： 线程池上限 keepAliveTime： 当线程池中线程数超过corePoolSize后，完成工作后的线程存活时间 unit： 单位其余的几个参数我们会在后面着重介绍。 这里介绍一下ThreadPoolExecutor的核心工作原理 123456789101112131415int c = ctl.get(); if (workerCountOf(c) &lt; corePoolSize) &#123; if (addWorker(command, true)) return; c = ctl.get(); &#125; if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123; int recheck = ctl.get(); if (! isRunning(recheck) &amp;&amp; remove(command)) reject(command); else if (workerCountOf(recheck) == 0) addWorker(null, false); &#125; else if (!addWorker(command, false)) reject(command); workderCountOf()获得当前线程池线程总数，若小于corePoolSize，则直接将任务通过addWorker()方法执行，否则在workQueue.offer()进入等待队列，若进入失败，则任务直接交给线程池，若线程池达到了maximumPoolSize则提交失败执行拒绝策略 任务队列BlockingQueue：接口，阻塞队列，数据共享通道任务队列的作用在于，当线程池中线程数达到corePoolSize的时候，接下来的任务将进入这个队列进行等待，等待执行。 简单原理服务线程（获取队列信息并处理的线程）在队列为空时进行读等待，有新的消息进入队列后自动唤醒，反之，当队列满时进行写等待直到有消息出队。 不同于常用的offer()和poll()方法，这里我们使用take()和put()方法进行读写。我们以ArrayBlockingQueue的为例子,其中包括了这几个控制对象123final ReentrantLock lick;private final Condition notEmpty;private final Condition notFull; 就拿take()来说1234567891011public E take() throws InterruptedException &#123; final ReentrantLock lock = this.lock; lock.lockInterruptibly(); try &#123; while (count == 0) notEmpty.await(); return dequeue(); &#125; finally &#123; lock.unlock(); &#125;&#125; 当队列为空时，他会在notEmpty上进行等待，在线程等待时，若有新的元素插入，线程就会被唤醒12345678910private void enqueue(E x) &#123; // assert lock.getHoldCount() == 1; // assert items[putIndex] == null; final Object[] items = this.items; items[putIndex] = x; if (++putIndex == items.length) putIndex = 0; count++; notEmpty.signal();&#125; 常用实现 SynchronousQueue(直接提交队列): 一个零容量队列，每个插入操作要对应一个删除操作。提交的任务不会被真实保存，其实就是将新任务交给了线程执行。 ArrayBlockingQueue(有界任务队列): 这里就会用到线程池中另一个参数maximumPoolSize, 若当前线程池中线程小于corePoolSize则直接在线程池中增加线程，若大于，则加入该任务队列，若队列满则继续加入线程池，若线程池中数目多余maximumPoolSize则执行拒绝策略。 LinkedBlockingQueue(无界任务队列)：如果未指定容量，那么容量将等于 Integer.MAX_VALUE。只要插入元素不会使双端队列超出容量，每次插入后都将动态地创建链接节点。 PriorityBlockingQueue(优先任务队列)： 一个特殊的无界任务队列，前面两者都是按FIFO的顺序执行，而这个是可以按照优先级执行。拒绝策略JDK内置拒绝策略如下 AboerPolicy(默认）：直接抛出异常，阻止系统正常工作。 CallerRunsPolicy: 只要线程池未关闭，该策略直接在调用者线程中，运行当前被丢弃的任务。（伪丢弃，但是任务提交线程性能大幅度下降） DiscardOledestPolicy:和名字一样，丢弃最老的一个请求，也就是即将被执行的一个任务，并尝试再次提交当前任务。 DiscardPolicy: 丢弃无法处理的任务，不给任何处理。 异常堆栈首先给出一个例子：123456789101112131415161718192021222324252627public class Main implements Runnable&#123; int a, b; public Main(int a, int b) &#123; this.a = a; this.b = b; &#125; @Override public void run() &#123; int ret = a / b; System.out.println(ret); &#125; public static void main(String[] args) &#123; ThreadPoolExecutor pools = new ThreadPoolExecutor(0, Integer.MAX_VALUE, 0L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;()); for (int i = 0; i &lt; 5; i++) &#123; pools.submit(new Main(100, i)); &#125; &#125;&#125;/* 结果如下100335025*/ 可以发现，其中一个显然的异常除数为0不见了，我们可以通过将submit方法改为execute方法来打印部分异常信息，但是我们仍然不能发现他的调用线程在哪儿。这里我们通过扩展线程池给出一种解决办法。1234567891011121314151617181920212223242526272829303132333435363738394041424344package first_maven;import java.util.concurrent.BlockingQueue;import java.util.concurrent.Future;import java.util.concurrent.SynchronousQueue;import java.util.concurrent.ThreadPoolExecutor;import java.util.concurrent.TimeUnit;public class Main extends ThreadPoolExecutor&#123; public Main(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue) &#123; super(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue); &#125; @Override public void execute(Runnable task) &#123; super.execute(wrap(task, clientTrace(), Thread.currentThread().getName())); &#125; @Override public Future&lt;?&gt; submit(Runnable task) &#123; return super.submit(wrap(task, clientTrace(), Thread.currentThread().getName())); &#125; private Exception clientTrace() &#123; return new Exception("Client stack trace"); &#125; private Runnable wrap(final Runnable task, final Exception clientStack, String clientThreadName) &#123; return new Runnable() &#123; @Override public void run() &#123; try &#123; task.run(); &#125; catch(Exception e) &#123; clientStack.printStackTrace(); System.out.println(" 1212"); throw e; &#125; &#125; &#125;; &#125;&#125; 我们通过扩展ThreadPoolExecutor，将要执行的Runnable进行包装，通过手动创建异常，获取当前主线程的调用堆栈，从而得到线程池的调用信息，并打印相应的运行异常，这样我们就可以追踪到完整的异常信息。 总结在使用多线程的时候，要通过ThreadPoolExecutor来手动创建，根据当前任务的需求分配相应的线程池大小和阻塞队列以及拒绝策略，这样才能知根知底。 五. References 《实战Java高并发程序设计》 《阿里巴巴Java开发手册》]]></content>
      <categories>
        <category>并发</category>
      </categories>
      <tags>
        <tag>多线程</tag>
        <tag>容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[工厂模式]]></title>
    <url>%2F2018%2F01%2F13%2F%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[这两天正在看关于多线程的一些内容，看到线程池的时候发现它的实现使用了工厂模式，之前对工厂模式的了解不深，只是知道他是根据需要创建对象的，索性就开个支线，找了本书看了看关于工厂模式的一些知识，书中讲的也比较有意思，以下是一些心得。 概述对于设计模式来说，模式本身固然重要，但是模式设计的思想也同样很有味道，其中带来的一些OO的原则更是我们平时写代码需要注意的地方。而对于OO的设计原则其中有一个重要的思想就是将固定与变化分开，也就是简单的策略模式，将变化抽象，针对同一个接口，有各自的实现。 但是对于创建对象来说，java中只有new这一种方法，这就不可避免的要将代码写死，这又是我们不想看到的事情，由于硬编码带来的一系列拓展上的不便，使我们无法针对接口编程。就好像当我们使用集合的使用都会这么写： 1List&lt;T&gt; list = new XXXList&lt;&gt;(); 因为这种针对接口的编程给了我们更多的自由。那么有没有一种灵活的方式创建对象，那就是工厂模式，所有的工厂模式都是针对对象的创建。 一.简单工厂首先声明一下，简单工厂不是一种设计模式，只是一种习惯而已。他将动态的创建对象这一过程与固定的使用对象的代码分隔开。我们结合一个简单的例子来说：1234567891011121314151617181920212223242526272829303132333435363738394041class PizzaStore &#123; SimplePizzaFactory factory; public PizzaStore(SimplePizzaFactory factory) &#123; this.factory = factory; &#125; public Pizza orderPizza(String type) &#123; Pizza pizza; pizza = factory.createPizza(type); pizza.prepare(); pizza.bake(); pizza.cut(); pizza.box(); return pizza; &#125;&#125;public class SimplePizzaFactory &#123; public Pizza createPizza(String type) &#123; Pizza pizza = null; if (type.equals("cheese")) &#123; pizza = new CheesePizza(); &#125; else if (type.equals("pepperoni")) &#123; pizza = new PepperoniPizza(); &#125; else if (type.equals("clam")) &#123; pizza = new ClamPizza(); &#125; else if (type.equals("veggie")) &#123; pizza = new VeggiePizza(); &#125; return pizza; &#125;&#125; 在这个例子中，我们所需要创建的对象是Pizza，但在这里我们通过一个factory代替了以往的new关键字来创建对象，而这样的好处也是显而易见的，在这个服务中，变化的是Pizza的种类，而处理Pizza 的流程是固定的。我们只需根据需要传入所需的factory，就能实现创建对象与使用对象的解耦。 我们通过定义一个工厂类，将创建对象的操作通过这个类来进行，当对象种类增加时，我们只需要修改工厂类，就是所谓类对修改关闭，对扩展开放。二. 工厂方法在上一个例子中，我们在PizzaStore中创建简单工厂对象，通过简单工厂创建对象，这不免让代码失去了一点弹性，让我们进一步抽象，将创建对象的方法进一步封装，形成一个抽象基类，让每个子类去各自实现自己所需的创建对象的方法。提高代码的可扩展性。下面给出例子：12345678910111213141516171819202122232425262728abstract class PizzaStore &#123; abstract Pizza createPizza(String item); public Pizza orderPizza(String type) &#123; Pizza pizza = createPizza(type); System.out.println("--- Making a " + pizza.getName() + " ---"); pizza.prepare(); pizza.bake(); pizza.cut(); pizza.box(); return pizza; &#125;&#125;public class ChicagoPizzaStore extends PizzaStore &#123; Pizza createPizza(String item) &#123; if (item.equals("cheese")) &#123; return new ChicagoStyleCheesePizza(); &#125; else if (item.equals("veggie")) &#123; return new ChicagoStyleVeggiePizza(); &#125; else if (item.equals("clam")) &#123; return new ChicagoStyleClamPizza(); &#125; else if (item.equals("pepperoni")) &#123; return new ChicagoStylePepperoniPizza(); &#125; else return null; &#125;&#125; 在上面的代码中，对象的创建只给出了一个抽象方法，而具体的实现，则有子类自由选择决定，这样极大的丰富了代码的选择性和扩展性。基类实际上并不知道他持有的是什么对象，他主要负责持有对象后的一系列固定流程操作。 定义了一个创建对象的接口，但由子类决定要实例化的类是哪一个。工厂方法让类把实例化推迟到子类。 对比工厂方法和简单工厂原本有一个对象负责所有具体类的实例化，而在工厂方法中则由一些子类来负责实例化。工厂方法用来处理对象的创建，并将行为封装在子类，这样基类的代码就和子类的对象创建完全解耦。 三. 抽象工厂当你需要创建的对象也依赖了一系列可变对象，那么就需要工厂模式中的最后一种方式–抽象工厂。我们首先给出抽象工厂的定义： 提供一个借口，用于创建相关或依赖对象的家族，而不需要明确指定具体类。让我们再用Pizza来举例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public class ChicagoPizzaStore extends PizzaStore &#123; protected Pizza createPizza(String item) &#123; Pizza pizza = null; PizzaIngredientFactory ingredientFactory = new ChicagoPizzaIngredientFactory(); if (item.equals("cheese")) &#123; pizza = new CheesePizza(ingredientFactory); pizza.setName("Chicago Style Cheese Pizza"); &#125; else if (item.equals("veggie")) &#123; pizza = new VeggiePizza(ingredientFactory); pizza.setName("Chicago Style Veggie Pizza"); &#125; else if (item.equals("clam")) &#123; pizza = new ClamPizza(ingredientFactory); pizza.setName("Chicago Style Clam Pizza"); &#125; else if (item.equals("pepperoni")) &#123; pizza = new PepperoniPizza(ingredientFactory); pizza.setName("Chicago Style Pepperoni Pizza"); &#125; return pizza; &#125;&#125;class ChicagoPizzaIngredientFactory implements PizzaIngredientFactory &#123; public Dough createDough() &#123; return new ThickCrustDough(); &#125; public Sauce createSauce() &#123; return new PlumTomatoSauce(); &#125; public Cheese createCheese() &#123; return new MozzarellaCheese(); &#125; public Veggies[] createVeggies() &#123; Veggies veggies[] = &#123; new BlackOlives(), new Spinach(), new Eggplant() &#125;; return veggies; &#125; public Pepperoni createPepperoni() &#123; return new SlicedPepperoni(); &#125; public Clams createClam() &#123; return new FrozenClams(); &#125;&#125; PizzaStore和之前一样，这里就不重复了，和之前不一样的是在子类的createPizza方法中，我们不是简单的返回对象，而是根据创建对象所依赖的成员的不同，也进行了“个性化定制”。 其本质上其实也是用工厂方法对依赖对象进行创建。四. 抽象工厂与抽象方法的比较 工厂方法使用继承，把对象的创建委托给子类，子类实现工厂方法来创建对象，并将实例化延迟到子类。 抽象工厂使用组合，对象的创建被实现在工厂接口所暴露出来的方法中。 抽象工厂创建相关的对象家族，并让他们集合起来，而不需要依赖他们的具体类 五. References 《Head First 设计模式》]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>工厂</tag>
        <tag>OO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库索引]]></title>
    <url>%2F2017%2F11%2F27%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[索引在存储引擎层实现，所以我们先介绍一下不同的数据库引擎。 一.数据库引擎的类型1. InnoDBMySQL的默认事务型引擎，使用mvcc来支持高并发，实现了四个标准的隔离级别，默认级别是rr，通过间隙锁策略防止幻读。 InnoDB表基于聚簇索引建立的，数据存储在表空间中，可以将每个表的数据和索引存放在单独的文件中，支持热备份，其他的引擎都不支持。他的索引结构和其他存储引擎有很大不同，他的二级索引中必须包含主键 2. MyISAMMysql5.1及之前版本的默认引擎，有大量特性，包括全文索引、压缩、空间函数，不支持事务和行级锁，崩溃后无法安全恢复。 MyISAM会将表存储在两个文件中：数据文件和索引文件。 InnoDB和MyISAM的区别 InnoDB支持事务，MyISAM不支持 InnoDB是聚集索引，数据文件是和索引绑在一起的，必须要有主键，通过主键索引效率很高。但是辅助索引需要两次查询，先查询到主键，然后再通过主键查询到数据。因此，主键不应该过大，因为主键太大，其他索引也都会很大。而MyISAM是非聚集索引，数据文件是分离的，索引保存的是数据文件的物理地址。主键索引和辅助索引是独立的。 3.Memory如果需要快速的访问数据，并且这些数据不会被修改，重启后丢失也没有关系，那么使用Memory表很有用，他比MyISAM快一个数量级，因为索欧数据都保存在内存中。重启后结构保留，数据丢失。 二.索引的类型1.B-Tree索引实际上在很多引擎中使用的是B+树进行优化比如innoDB，之后再写一篇详细介绍B-树索引的文章，这里先简单介绍一下，他的工作原理，存储引擎不需要进行权标扫描来获取数据，而是从根节点开始搜索，根节点存了指向子节点的指针，根据这些指针向下层搜索，通过比较节点页的值和要查找的值，可以找到合适的指针进入下一层。 索引对多个值进行排序依据的是CREATE TABLE语句中定义索引时列的顺序 可以使用B-Tree索引的查询类型适用于全键值、键值范围、或者键前缀查找，假设索引建立在姓、名、出生日期上，实际使用如下所示 全值匹配： 匹配Cuba Allen、生日是1970-01-01的人 匹配最左前缀：可以用于查找姓为Allen的人，即只用第一列 匹配列前缀：匹配姓开头为J的 匹配范围值：查找姓在Allen和Jack之间的人 精确匹配某一列并范围匹配另一列：查找姓为Allen名字是K开头的 优点 大大减少了服务器需要扫描的数据量 可以帮助服务器避免排序和临时表 可以将随机io编程顺序io限制 若果不按引的最左列开始查找，则无法使用：比如无法找到特定生日的人活着名字叫Bill的人，要从索引的最左列开始，所以也无法找找姓氏以某个字母结尾的人 不能跳过索引：即使无法找到姓为Smith并且在某个特定日期出生的人 如果查询中有某个范围查询，那么他右边的所有列都无法使用索引优化查找：例如where last_name = “Smith” and first_name like “J%” and dob = ‘1990-01-01’,由于第二列是范围查询，所以第三列作废 Tips 对于BLOB、TEXT或者很长的VARCHAR类型的列，必须使用前缀索引，因为MySQL不允许索引这些列的完整长度。 选择合适的索引列顺序 2.哈希索引只有精确匹配索引所有列的查询才有效，对于每一行数据，都会对所有的索引列计算一个hashcode，哈希索引将所有的哈希码存储在索引中，同时在哈希表中保存指向每个数据行的指针 Innodb有一个功能叫做自适应哈希索引，当innodb注意到某些索引值使用的很频繁的时候，会在内存中基于B-Tree索引之上再创建一个哈希索引。 缺陷 索引只包含哈希值和行指针不存储字段值，无法通过索引值来避免读行 不是按索引值的顺序存储的，无法排序 不支持部分索引列匹配查找 只支持等值比较 访问哈希索引的数据很快，除非有很多冲突，冲突多的时候维护代价很大 3.空间数据索引(R_Tree)4. 全文索引InnoDB和MyISAM中B-Tree实现区别聚簇索引和非聚簇索引聚簇索引的顺序就是数据的物理存储顺序，而对非聚簇索引的索引顺序与数据物理排列顺序无关其实就是一个存储的是具体数据，一个存储了物理地址。正是聚簇索引的顺序就是数据的物理存储顺序，所以一个表最多只能有一个聚簇索引，因为物理存储只能有一个顺序。 InnoDB和MyISAM的数据分布对比下面分别是聚簇索引和非聚簇索引的存储结构图 总结： InnoDB是聚簇索引，通过主键引用被索引的值，数据文件和索引文件在一起，MyISAM是非聚簇索引，通过物理地址索引被索引的值，数据文件和索引文件在两个文件中 InnoDB的二级索引需要包含主键，MyISAM不需要，仍然只需要存储地址，他的主键索引有唯一性要求，二级索引没有 InnoDB索引保存了原格式文件，MyISAM使用了前缀压缩]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>索引</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库隔离级别]]></title>
    <url>%2F2017%2F11%2F26%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%2F</url>
    <content type="text"><![CDATA[READ UNCOMMITTED(未提交读)事务中的修改，即使没有提交，对其他事务也都是可见的，即读取了一个未提交的修改（脏读） READ COMMITTED(提交读)大多数数据库的默认隔离级别(mysql不是)，满足ACID中的隔离性定义：一个事务开始时，只能看见已经提交的事务所做的修改。有点不能理解，在网上看到了一个例子： singo拿着工资卡去消费，系统读取到卡里确实有2000元，而此时她的老婆也正好在网上转账，把singo工资卡的2000元转到另一账户，并在 singo之前提交了事务，当singo扣款时，系统检查到singo的工资卡已经没有钱，扣款失败，singo十分纳闷，明明卡里有钱，为 何…… 出现上述情况，即我们所说的不可重复读 ，两个并发的事务，“事务A：singo消费”、“事务B：singo的老婆网上转账”，事务A事先读取了数据，事务B紧接了更新了数据，并提交了事务，而事务A再次读取该数据时，数据已经发生了改变。 这里事务A两次读取到的数据都是已经提交的，不管是首次读取的2000，还是在B事务提交之后读取到的0，所以这个级别有时候也叫不可重复读。 当隔离级别设置为Read committed 时，避免了脏读，但是可能会造成不可重复读。 REPEATABLE READ(可重复度， mysql默认级别)该级别保证同一个事务多次读取的结果是一致的，在同一个事务里，SELECT的结果是事务开始时时间点的状态，因此，同样的SELECT操作读到的结果会是一致的。用上面的例子说就是当singo拿着工资卡去消费时，一旦系统开始读取工资卡信息（即事务开始），singo的老婆就不可能对该记录进行修改，也就是singo的老婆不能在此时转账。但是这个级别会产生幻读，所谓幻读，指的是当某个事务在读取某个范围内的记录时，另一个事务又在该范围内插入新纪录，当前事务再次读取该范围的记录时会产生幻行。 幻读和不可重复读的区别幻读是指当事务不是独立执行时发生的一种现象，例如第一个事务对一个表中的数据进行了修改，这种修改涉及到表中的全部数据行。同时，第二个事务也修改这个表中的数据，这种修改是向表中插入一行新数据。那么，以后就会发生操作第一个事务的用户发现表中还有没有修改的数据行，就好象发生了幻觉一样。 不可重复读强调的是对同一个数据的修改得到两个不同的结果，幻读强调的是对结果集的插入或删除操作，产生了新的结果集。 Serializable (序列化)Serializable 是最高的事务隔离级别，强制事务串行执行，简单来说就是读取的每一行数据都加锁，同时代价也花费最高，性能很低，一般很少使用，在该级别下，事务顺序执行，不仅可以避免脏读、不可重复读，还避免了幻像读。 间隙锁（Next-Key锁）当我们用范围条件而不是相等条件检索数据，并请求共享或排他锁时，InnoDB会给符合条件的已有数据记录的索引项加锁；对于键值在条件范围内但并不存在的记录，叫做“间隙（GAP)”，InnoDB也会对这个“间隙”加锁，这种锁机制就是所谓的间隙锁（Next-Key锁）,另外一方面，是为了满足其恢复和复制的需要。 举例来说，假如emp表中只有101条记录，其empid的值分别是 1,2,…,100,101，下面的SQL： Select * from emp where empid &gt; 100 for update;是一个范围条件的检索，InnoDB不仅会对符合条件的empid值为101的记录加锁，也会对empid大于101（这些记录并不存在）的“间隙”加锁。 InnoDB使用间隙锁的目的，一方面是为了防止幻读，以满足相关隔离级别的要求， √: 可能出现 ×: 不会出现 隔离级别 脏读 不可重复读 幻读 加锁读 Read uncommitted √ √ √ × Read committed × √ √ × Repeatable read × × √ × Serializable × × × √ 最后提一句InnoDB采用两阶段锁协议，只有在commit和rollback的时候才会释放锁并且是同一时间释放，并且这里说的锁都是隐式加锁，InnoDB会根据隔离级别在需要的时候自动加锁。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>事务</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[为什么String是不可变对象(译)]]></title>
    <url>%2F2017%2F11%2F23%2F%E4%B8%BA%E4%BB%80%E4%B9%88String%E6%98%AF%E4%B8%8D%E5%8F%AF%E5%8F%98%E5%AF%B9%E8%B1%A1(%E8%AF%91)%2F</url>
    <content type="text"><![CDATA[原文:Why String is immutable in Java ? String是Java中的一个不可变类。所谓不可变，简单来说就是其对象不能被修改。实例中的所有信息在初始化的时候就已经具备，并且不能被修改。不可变类有很多优点。这篇文章简要说明了为什么String被设计为不可变类。关于其好的回答应该建立在对内存模型、同步和数据结构等的理解之上。 1. 字符串池的需求字符串池是一个位于方法区的特殊区域。当一个字符串被创建的时候，如果该字符串已经存在于字符串池中，那么直接返回该字符串的引用，而不是创建一个新的字符串。下边的代码将只会创建一个字符串对象：12String s1 = "abcd";String s2 = "abcd"; s1和s2都指向同一个字符串对象。如果String不是不可变的，那么修改s1的字符串对象同样也会导致s2的内容发生变化。 2. 缓存Hashcode字符串的hashcode在Java中经常被用到。例如，在一个HashMap中。其不可变性保证了hashcode（哈希值）总是保持不变，从而不用担心因hashcode变化导致的缓存问题。那就意味着，不用每次在其使用的时候计算其hashcode，从而更加高效。在String类中，有如下代码：1private int hash; //用来缓存hash code 3. 简化其他对象的使用为了理解这一点，请看下边的代码：123456HashSet&lt;String&gt; set = new HashSet&lt;String&gt;();set.add(new String(&quot;a&quot;));set.add(new String(&quot;b&quot;));set.add(new String(&quot;c&quot;));for (String a : set) a.value = &quot;a&quot;; 这个例子中，如果String是可变的，也就是说set中的值是可变的，这会影响到set的设计（set包含不重复的元素）。当然这个例子是有问题的，在String类中是不存在value这个属性的。（ps：个人觉得应该是没有可以直接访问的value，毕竟String中value数组是可以通过反射访问的，不知道，不知道这个老外是怎么个意思） 4.安全性字符串在许多的java类中都用作参数，例如网络连接，打开文件等等。如果字符串是可变的，一个连接或文件就会被修改从而导致严重的错误。可变的字符串也会导致在使用反射时导致严重的问题，因为参数是字符串形式的。举例如下：1234567boolean connect(String s) &#123; if (!isSecure(s)) &#123; throw new SecurityException(); &#125; // 如果s内的值被修改，则会导致出现问题 doSomethind(s); &#125; （虽然略牵强，但是也有一定道理） 5. 不可变的对象本身就是线程安全的不可变的对象，可以在多个线程间自由共享。从而免除了进行同步的麻烦。 总之， String被设计为不可变的类，是出于性能和安全性的考虑，这也是其他所有不可变类应用的初衷。 6. 引用 http://blog.csdn.net/get_set/article/details/49926511]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>源码分析</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
</search>
